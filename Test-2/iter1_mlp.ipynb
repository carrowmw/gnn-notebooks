{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Iter 1: MLPZINBNLL Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "For this iteration, we implement a simple MLP baseline model that predicts traffic values based solely on static node features and time features. The model does not utilise any graph structure or temporal dependencies, serving as a foundational benchmark for future, more complex models. It is fed each timestep as a seperate feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "SCALER_MU = 5.887820243835449\n",
    "SCALER_SIGMA = 7.024876594543457"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = torch.load(\"data/processed/9f751fe859f07b5c/edge_index.pt\", weights_only=False)\n",
    "edge_attr_data = torch.load(\"data/processed/9f751fe859f07b5c/edge_attr.pt\", weights_only=False)\n",
    "static_features = torch.load(\"data/processed/9f751fe859f07b5c/static_features.pt\", weights_only=False)\n",
    "sensor_mask = torch.load(\"data/processed/9f751fe859f07b5c/sensor_mask.pt\", weights_only=False)\n",
    "train_loader = torch.load(\"data/processed/9f751fe859f07b5c/train_loader.pt\", weights_only=False)\n",
    "val_loader = torch.load(\"data/processed/9f751fe859f07b5c/val_loader.pt\", weights_only=False)\n",
    "test_loader = torch.load(\"data/processed/9f751fe859f07b5c/test_loader.pt\", weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"n batches:\", len(train_loader))\n",
    "for X, y in train_loader:\n",
    "    print(X.shape, y.shape)\n",
    "    print(\"Shape of tensor X: [batch_size, seq_len, num_nodes, num_features]\")\n",
    "    sample_window_a = X[0, :, 0, :]\n",
    "    sample_window_b = X[1, :, 0, :]\n",
    "    sample_window_c = X[2, :, 0, :]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_names = [\"value__mean_L12\",\n",
    "      \"value__std_L12\",\n",
    "      \"value__min_L12\",\n",
    "      \"value__max_L12\",\n",
    "      \"value__q25_L12\",\n",
    "      \"value__q75_L12\",\n",
    "      \"value__slope_L12\",\n",
    "      \"value__energy_L12\",\n",
    "      \"value__valid_frac_L12\",\n",
    "      \"value\"]\n",
    "fig, ax = plt.subplots(5,2,figsize=(12,16))\n",
    "for i, feature in enumerate(features_names):\n",
    "      ax[i//2, i%2].plot(sample_window_a[:, i], label=features_names[i])\n",
    "      ax[i//2, i%2].plot(sample_window_b[:, i], linestyle='--', label=f\"{features_names[i]} (Window B)\")\n",
    "      ax[i//2, i%2].plot(sample_window_c[:, i], linestyle=':', label=f\"{features_names[i]} (Window C)\")\n",
    "      ax[i//2, i%2].set_title(features_names[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This sections flattens the complex input structure for MLP input\n",
    "def flatten_loader(loader, batch_size):\n",
    "    # Only keep the first timestep for aggregated features\n",
    "    agg_loader = [(X[:, 0:1, :, :-1], y[:, 0:1, :, :]) for X, y in loader]\n",
    "    print(agg_loader[0][0].shape, agg_loader[0][1].shape)\n",
    "    raw_loader = [(X[:, :, :, -1:].permute(0,3,2,1), y[:, :1, :, :]) for X, y in loader]\n",
    "    print(raw_loader[0][0].shape, raw_loader[0][1].shape)\n",
    "    # Concatenate all batches into a single batch\n",
    "    flatten_X_agg = torch.cat([X for X, _ in agg_loader], dim=0)\n",
    "    flatten_X_raw = torch.cat([X for X, _ in raw_loader], dim=0)\n",
    "    flatten_X = torch.cat([flatten_X_agg, flatten_X_raw], dim=3)\n",
    "    flatten_y = torch.cat([y for _, y in agg_loader], dim=0)\n",
    "    print(\"New shape of tensor X:\", flatten_X.shape, \"tensor y:\", flatten_y.shape)\n",
    "    # Recreate DataLoader with flattened data\n",
    "    flat_dataset = torch.utils.data.TensorDataset(flatten_X, flatten_y)\n",
    "    flat_loader = torch.utils.data.DataLoader(flat_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "    print(\"New shape of DataLoader batches:\", next(iter(flat_loader))[0].shape, next(iter(flat_loader))[1].shape)\n",
    "    print(\"Length of new DataLoader:\", len(flat_loader))\n",
    "    return flat_loader\n",
    "\n",
    "train_loader = flatten_loader(train_loader, batch_size=16)\n",
    "val_loader = flatten_loader(val_loader, batch_size=16)\n",
    "test_loader = flatten_loader(test_loader, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "def _finite_stats(name, t: torch.Tensor):\n",
    "    if t is None:\n",
    "        print(f\"[DEBUG] {name}: None\")\n",
    "        return False\n",
    "    is_finite = torch.isfinite(t)\n",
    "    if not is_finite.all():\n",
    "        n_nan = torch.isnan(t).sum().item()\n",
    "        n_inf = torch.isinf(t).sum().item()\n",
    "        print(f\"[NON-FINITE] {name}: nan={n_nan}, inf={n_inf}, shape={tuple(t.shape)}\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def _check(name, t:torch.Tensor):\n",
    "    ok = _finite_stats(name, t)\n",
    "    # if not ok:\n",
    "    #     # Early stop by rasing to surface the exact point\n",
    "    #     raise RuntimeError(f\"Non-finite tensor detected in {name}\")\n",
    "\n",
    "class SimpleNodeMLPZINB(nn.Module):\n",
    "    def __init__(self, dynamic_node_dim, static_node_dim, n_embd, static_node_features, dropout_rate):\n",
    "        super().__init__()\n",
    "\n",
    "        # Input feature dim is doubled to include missingness mask\n",
    "        dynamic_input_dim = dynamic_node_dim * 2\n",
    "        static_input_dim = static_node_dim\n",
    "        # Static feature dims are added to the dynamic input dims\n",
    "        mlp_input_channels = dynamic_input_dim + static_input_dim\n",
    "\n",
    "        self.embedder = nn.Linear(mlp_input_channels, n_embd)\n",
    "        self.hidden_layer = nn.Linear(n_embd, n_embd)\n",
    "        self.norm = nn.LayerNorm(n_embd)\n",
    "\n",
    "        # ZINB has 3 parameter per output:\n",
    "        # 1. mu (mean of NB)\n",
    "        # 2. theta (dispersion of NB)\n",
    "        # 3. pi (probability of zero-inflation)\n",
    "        self.mu_head = nn.Linear(n_embd, 1) # Mean (positive)\n",
    "        self.theta_head = nn.Linear(n_embd, 1) # Dispersion (positive)\n",
    "        self.pi_head = nn.Linear(n_embd, 1) # Zero-inflation probability (0-1)\n",
    "\n",
    "        # Output layer takes the input of the 'embedder layer'\n",
    "        # self.output_layer = nn.Linear(n_embd, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.register_buffer('static_node_features', static_node_features)\n",
    "\n",
    "    def forward(self, X_batch, targets, node_mask):\n",
    "        # X_batch: (Num_Nodes, Input_Batch_Feature_Dim)\n",
    "        # targets: (Num_Nodes, Output_Dim) - for training\n",
    "        # node_mask: (Num_Nodes) - boolean, True for nodes to include (e.g., sensor nodes)\n",
    "        B, S, N, _ = X_batch.shape\n",
    "        missing_mask = torch.isnan(X_batch)\n",
    "        imputed_X = torch.nan_to_num(X_batch, nan=0.0)\n",
    "        _check(\"imputed_X\", imputed_X)\n",
    "        mask_features = missing_mask.float()\n",
    "        combined_input = torch.cat([imputed_X, mask_features], dim=-1)\n",
    "        _check(\"combined_input\", combined_input)\n",
    "        static_expanded = self.static_node_features.unsqueeze(0).unsqueeze(1).expand(B, S, N, -1)\n",
    "        # Unsqueeze static features to match batch size\n",
    "        combined_features = torch.cat([combined_input, static_expanded], dim=-1)\n",
    "\n",
    "        x = self.embedder(combined_features)\n",
    "        _check(\"embedder output\", x)\n",
    "        x = self.relu(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.hidden_layer(x)\n",
    "        _check(\"hidden_layer output\", x)\n",
    "        x = self.relu(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Predict ZINB parameters\n",
    "        mu = F.softplus(self.mu_head(x)) + 1e-6 # Ensure positivity\n",
    "        theta = F.softplus(self.theta_head(x)) + 1e-6 # Ensure positivity\n",
    "        pi = torch.sigmoid(self.pi_head(x)) # Probability between 0 and 1\n",
    "\n",
    "        _check(\"mu\", mu)\n",
    "        _check(\"theta\", theta)\n",
    "        _check(\"pi\", pi)\n",
    "\n",
    "        zinb_nll_loss = None\n",
    "\n",
    "        zinb_nll_loss, valid_sum = self.zinb_nll_loss(mu, theta, pi, targets, node_mask)\n",
    "        # Return mu as point prediction for evaluation\n",
    "        preds = mu * (1 - pi) # Expected value of ZINB\n",
    "        mse_loss, _ = self.mse_loss(preds, targets, node_mask)\n",
    "\n",
    "        return preds, zinb_nll_loss, mse_loss, {'mu': mu, 'theta': theta, 'pi': pi, 'valid_sum': valid_sum}\n",
    "\n",
    "    def zinb_nll_loss(self, mu, theta, pi, targets, node_mask):\n",
    "        \"\"\"\n",
    "        Zero-Inflated Negative Binomial Negative Log-Likelihood Loss\n",
    "\n",
    "        Args:\n",
    "            mu: predicted mean (batch_size, 1)\n",
    "            theta: dispersion parameter (batch_size, 1)\n",
    "            pi: zero-inflation probability (batch_size, 1)\n",
    "            targets: actual counts (batch_size, 1)\n",
    "            node_mask: boolean mask for which nodes to include\n",
    "        \"\"\"\n",
    "        eps = 1e-8\n",
    "\n",
    "        # Creat NaN mask - True for valid (non-NaN) values\n",
    "        nan_mask = ~torch.isnan(targets)\n",
    "\n",
    "        # Combine with existing node_mask if provided\n",
    "        if node_mask is not None:\n",
    "            valid_mask = nan_mask & node_mask\n",
    "        else:\n",
    "            valid_mask = nan_mask\n",
    "\n",
    "        # Check if we have any valid values\n",
    "        if valid_mask.sum() == 0:\n",
    "            return torch.tensor(0.0, device=mu.device, requires_grad=True)\n",
    "\n",
    "\n",
    "\n",
    "        # Index only valid positions\n",
    "        mu_valid = mu[valid_mask]\n",
    "        theta_valid = theta[valid_mask]\n",
    "        pi_valid = pi[valid_mask]\n",
    "        targets_valid = targets[valid_mask]\n",
    "\n",
    "        _check(\"mu_valid\", mu_valid)\n",
    "        _check(\"theta_valid\", theta_valid)\n",
    "        _check(\"pi_valid\", pi_valid)\n",
    "        _check(\"targets_valid\", targets_valid)\n",
    "\n",
    "        # NB log probability\n",
    "        # log p(y(NB)) = log Gamma(theta+y) - log Gamma(theta) - log Gamma(y+1)\n",
    "        #               + theta*log(theta) - theta*log(theta + mu)\n",
    "        #               + y*log(mu) - y*log(theta + mu)\n",
    "\n",
    "        theta_mu = theta_valid + mu_valid\n",
    "\n",
    "        # Using lgamma for numerical stability\n",
    "        nb_log_prob = (\n",
    "            torch.lgamma(theta_valid + targets_valid + eps)\n",
    "            - torch.lgamma(theta_valid + eps)\n",
    "            - torch.lgamma(targets_valid + 1)\n",
    "            + theta_valid * torch.log(theta_valid + eps)\n",
    "            - theta_valid * torch.log(theta_mu + eps)\n",
    "            + targets_valid * torch.log(mu_valid + eps)\n",
    "            - targets_valid * torch.log(theta_mu + eps)\n",
    "        )\n",
    "\n",
    "        # ZINB combine zero-inflation with NB\n",
    "        # p(y) = pi*I(y=0) + (1-pi)*NB(y)\n",
    "        # For y=0: log p(0) = log(pi + (1-pi)*NB(0))\n",
    "        # For y>0: log p(y) = log(1-pi) + log NB(y)\n",
    "\n",
    "        zero_mask = (targets_valid < eps).float()\n",
    "\n",
    "        # For zero counts\n",
    "        nb_zero_prob = theta_valid * torch.log(theta_valid / (theta_mu + eps))\n",
    "        zero_log_prob = torch.log(pi_valid + (1 - pi_valid) * torch.exp(nb_zero_prob) + eps)\n",
    "\n",
    "        # For non-zer counts\n",
    "        non_zero_log_prob = torch.log(1 - pi_valid + eps) + nb_log_prob\n",
    "\n",
    "        # Combine\n",
    "        log_prob = zero_mask * zero_log_prob + (1 - zero_mask) * non_zero_log_prob\n",
    "\n",
    "        # Mean over valid samples only\n",
    "        nll = -log_prob.mean()\n",
    "\n",
    "        return nll, valid_mask.sum()\n",
    "\n",
    "    def mse_loss(self, predictions, targets, node_mask):\n",
    "        # Create NaN mask - True for valid (non-NaN) values\n",
    "        nan_mask = ~torch.isnan(targets)\n",
    "\n",
    "        # Combine with existing node_mask if provided\n",
    "        if node_mask is not None:\n",
    "            valid_mask = nan_mask & node_mask\n",
    "        else:\n",
    "            valid_mask = nan_mask\n",
    "\n",
    "        # Check if we have any valid values\n",
    "        if valid_mask.sum() == 0:\n",
    "            return torch.tensor(0.0, device=predictions.device, requires_grad=True), 0\n",
    "\n",
    "        preds_valid = predictions[valid_mask]\n",
    "        targets_valid = targets[valid_mask]\n",
    "\n",
    "        mse_loss = ((targets_valid - preds_valid)**2).mean()\n",
    "\n",
    "        return mse_loss, valid_mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 128  # Embedding dimension for the MLP\n",
    "output_dim = 1 # Predicting a single traffic value\n",
    "dropout = 0.1\n",
    "lr = 1e-4\n",
    "epochs = 40 # More epochs as the model is simple\n",
    "\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else ('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_static_input = static_features.to(device)\n",
    "edge_index_input = edge_index.to(device)\n",
    "edge_attr_input = edge_attr_data.to(device)\n",
    "\n",
    "sensor_mask_input = sensor_mask.to(device)\n",
    "\n",
    "num_nodes_input = static_features.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_nodes_dim = static_features.shape[1]\n",
    "edge_attr_dim = edge_attr_data.shape[1]\n",
    "print(\"Static Node Features Dim:\", static_nodes_dim)\n",
    "print(\"Edge Attribute Dim:\", edge_attr_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleNodeMLPZINB(\n",
    "    dynamic_node_dim=21,\n",
    "    static_node_dim=static_nodes_dim,\n",
    "    n_embd=n_embd,\n",
    "    dropout_rate=dropout,\n",
    "    static_node_features=X_static_input\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "print(f\"Model Iteration 1 Parameters: {sum(p.numel() for p in model.parameters())/1e3:.1f} K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot(steps, train_zinb_nll, val_zinb_nll, train_mse, val_mse, mu, theta, pi):\n",
    "    # Append scalar values, not lists\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10,10))\n",
    "\n",
    "    ax1.clear()\n",
    "    ax2.clear()\n",
    "\n",
    "    ax1.plot(steps, train_zinb_nll, label='Train ZINB NLL', marker='o')\n",
    "    ax1.plot(steps, val_zinb_nll, label='Validation ZINB NLL', marker='o')\n",
    "    ax1.plot(steps, train_mse, label='Train MSE', marker='x')\n",
    "    ax1.plot(steps, val_mse, label='Validation MSE', marker='x')\n",
    "    ax1.legend()\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Training and Validation Loss Over Time')\n",
    "    ax1.grid(True)\n",
    "\n",
    "    ax2.plot(steps, mu, label='Mu', color='green', marker='o')\n",
    "    ax2.plot(steps, theta, label='Theta', color='red', marker='o')\n",
    "    ax2.plot(steps, pi, label='Pi', color='purple', marker='o')\n",
    "    ax2.legend()\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Parameter Value')\n",
    "    ax2.set_title('Model Parameters Over Time')\n",
    "    ax2.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "def format_params(params):\n",
    "    params_cat = {\"mu\":[], \"theta\":[], \"pi\": []}\n",
    "    for batch in params:\n",
    "        for key in params_cat.keys():\n",
    "            value = batch[key]\n",
    "            params_cat[key].append(value)\n",
    "\n",
    "    for key in params_cat.keys():\n",
    "        params_cat[key] = torch.cat(params_cat[key], dim=0)\n",
    "\n",
    "    # Return as tuple for easy unpacking\n",
    "    return params_cat[\"mu\"], params_cat[\"theta\"], params_cat[\"pi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training Loop Iteration 1 ---\n",
    "import time\n",
    "start_time = time.time()\n",
    "print(f\"Training started at {start_time}\")\n",
    "model.train()\n",
    "# For plotting\n",
    "avg_train_zinb_nll = []\n",
    "avg_train_mse = []\n",
    "avg_val_zinb_nll = []\n",
    "avg_val_mse = []\n",
    "avg_mu = []\n",
    "avg_theta = []\n",
    "avg_pi = []\n",
    "for epoch in range(epochs):\n",
    "    epoch_zinb_nll = 0\n",
    "    epoch_mse = 0\n",
    "    epoch_valid_samples = 0\n",
    "    epoch_total_samples = 0\n",
    "    num_batches = 0\n",
    "    val_epoch_zinb_nll = 0\n",
    "    val_epoch_mse = 0\n",
    "    val_epoch_valid_samples = 0\n",
    "    val_epoch_total_samples = 0\n",
    "    val_num_batches = 0\n",
    "    epoch_params = []\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        # Track batch statistics\n",
    "        batch_size = y_batch.shape[0]\n",
    "        batch_total_samples = y_batch.numel() # Total elements in batch\n",
    "\n",
    "        # 4. Unnormalize the target variable\n",
    "        # y_raw = (y_batch_normalized * sigma) + mu\n",
    "        y_raw = (y_batch * SCALER_SIGMA) + SCALER_MU # Using your notebook's variable name\n",
    "        # 5. Round to nearest integer and cast to long\n",
    "        # This is ESSENTIAL for the ZINB loss function\n",
    "        y_raw_int = torch.round(y_raw).long()\n",
    "\n",
    "        predictions, zinb_nll_loss, mse_loss, params = model(X_batch=X_batch, targets=y_raw_int, node_mask=None)\n",
    "\n",
    "\n",
    "        zinb_nll_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate metrics\n",
    "        epoch_params.append(params)\n",
    "        epoch_zinb_nll += zinb_nll_loss.item()\n",
    "        epoch_mse += mse_loss.item()\n",
    "        epoch_valid_samples += params['valid_sum'].item()\n",
    "        epoch_total_samples += batch_total_samples\n",
    "        num_batches += 1\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            # 4. Unnormalize the target variable\n",
    "            # y_raw = (y_batch_normalized * sigma) + mu\n",
    "            y_raw = (y_batch * SCALER_SIGMA) + SCALER_MU # Using your notebook's variable name\n",
    "            # 5. Round to nearest integer and cast to long\n",
    "            # This is ESSENTIAL for the ZINB loss function\n",
    "            y_raw_int = torch.round(y_raw).long()\n",
    "\n",
    "            predictions, zinb_nll_loss, mse_loss, params = model(X_batch=X_batch, targets=y_raw_int, node_mask=None)\n",
    "\n",
    "            val_epoch_zinb_nll += zinb_nll_loss.item()\n",
    "            val_epoch_mse += mse_loss.item()\n",
    "            val_epoch_valid_samples += params['valid_sum'].item()\n",
    "            val_epoch_total_samples += y_batch.numel()\n",
    "            val_num_batches += 1\n",
    "\n",
    "    avg_zinb_nll = epoch_zinb_nll / num_batches\n",
    "    avg_mse = epoch_mse / num_batches\n",
    "    valid_percentage = (epoch_valid_samples / epoch_total_samples) * 100\n",
    "    avg_valid_per_batch = epoch_valid_samples / num_batches\n",
    "\n",
    "    val_avg_zinb_nll = val_epoch_zinb_nll / val_num_batches\n",
    "    val_avg_mse = val_epoch_mse / val_num_batches\n",
    "    val_valid_percentage = (val_epoch_valid_samples / val_epoch_total_samples) * 100\n",
    "    val_avg_valid_per_batch = val_epoch_valid_samples / val_num_batches\n",
    "\n",
    "    if (epoch+1) % 10 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1:3d} | \"\n",
    "        f\"ZINB NLL: {avg_zinb_nll:.4f} | \"\n",
    "        f\"MSE: {avg_mse:.4f} | \"\n",
    "        f\"Valid: {epoch_valid_samples}/{epoch_total_samples} ({valid_percentage:.1f}%) | \"\n",
    "        f\"Avg/Batch: {avg_valid_per_batch:.1f} | \"\n",
    "        f\"Batches: {num_batches}\")\n",
    "        print(f\"      VAL | \"\n",
    "        f\"ZINB NLL: {val_avg_zinb_nll:.4f} | \"\n",
    "        f\"MSE: {val_avg_mse:.4f} | \"\n",
    "        f\"Valid: {val_epoch_valid_samples}/{val_epoch_total_samples} ({val_valid_percentage:.1f}%) | \"\n",
    "        f\"Avg/Batch: {val_avg_valid_per_batch:.1f} | \"\n",
    "        f\"Batches: {val_num_batches}\")\n",
    "\n",
    "    mu, theta, pi = format_params(epoch_params)\n",
    "\n",
    "    avg_train_zinb_nll.append(avg_zinb_nll)\n",
    "    avg_val_zinb_nll.append(val_avg_zinb_nll)\n",
    "\n",
    "    avg_train_mse.append(avg_mse)\n",
    "    avg_val_mse.append(val_avg_mse)\n",
    "\n",
    "    avg_mu.append(mu.mean().item())\n",
    "    avg_theta.append(theta.mean().item())\n",
    "    avg_pi.append(pi.mean().item())\n",
    "\n",
    "print(f\"Training completed at {time.time()}, duration: {time.time() - start_time:.2f}s\")\n",
    "steps = list(range(1, epochs+1))\n",
    "plot(steps, avg_train_zinb_nll, avg_val_zinb_nll, avg_train_mse, avg_val_mse, avg_mu, avg_theta, avg_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced metrics with node-level tracking\n",
    "def detailed_evaluation(model, data_loader, device, split_name=\"Val\"):\n",
    "    model.eval()\n",
    "\n",
    "    # Track per-node validity (assuming shape [batch, 1, num_nodes, 1])\n",
    "    num_nodes = None\n",
    "    node_valid_counts = None\n",
    "    node_total_counts = None\n",
    "\n",
    "    total_zinb_nll_loss = 0.0\n",
    "    total_mse_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    all_preds = []\n",
    "    all_params = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            if num_nodes is None:\n",
    "                num_nodes = y_batch.shape[2]\n",
    "                node_valid_counts = torch.zeros(num_nodes)\n",
    "                node_total_counts = torch.zeros(num_nodes)\n",
    "\n",
    "            # 4. Unnormalize the target variable\n",
    "            # y_raw = (y_batch_normalized * sigma) + mu\n",
    "            y_raw = (y_batch * SCALER_SIGMA) + SCALER_MU # Using your notebook's variable name\n",
    "            # 5. Round to nearest integer and cast to long\n",
    "            # This is ESSENTIAL for the ZINB loss function\n",
    "            y_raw_int = torch.round(y_raw).long()\n",
    "\n",
    "            preds, zinb_nll_loss, mse_loss, params = model(X_batch=X_batch, targets=y_raw_int, node_mask=None)\n",
    "\n",
    "            all_preds.append(preds.cpu())\n",
    "            all_params.append(params)\n",
    "\n",
    "            if zinb_nll_loss is not None:\n",
    "                total_zinb_nll_loss += zinb_nll_loss.item()\n",
    "                total_mse_loss += mse_loss.item()\n",
    "                num_batches += 1\n",
    "\n",
    "                # Count valid samples per node\n",
    "                nan_mask = ~torch.isnan(y_batch)  # [batch, 1, num_nodes, 1]\n",
    "                node_valid_counts += nan_mask.sum(dim=(0, 1, 3)).cpu()\n",
    "                node_total_counts += torch.ones_like(node_valid_counts) * y_batch.shape[0]\n",
    "\n",
    "    if num_batches > 0:\n",
    "        avg_zinb_nll_loss = total_zinb_nll_loss / num_batches\n",
    "        avg_mse_loss = total_mse_loss / num_batches\n",
    "\n",
    "        print(f\"\\n{split_name} Detailed Metrics:\")\n",
    "        print(f\"  Avg ZINB NLL Loss: {avg_zinb_nll_loss:.4f}\")\n",
    "        print(f\"  Avg MSE Loss: {avg_mse_loss:.4f}\")\n",
    "        print(f\"  Total batches: {num_batches}\")\n",
    "\n",
    "        # Per-node statistics\n",
    "        node_valid_pct = (node_valid_counts / node_total_counts * 100)\n",
    "        print(f\"\\n  Node Validity Statistics:\")\n",
    "        print(f\"    Min: {node_valid_pct.min():.1f}%\")\n",
    "        print(f\"    Max: {node_valid_pct.max():.1f}%\")\n",
    "        print(f\"    Mean: {node_valid_pct.mean():.1f}%\")\n",
    "        print(f\"    Nodes with 100% valid: {(node_valid_pct == 100).sum().item()}/{num_nodes}\")\n",
    "        print(f\"    Nodes with <50% valid: {(node_valid_pct < 50).sum().item()}/{num_nodes}\")\n",
    "\n",
    "        return all_preds, all_params\n",
    "\n",
    "# Run after training\n",
    "train_results = detailed_evaluation(model, train_loader, device, \"Train\")\n",
    "val_results = detailed_evaluation(model, val_loader, device, \"Validation\")\n",
    "test_results = detailed_evaluation(model, test_loader, device, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds, train_params = train_results\n",
    "val_preds, val_params = val_results\n",
    "test_preds, test_params = test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mu, train_theta, train_pi = format_params(train_params)\n",
    "val_mu, val_theta, val_pi = format_params(val_params)\n",
    "test_mu, test_theta, test_pi = format_params(test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data_for_plotting(preds, dataloader):\n",
    "    targets = []\n",
    "    for _, y in dataloader:\n",
    "        targets.append(y)\n",
    "    cat_targets = torch.cat(targets, dim=0)\n",
    "    cat_preds = torch.cat(preds, dim=0)\n",
    "    return cat_targets, cat_preds\n",
    "\n",
    "train_targets, train_preds = format_data_for_plotting(train_preds, train_loader)\n",
    "val_targets, val_preds = format_data_for_plotting(val_preds, val_loader)\n",
    "test_targets, test_preds = format_data_for_plotting(test_preds, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.distributions as dist\n",
    "import numpy as np\n",
    "from scipy import stats as sp_stats # Import scipy.stats\n",
    "\n",
    "with open(\"data/processed/processed_ac88600464456762_sensor_name_to_id_map.json\", \"r\") as f:\n",
    "    name_to_id_map = json.load(f)\n",
    "    id_to_name_map = {v: k for k, v in name_to_id_map.items()}\n",
    "\n",
    "# This function is now correct, just a small rename for clarity\n",
    "def format_data_for_plotting(preds_list, params_list, dataloader):\n",
    "    # 1. Unnormalize Ground Truth Targets\n",
    "    targets = []\n",
    "    for _, y in dataloader:\n",
    "        y_raw = (y * SCALER_SIGMA) + SCALER_MU\n",
    "        y_raw_int = torch.round(y_raw).long()\n",
    "        targets.append(y_raw_int)\n",
    "    cat_targets = torch.cat(targets, dim=0)\n",
    "\n",
    "    # 2. Concatenate Model Outputs\n",
    "    cat_mu, cat_theta, cat_pi = format_params(params_list)\n",
    "\n",
    "    return cat_targets, cat_mu, cat_theta, cat_pi\n",
    "\n",
    "# --- RE-RUN YOUR DATA FORMATTING ---\n",
    "# We need pi, so we must re-run this part\n",
    "train_targets, train_mu, train_theta, train_pi = format_data_for_plotting(train_preds, train_params, train_loader)\n",
    "val_targets, val_mu, val_theta, val_pi = format_data_for_plotting(val_preds, val_params, val_loader)\n",
    "test_targets, test_mu, test_theta, test_pi = format_data_for_plotting(test_preds, test_params, test_loader)\n",
    "\n",
    "\n",
    "# --- YOUR NEW PLOTTING FUNCTION ---\n",
    "\n",
    "def plot_preds_and_ground_truth(targets, mu, theta, pi, names):\n",
    "    \"\"\"\n",
    "    Plots the ground truth against the predicted ZINB distribution.\n",
    "\n",
    "    - 'True': The actual ground truth counts.\n",
    "    - 'Predicted': The ZINB Expected Value, E[y] = (1-pi) * mu\n",
    "    - '95% P.I.': The 95% prediction interval (2.5th to 97.5th percentile)\n",
    "    \"\"\"\n",
    "\n",
    "    # An small value to prevent division by zero or invalid probs\n",
    "    eps = 1e-8\n",
    "\n",
    "    # --- 1. Calculate the Expected Value (The \"Prediction\") ---\n",
    "    # This can stay in torch\n",
    "    expected_value = (1 - pi) * mu\n",
    "\n",
    "    # --- 3. Plot ---\n",
    "    num_nodes = targets.shape[2]\n",
    "    for node in range(num_nodes):\n",
    "        plt.figure(figsize=(10, 4))\n",
    "\n",
    "        # Get data for the current node and move to CPU/NumPy\n",
    "        # We plot the last feature dimension\n",
    "        true_node = targets[:, 0, node, -1].cpu().numpy()\n",
    "        pred_node = expected_value[:, 0, node, -1].cpu().numpy()\n",
    "\n",
    "        # --- 2. Calculate Prediction Interval (with Scipy) ---\n",
    "        # We do this inside the loop, one node at a time\n",
    "        try:\n",
    "            # Move params to numpy for scipy\n",
    "            mu_node = mu[:, 0, node, -1].cpu().numpy()\n",
    "            theta_node = theta[:, 0, node, -1].cpu().numpy()\n",
    "            pi_node = pi[:, 0, node, -1].cpu().numpy()\n",
    "\n",
    "            # A. Create the Negative Binomial component\n",
    "            # Scipy uses 'n' (total_count) and 'p' (prob)\n",
    "            n_scipy = np.maximum(theta_node, eps) # n = theta\n",
    "            p_scipy = n_scipy / (mu_node + n_scipy + eps) # p = theta / (mu + theta)\n",
    "            p_scipy = np.clip(p_scipy, eps, 1-eps)\n",
    "\n",
    "            # B. Calculate the total probability of zero\n",
    "            prob_zero = pi_node + (1 - pi_node) * sp_stats.nbinom.pmf(0, n=n_scipy, p=p_scipy)\n",
    "\n",
    "            # C. Define the quantiles we want\n",
    "            q_lower = 0.025\n",
    "            q_upper = 0.975\n",
    "\n",
    "            # D. Calculate lower bound\n",
    "            q_lower_adj = (q_lower - pi_node) / (1 - pi_node + eps)\n",
    "            q_lower_adj = np.clip(q_lower_adj, eps, 1-eps)\n",
    "            nb_quantile_lower = sp_stats.nbinom.ppf(q_lower_adj, n=n_scipy, p=p_scipy)\n",
    "            lower_bound = np.where(q_lower <= prob_zero, 0.0, nb_quantile_lower)\n",
    "\n",
    "            # E. Calculate upper bound\n",
    "            q_upper_adj = (q_upper - pi_node) / (1 - pi_node + eps)\n",
    "            q_upper_adj = np.clip(q_upper_adj, eps, 1-eps)\n",
    "            nb_quantile_upper = sp_stats.nbinom.ppf(q_upper_adj, n=n_scipy, p=p_scipy)\n",
    "            upper_bound = np.where(q_upper <= prob_zero, 0.0, nb_quantile_upper)\n",
    "\n",
    "            plot_interval = True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not compute prediction interval for node {node}. Plotting mean only. Error: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            plot_interval = False\n",
    "\n",
    "        # --- Plotting ---\n",
    "        # Plot Ground Truth\n",
    "        plt.plot(true_node, label='True', alpha=0.9, color='blue')\n",
    "\n",
    "        # Plot Predicted Mean\n",
    "        plt.plot(pred_node, label='Predicted (E[y])', alpha=0.9, color='orange', linestyle='--')\n",
    "\n",
    "        # Plot Prediction Interval\n",
    "        if plot_interval:\n",
    "            plt.fill_between(range(len(true_node)),\n",
    "                             lower_bound,\n",
    "                             upper_bound,\n",
    "                             color='orange',\n",
    "                             alpha=0.2,\n",
    "                             label='95% P.I.')\n",
    "\n",
    "        plt.title(f'Node {node} {names[node]} Traffic Prediction')\n",
    "        plt.xlabel('Sample Index')\n",
    "        plt.ylabel('Traffic Count')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "# --- YOUR NEW FUNCTION CALLS ---\n",
    "# Make sure to pass pi as the fourth argument\n",
    "print(\"--- Plotting Test Set ---\")\n",
    "plot_preds_and_ground_truth(test_targets, test_mu, test_theta, test_pi, id_to_name_map)\n",
    "print(\"--- Plotting Validation Set ---\")\n",
    "plot_preds_and_ground_truth(val_targets, val_mu, val_theta, val_pi, id_to_name_map)\n",
    "print(\"--- Plotting Training Set ---\")\n",
    "plot_preds_and_ground_truth(train_targets, train_mu, train_theta, train_pi, id_to_name_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".updated-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
