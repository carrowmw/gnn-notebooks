{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Iter 2 - GATZIPNLL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "For this iteration, we will change the simple MLP model to a Graph Attention Network (GAT) to allow information to flow between nodes. This should help the model better capture the spatial relationships in the traffic data. This model uses a zero-inflated poisson negative log likelihood loss function. Unlike the zero-inflated negative binomial loss function, the ZIP loss function does not account for overdispersion in the data (no theta parameter).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SCALER_MU = 5.887820243835449\n",
    "SCALER_SIGMA = 7.024876594543457"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = torch.load(\"data/processed/9f751fe859f07b5c/edge_index.pt\", weights_only=False)\n",
    "edge_attr_data = torch.load(\"data/processed/9f751fe859f07b5c/edge_attr.pt\", weights_only=False)\n",
    "static_features = torch.load(\"data/processed/9f751fe859f07b5c/static_features.pt\", weights_only=False)\n",
    "sensor_mask = torch.load(\"data/processed/9f751fe859f07b5c/sensor_mask.pt\", weights_only=False)\n",
    "train_loader = torch.load(\"data/processed/9f751fe859f07b5c/train_loader.pt\", weights_only=False)\n",
    "val_loader = torch.load(\"data/processed/9f751fe859f07b5c/val_loader.pt\", weights_only=False)\n",
    "test_loader = torch.load(\"data/processed/9f751fe859f07b5c/test_loader.pt\", weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"n batches:\", len(train_loader))\n",
    "for X, y in train_loader:\n",
    "    print(X.shape, y.shape)\n",
    "    print(\"Shape of tensor X: [batch_size, seq_len, num_nodes, num_features]\")\n",
    "    sample_window_a = X[0, :, 0, :]\n",
    "    sample_window_b = X[1, :, 0, :]\n",
    "    sample_window_c = X[2, :, 0, :]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_names = [\"value__mean_L12\",\n",
    "      \"value__std_L12\",\n",
    "      \"value__min_L12\",\n",
    "      \"value__max_L12\",\n",
    "      \"value__q25_L12\",\n",
    "      \"value__q75_L12\",\n",
    "      \"value__slope_L12\",\n",
    "      \"value__energy_L12\",\n",
    "      \"value__valid_frac_L12\",\n",
    "      \"value\"]\n",
    "fig, ax = plt.subplots(5,2,figsize=(12,16))\n",
    "for i, feature in enumerate(features_names):\n",
    "      ax[i//2, i%2].plot(sample_window_a[:, i], label=features_names[i])\n",
    "      ax[i//2, i%2].plot(sample_window_b[:, i], linestyle='--', label=f\"{features_names[i]} (Window B)\")\n",
    "      ax[i//2, i%2].plot(sample_window_c[:, i], linestyle=':', label=f\"{features_names[i]} (Window C)\")\n",
    "      ax[i//2, i%2].set_title(features_names[i])\n",
    "      ax[i//2, i%2].legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_hybrid_loader(loader, batch_size):\n",
    "    # Original X shape: [Batch, 12, 29, 10]\n",
    "    # Single iteration - collect all data first\n",
    "    all_batches = [(X, y) for X, y in loader]\n",
    "\n",
    "    # Temporal component (for LSTM)\n",
    "    # 1. Take all timesteps (12), all nodes, last feature (value)\n",
    "    # Shape: [Total_Samples, 12, 29, 1]\n",
    "    X_temporal_list = [(X[:, :, :, -1:]) for X, _ in all_batches]\n",
    "    X_temporal = torch.cat(X_temporal_list, dim=0)\n",
    "\n",
    "    # Spatial/Static Component for (GAT)\n",
    "    # Take first timestep (0:1), all nodes, features 0:-1 (aggregated stats) and the flattened value feature\n",
    "    # Shape: [Total_Samples, 1, 29, 21]\n",
    "\n",
    "\n",
    "    X_agg_list = [(X[:, 0:1, :, :-1]) for X, _ in all_batches]\n",
    "    X_raw_list = [(X[:, :, :, -1:].permute(0,3,2,1)) for X, _ in all_batches]\n",
    "    X_agg = torch.cat(X_agg_list, dim=0)\n",
    "    X_raw = torch.cat(X_raw_list, dim=0)\n",
    "    X_spatial = torch.cat([X_agg, X_raw], dim=3)\n",
    "\n",
    "    # 3. Targets\n",
    "    # Shape: [Total_Samples, 1, 29, 1]\n",
    "    y_list = [y[:, 0:1, :, :] for _, y in all_batches]\n",
    "    y_target = torch.cat(y_list, dim=0)\n",
    "\n",
    "    print(\"Temporal X shape:\", X_temporal.shape)\n",
    "    print(\"Spatial X shape:\", X_spatial.shape)\n",
    "    print(\"Target y shape:\", y_target.shape)\n",
    "\n",
    "    # Create a dataset that yields a tuple of inputs\n",
    "    dataset = torch.utils.data.TensorDataset(X_spatial, X_temporal, y_target)\n",
    "\n",
    "    # Shuffle should be True for training, False for validation/test\n",
    "    hybrid_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "    return hybrid_loader\n",
    "\n",
    "train_loader = prepare_hybrid_loader(train_loader, batch_size=16)\n",
    "val_loader = prepare_hybrid_loader(val_loader, batch_size=16)\n",
    "test_loader = prepare_hybrid_loader(test_loader, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def _finite_stats(name, t: torch.Tensor):\n",
    "    if t is None:\n",
    "        print(f\"[DEBUG] {name}: None\")\n",
    "        return False\n",
    "    is_finite = torch.isfinite(t)\n",
    "    if not is_finite.all():\n",
    "        n_nan = torch.isnan(t).sum().item()\n",
    "        n_inf = torch.isinf(t).sum().item()\n",
    "        print(f\"[NON-FINITE] {name}: nan={n_nan}, inf={n_inf}, shape={tuple(t.shape)}\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def _check(name, t:torch.Tensor):\n",
    "    ok = _finite_stats(name, t)\n",
    "    # if not ok:\n",
    "    #     # Early stop by rasing to surface the exact point\n",
    "    #     raise RuntimeError(f\"Non-finite tensor detected in {name}\")\n",
    "\n",
    "class SimpleNodeGATZIP(nn.Module):\n",
    "    def __init__(self, dynamic_node_dim, static_node_dim, edge_dim, n_embd, n_heads, edge_index, edge_attr, static_node_features, dropout_rate):\n",
    "        super().__init__()\n",
    "\n",
    "        dynamic_input_dim = dynamic_node_dim * 2  # Original feature + missing mask\n",
    "        static_input_dim = static_node_dim\n",
    "        gat1_input_channels = dynamic_input_dim + static_input_dim\n",
    "\n",
    "        # Layer 1: Input features to embedding dimensions (our \"node embedder\")\n",
    "        # Use concat=False so output dims remain n_embd when using multi-head attention\n",
    "        self.gat1 = GATv2Conv(in_channels=gat1_input_channels, out_channels=n_embd, edge_dim=edge_dim, heads=n_heads, concat=False, dropout=dropout_rate)\n",
    "        self.gat2 = GATv2Conv(in_channels=n_embd, out_channels=n_embd, edge_dim=edge_dim, heads=n_heads, concat=False, dropout=dropout_rate)\n",
    "        self.norm = nn.LayerNorm(n_embd)\n",
    "\n",
    "        # ZIP has 2 parameters per output (no theta like ZINB):\n",
    "        # 1. mu (mean/rate of Poisson)\n",
    "        # 2. pi (probability of zero-inflation)\n",
    "        self.mu_head = nn.Linear(n_embd, 1) # Mean (positive)\n",
    "        self.pi_head = nn.Linear(n_embd, 1) # Zero-inflation probability (0-1)\n",
    "\n",
    "        # Output layer takes the input of the 'embedder layer'\n",
    "        # self.output_layer = nn.Linear(n_embd, output_dim)\n",
    "        self.elu = nn.ELU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # Register Static Data as Buffers\n",
    "        # This makes them part of the model, moves them to CUDA with .to(device),\n",
    "        # and saves them in the state_dict, without being trainable.\n",
    "        self.register_buffer('edge_index', edge_index)\n",
    "        self.register_buffer('edge_attr', edge_attr)\n",
    "        self.register_buffer('static_node_features', static_node_features)\n",
    "\n",
    "    def forward(self, X_batch, targets, node_mask):\n",
    "\n",
    "        # X_batch: (Batch, 1, Num_Nodes, Input_Batch_Feature_Dim)\n",
    "        # targets: (Batch, 1, Num_Nodes, 1) - for training\n",
    "        # node_mask: (Num_Nodes) - boolean, True for nodes to include (e.g., sensor nodes)\n",
    "\n",
    "        mu_list, pi_list = [], []\n",
    "\n",
    "        missing_mask = torch.isnan(X_batch)\n",
    "        imputed_X = torch.nan_to_num(X_batch, nan=0.0)\n",
    "        _check(\"imputed_X\", imputed_X)\n",
    "        mask_features = missing_mask.float()\n",
    "        combined_input = torch.cat([imputed_X, mask_features], dim=-1)\n",
    "        _check(\"combined_input\", combined_input)\n",
    "\n",
    "        B = combined_input.shape[0]\n",
    "        Xb = combined_input[:,0,:,:]\n",
    "\n",
    "        for b in range(B):\n",
    "            combined_features = torch.cat([Xb[b], self.static_node_features], dim=-1)\n",
    "            xb = self.dropout(combined_features)\n",
    "            _check(\"dropout_out\", xb)\n",
    "            xb = self.gat1(xb, self.edge_index, self.edge_attr)\n",
    "            _check(\"gat1_out\", xb)\n",
    "            xb = self.norm(xb)\n",
    "            xb = self.elu(xb)\n",
    "            xb = self.dropout(xb)\n",
    "            xb = self.gat2(xb, self.edge_index, self.edge_attr)\n",
    "            _check(\"gat2_out\", xb)\n",
    "            xb = self.norm(xb)\n",
    "            xb = self.elu(xb)\n",
    "            xb = self.dropout(xb)\n",
    "\n",
    "            # Predict ZIP parameters (no theta for ZIP)\n",
    "            mu_b = torch.nn.functional.softplus(self.mu_head(xb)) + 1e-6 # Ensure positivity\n",
    "            pi_b = torch.sigmoid(self.pi_head(xb)) # Probability between 0 and 1\n",
    "            pi_b = torch.clamp(pi_b, min=1e-6, max=1-1e-6) # Avoid exact 0 or 1\n",
    "\n",
    "            _check(\"mu_b\", mu_b)\n",
    "            _check(\"pi_b\", pi_b)\n",
    "\n",
    "            mu_list.append(mu_b)\n",
    "            pi_list.append(pi_b)\n",
    "\n",
    "\n",
    "        # Unsqueeze at the 1 position to match expected output shape [B, 1, N, 1]\n",
    "        mu = torch.stack(mu_list, dim=0).unsqueeze(1)\n",
    "        pi = torch.stack(pi_list, dim=0).unsqueeze(1)\n",
    "\n",
    "        zip_nll_loss, valid_sum = self.zip_nll_loss(mu, pi, targets, node_mask)\n",
    "\n",
    "        # Return mu as point prediction for evaluation\n",
    "        preds = mu * (1 - pi) # [B, 1, N, 1]\n",
    "        mse_loss, _ = self.mse_loss(preds, targets, node_mask)\n",
    "        return preds, zip_nll_loss, mse_loss, {'mu': mu, 'pi': pi, 'valid_sum': valid_sum}\n",
    "\n",
    "    def zip_nll_loss(self, mu, pi, targets, node_mask):\n",
    "        \"\"\"\n",
    "        Zero-Inflated Poisson Negative Log-Likelihood Loss\n",
    "\n",
    "        Args:\n",
    "            mu: predicted mean/rate (batch_size, 1)\n",
    "            pi: zero-inflation probability (batch_size, 1)\n",
    "            targets: actual counts (batch_size, 1)\n",
    "            node_mask: boolean mask for which nodes to include\n",
    "        \"\"\"\n",
    "        eps = 1e-8\n",
    "\n",
    "        # Create NaN mask - True for valid (non-NaN) values\n",
    "        nan_mask = ~torch.isnan(targets)\n",
    "\n",
    "        # Combine with existing node_mask if provided\n",
    "        if node_mask is not None:\n",
    "            valid_mask = nan_mask & node_mask\n",
    "        else:\n",
    "            valid_mask = nan_mask\n",
    "\n",
    "        # Check if we have any valid values\n",
    "        if valid_mask.sum() == 0:\n",
    "            return torch.tensor(0.0, device=mu.device, requires_grad=True), torch.tensor(0.0, device=mu.device)\n",
    "\n",
    "        # Index only valid positions\n",
    "        mu_valid = mu[valid_mask]\n",
    "        pi_valid = pi[valid_mask]\n",
    "        targets_valid = targets[valid_mask]\n",
    "\n",
    "        _check(\"mu_valid\", mu_valid)\n",
    "        _check(\"pi_valid\", pi_valid)\n",
    "        _check(\"targets_valid\", targets_valid)\n",
    "\n",
    "        # Poisson log probability\n",
    "        # log p(y) = y*log(mu) - mu - log(y!)\n",
    "        poisson_log_prob = (\n",
    "            targets_valid * torch.log(mu_valid + eps)\n",
    "            - mu_valid\n",
    "            - torch.lgamma(targets_valid + 1)\n",
    "        )\n",
    "\n",
    "        # ZIP combine zero-inflation with Poisson\n",
    "        # p(y) = pi*I(y=0) + (1-pi)*Poisson(y)\n",
    "\n",
    "        zero_mask = (targets_valid < eps).float()\n",
    "\n",
    "        # For zero counts\n",
    "        # Probability that Poisson generates a 0 is exp(-mu)\n",
    "        # Log-probability is just -mu\n",
    "        poisson_zero_log_prob = -mu_valid\n",
    "\n",
    "        # log(pi + (1-pi)*exp(-mu))\n",
    "        zero_log_prob = torch.log(pi_valid + (1 - pi_valid) * torch.exp(poisson_zero_log_prob) + eps)\n",
    "\n",
    "        # For non-zero counts\n",
    "        # log((1-pi) * Poisson(y)) = log(1-pi) + log(Poisson(y))\n",
    "        non_zero_log_prob = torch.log(1 - pi_valid + eps) + poisson_log_prob\n",
    "\n",
    "        # Combine\n",
    "        log_prob = zero_mask * zero_log_prob + (1 - zero_mask) * non_zero_log_prob\n",
    "\n",
    "        # Mean over valid samples only\n",
    "        nll = -log_prob.mean()\n",
    "\n",
    "        return nll, valid_mask.sum()\n",
    "\n",
    "    def mse_loss(self, predictions, targets, node_mask):\n",
    "        # Create NaN mask - True for valid (non-NaN) values\n",
    "        nan_mask = ~torch.isnan(targets)\n",
    "\n",
    "        # Combine with existing node_mask if provided\n",
    "        if node_mask is not None:\n",
    "            valid_mask = nan_mask & node_mask\n",
    "        else:\n",
    "            valid_mask = nan_mask\n",
    "\n",
    "        # Check if we have any valid values\n",
    "        if valid_mask.sum() == 0:\n",
    "            return torch.tensor(0.0, device=predictions.device, requires_grad=True), 0\n",
    "\n",
    "        preds_valid = predictions[valid_mask]\n",
    "        targets_valid = targets[valid_mask]\n",
    "\n",
    "        mse_loss = ((targets_valid - preds_valid)**2).mean()\n",
    "\n",
    "        return mse_loss, valid_mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 32  # Embedding dimension for the MLP\n",
    "n_heads = 4\n",
    "output_dim = 1 # Predicting a single traffic value\n",
    "dropout = 0.1\n",
    "lr = 1e-4\n",
    "epochs = 40\n",
    "\n",
    "# device = torch.device('mps' if torch.backends.mps.is_available() else ('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "device = \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_static_input = static_features.to(device)\n",
    "edge_index_input = edge_index.to(device)\n",
    "edge_attr_input = edge_attr_data.to(device)\n",
    "\n",
    "sensor_mask_input = sensor_mask.to(device)\n",
    "\n",
    "num_nodes_input = static_features.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_nodes_dim = static_features.shape[1]\n",
    "edge_attr_dim = edge_attr_data.shape[1]\n",
    "print(\"Static Node Features Dim:\", static_nodes_dim)\n",
    "print(\"Edge Attribute Dim:\", edge_attr_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleNodeGATZIP(\n",
    "    dynamic_node_dim=21,\n",
    "    static_node_dim=static_nodes_dim,\n",
    "    edge_dim=edge_attr_dim,\n",
    "    n_embd=n_embd,\n",
    "    n_heads=n_heads,\n",
    "    dropout_rate=dropout,\n",
    "    edge_index=edge_index_input,\n",
    "    edge_attr=edge_attr_input,\n",
    "    static_node_features=X_static_input,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "print(f\"Model Iteration 1 Parameters: {sum(p.numel() for p in model.parameters())/1e3:.1f} K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot(steps, train_zip_nll, val_zip_nll, train_mse, val_mse, mu, pi):\n",
    "    # Append scalar values, not lists\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8,16))\n",
    "\n",
    "    ax1.clear()\n",
    "    ax2.clear()\n",
    "\n",
    "    ax1.plot(steps, train_zip_nll, label='Train ZIP NLL', marker='o')\n",
    "    ax1.plot(steps, val_zip_nll, label='Validation ZIP NLL', marker='o')\n",
    "    ax1.legend()\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('ZIP Training and Validation Loss Over Time')\n",
    "    ax1.grid(True)\n",
    "\n",
    "    ax2.plot(steps, train_mse, label='Train MSE', marker='x')\n",
    "    ax2.plot(steps, val_mse, label='Validation MSE', marker='x')\n",
    "    ax2.legend()\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.set_title('MSE Training and Validation Loss Over Time')\n",
    "    ax2.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "def format_params(params):\n",
    "    params_cat = {\"mu\":[], \"pi\": []}\n",
    "    for batch in params:\n",
    "        for key in params_cat.keys():\n",
    "            value = batch[key]\n",
    "            params_cat[key].append(value)\n",
    "\n",
    "    for key in params_cat.keys():\n",
    "        params_cat[key] = torch.cat(params_cat[key], dim=0)\n",
    "\n",
    "    # Return as tuple for easy unpacking (no theta for ZIP)\n",
    "    return params_cat[\"mu\"], params_cat[\"pi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training Loop Iteration 1 ---\n",
    "import time\n",
    "start_time = time.time()\n",
    "print(f\"Training started at {start_time}\")\n",
    "model.train()\n",
    "# For plotting\n",
    "avg_train_zip_nll = []\n",
    "avg_train_mse = []\n",
    "avg_val_zip_nll = []\n",
    "avg_val_mse = []\n",
    "avg_mu = []\n",
    "avg_pi = []\n",
    "for epoch in range(epochs):\n",
    "    epoch_zip_nll = 0\n",
    "    epoch_mse = 0\n",
    "    epoch_valid_samples = 0\n",
    "    epoch_total_samples = 0\n",
    "    num_batches = 0\n",
    "    val_epoch_zip_nll = 0\n",
    "    val_epoch_mse = 0\n",
    "    val_epoch_valid_samples = 0\n",
    "    val_epoch_total_samples = 0\n",
    "    val_num_batches = 0\n",
    "    epoch_params = []\n",
    "    for X_spatial, _, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        X_batch = X_spatial.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        # Track batch statistics\n",
    "        batch_size = y_batch.shape[0]\n",
    "        batch_total_samples = y_batch.numel() # Total elements in batch\n",
    "\n",
    "        # 4. Unnormalize the target variable\n",
    "        # y_raw = (y_batch_normalized * sigma) + mu\n",
    "        y_raw = (y_batch * SCALER_SIGMA) + SCALER_MU # Using your notebook's variable name\n",
    "        # 5. Round to nearest integer and cast to long\n",
    "        # This is ESSENTIAL for the ZIP loss function\n",
    "        y_raw_int = torch.round(y_raw).long()\n",
    "\n",
    "        predictions, zip_nll_loss, mse_loss, params = model(X_batch=X_batch, targets=y_raw_int, node_mask=None)\n",
    "\n",
    "        zip_nll_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate metrics\n",
    "        epoch_params.append(params)\n",
    "        epoch_zip_nll += zip_nll_loss.item()\n",
    "        epoch_mse += mse_loss.item()\n",
    "        epoch_valid_samples += params['valid_sum'].item()\n",
    "        epoch_total_samples += batch_total_samples\n",
    "        num_batches += 1\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_spatial, _, y_batch in val_loader:\n",
    "            X_batch = X_spatial.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            # 4. Unnormalize the target variable\n",
    "            # y_raw = (y_batch_normalized * sigma) + mu\n",
    "            y_raw = (y_batch * SCALER_SIGMA) + SCALER_MU # Using your notebook's variable name\n",
    "\n",
    "            # 5. Round to nearest integer and cast to long\n",
    "            # This is ESSENTIAL for the ZIP loss function\n",
    "            y_raw_int = torch.round(y_raw).long()\n",
    "\n",
    "            predictions, zip_nll_loss, mse_loss, params = model(X_batch=X_batch, targets=y_raw_int, node_mask=None)\n",
    "\n",
    "            val_epoch_zip_nll += zip_nll_loss.item()\n",
    "            val_epoch_mse += mse_loss.item()\n",
    "            val_epoch_valid_samples += params['valid_sum'].item()\n",
    "            val_epoch_total_samples += y_batch.numel()\n",
    "            val_num_batches += 1\n",
    "\n",
    "    avg_zip_nll = epoch_zip_nll / num_batches\n",
    "    avg_mse = epoch_mse / num_batches\n",
    "    valid_percentage = (epoch_valid_samples / epoch_total_samples) * 100\n",
    "    avg_valid_per_batch = epoch_valid_samples / num_batches\n",
    "\n",
    "    val_avg_zip_nll = val_epoch_zip_nll / val_num_batches\n",
    "    val_avg_mse = val_epoch_mse / val_num_batches\n",
    "    val_valid_percentage = (val_epoch_valid_samples / val_epoch_total_samples) * 100\n",
    "    val_avg_valid_per_batch = val_epoch_valid_samples / val_num_batches\n",
    "\n",
    "    if (epoch+1) % 10 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1:3d} | \"\n",
    "        f\"ZIP NLL: {avg_zip_nll:.4f} | \"\n",
    "        f\"MSE: {avg_mse:.4f} | \"\n",
    "        f\"Valid: {epoch_valid_samples}/{epoch_total_samples} ({valid_percentage:.1f}%) | \"\n",
    "        f\"Avg/Batch: {avg_valid_per_batch:.1f} | \"\n",
    "        f\"Batches: {num_batches}\")\n",
    "        print(f\"      VAL | \"\n",
    "        f\"ZIP NLL: {val_avg_zip_nll:.4f} | \"\n",
    "        f\"MSE: {val_avg_mse:.4f} | \"\n",
    "        f\"Valid: {val_epoch_valid_samples}/{val_epoch_total_samples} ({val_valid_percentage:.1f}%) | \"\n",
    "        f\"Avg/Batch: {val_avg_valid_per_batch:.1f} | \"\n",
    "        f\"Batches: {val_num_batches}\")\n",
    "\n",
    "    mu, pi = format_params(epoch_params)\n",
    "\n",
    "    avg_train_zip_nll.append(avg_zip_nll)\n",
    "    avg_val_zip_nll.append(val_avg_zip_nll)\n",
    "\n",
    "    avg_train_mse.append(avg_mse)\n",
    "    avg_val_mse.append(val_avg_mse)\n",
    "\n",
    "    avg_mu.append(mu.mean().item())\n",
    "    avg_pi.append(pi.mean().item())\n",
    "\n",
    "print(f\"Training completed at {time.time()}, duration: {time.time() - start_time:.2f}s\")\n",
    "steps = list(range(1, epochs+1))\n",
    "plot(steps, avg_train_zip_nll, avg_val_zip_nll, avg_train_mse, avg_val_mse, avg_mu, avg_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training, check gradient magnitudes\n",
    "for name, param in model.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        print(f\"{name}: grad norm = {param.grad.norm().item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced metrics with node-level tracking\n",
    "def detailed_evaluation(model, data_loader, device, split_name=\"Val\"):\n",
    "    model.eval()\n",
    "\n",
    "    # Track per-node validity (assuming shape [batch, 1, num_nodes, 1])\n",
    "    num_nodes = None\n",
    "    node_valid_counts = None\n",
    "    node_total_counts = None\n",
    "\n",
    "    total_zip_nll_loss = 0.0\n",
    "    total_mse_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    all_preds = []\n",
    "    all_params = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_spatial, _, y_batch in data_loader:\n",
    "            X_batch = X_spatial.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            if num_nodes is None:\n",
    "                num_nodes = y_batch.shape[2]\n",
    "                node_valid_counts = torch.zeros(num_nodes)\n",
    "                node_total_counts = torch.zeros(num_nodes)\n",
    "\n",
    "            # 4. Unnormalize the target variable\n",
    "            # y_raw = (y_batch_normalized * sigma) + mu\n",
    "            y_raw = (y_batch * SCALER_SIGMA) + SCALER_MU # Using your notebook's variable name\n",
    "\n",
    "            # 5. Round to nearest integer and cast to long\n",
    "            # This is ESSENTIAL for the ZIP loss function\n",
    "            y_raw_int = torch.round(y_raw).long()\n",
    "\n",
    "            preds, zip_nll_loss, mse_loss, params = model(X_batch=X_batch, targets=y_raw_int, node_mask=None)\n",
    "\n",
    "            all_preds.append(preds.cpu())\n",
    "            all_params.append(params)\n",
    "\n",
    "            if zip_nll_loss is not None:\n",
    "                total_zip_nll_loss += zip_nll_loss.item()\n",
    "                total_mse_loss += mse_loss.item()\n",
    "                num_batches += 1\n",
    "\n",
    "                # Count valid samples per node\n",
    "                nan_mask = ~torch.isnan(y_batch)  # [batch, 1, num_nodes, 1]\n",
    "                node_valid_counts += nan_mask.sum(dim=(0, 1, 3)).cpu()\n",
    "                node_total_counts += torch.ones_like(node_valid_counts) * y_batch.shape[0]\n",
    "\n",
    "    if num_batches > 0:\n",
    "        avg_zip_nll_loss = total_zip_nll_loss / num_batches\n",
    "        avg_mse_loss = total_mse_loss / num_batches\n",
    "\n",
    "        print(f\"\\n{split_name} Detailed Metrics:\")\n",
    "        print(f\"  Avg ZIP NLL Loss: {avg_zip_nll_loss:.4f}\")\n",
    "        print(f\"  Avg MSE Loss: {avg_mse_loss:.4f}\")\n",
    "        print(f\"  Total batches: {num_batches}\")\n",
    "\n",
    "        # Per-node statistics\n",
    "        node_valid_pct = (node_valid_counts / node_total_counts * 100)\n",
    "        print(f\"\\n  Node Validity Statistics:\")\n",
    "        print(f\"    Min: {node_valid_pct.min():.1f}%\")\n",
    "        print(f\"    Max: {node_valid_pct.max():.1f}%\")\n",
    "        print(f\"    Mean: {node_valid_pct.mean():.1f}%\")\n",
    "        print(f\"    Nodes with 100% valid: {(node_valid_pct == 100).sum().item()}/{num_nodes}\")\n",
    "        print(f\"    Nodes with <50% valid: {(node_valid_pct < 50).sum().item()}/{num_nodes}\")\n",
    "\n",
    "        return all_preds, all_params\n",
    "\n",
    "# Run after training\n",
    "train_results = detailed_evaluation(model, train_loader, device, \"Train\")\n",
    "val_results = detailed_evaluation(model, val_loader, device, \"Validation\")\n",
    "test_results = detailed_evaluation(model, test_loader, device, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds, train_params = train_results\n",
    "val_preds, val_params = val_results\n",
    "test_preds, test_params = test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mu, train_pi = format_params(train_params)\n",
    "val_mu, val_pi = format_params(val_params)\n",
    "test_mu, test_pi = format_params(test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy import stats as sp_stats # Import scipy.stats\n",
    "\n",
    "with open(\"data/processed/9f751fe859f07b5c/sensor_name_to_id_map.json\", \"r\") as f:\n",
    "    name_to_id_map = json.load(f)\n",
    "    id_to_name_map = {v: k for k, v in name_to_id_map.items()}\n",
    "\n",
    "def format_data_for_plotting(params_list, dataloader):\n",
    "    # 1. Unnormalize Ground Truth Targets\n",
    "    targets = []\n",
    "    for _, _, y in dataloader:\n",
    "        y_raw = (y * SCALER_SIGMA) + SCALER_MU\n",
    "        y_raw_int = torch.round(y_raw).long()\n",
    "        targets.append(y_raw_int)\n",
    "    cat_targets = torch.cat(targets, dim=0)\n",
    "\n",
    "    # 2. Concatenate Model Outputs (no theta for ZIP)\n",
    "    cat_mu, cat_pi = format_params(params_list)\n",
    "\n",
    "    return cat_targets, cat_mu, cat_pi\n",
    "\n",
    "# --- RE-RUN YOUR DATA FORMATTING ---\n",
    "train_targets, train_mu, train_pi = format_data_for_plotting(train_params, train_loader)\n",
    "val_targets, val_mu, val_pi = format_data_for_plotting(val_params, val_loader)\n",
    "test_targets, test_mu, test_pi = format_data_for_plotting(test_params, test_loader)\n",
    "\n",
    "\n",
    "# --- PLOTTING FUNCTION FOR ZIP ---\n",
    "\n",
    "def plot_preds_and_ground_truth(targets, mu, pi, names, max_points):\n",
    "    \"\"\"\n",
    "    Plots the ground truth against the predicted ZIP distribution.\n",
    "\n",
    "    - 'True': The actual ground truth counts.\n",
    "    - 'Predicted': The ZIP Expected Value, E[y] = (1-pi) * mu\n",
    "    - '95% P.I.': The 95% prediction interval (2.5th to 97.5th percentile)\n",
    "\n",
    "    For ZIP distribution:\n",
    "    - The distribution is a mixture: with probability pi, Y=0\n",
    "    - With probability (1-pi), Y ~ Poisson(mu)\n",
    "    \"\"\"\n",
    "\n",
    "    eps = 1e-8\n",
    "\n",
    "    # --- 1. Calculate the Expected Value (The \"Prediction\") ---\n",
    "    expected_value = (1 - pi) * mu\n",
    "\n",
    "    # --- 2. Plot ---\n",
    "    num_nodes = targets.shape[2]\n",
    "    for node in range(num_nodes):\n",
    "        plt.figure(figsize=(10, 4))\n",
    "\n",
    "        # Get data for the current node and move to CPU/NumPy\n",
    "        true_node = targets[:, 0, node, -1].cpu().numpy()\n",
    "        pred_node = expected_value[:, 0, node, -1].cpu().numpy()\n",
    "\n",
    "        # --- Calculate Prediction Interval for ZIP ---\n",
    "        try:\n",
    "            # Move params to numpy for scipy\n",
    "            mu_node = mu[:, 0, node, -1].cpu().numpy()\n",
    "            pi_node = pi[:, 0, node, -1].cpu().numpy()\n",
    "\n",
    "            # For ZIP: P(Y=0) = pi + (1-pi)*exp(-mu) (zero-inflation + Poisson zero)\n",
    "            # For ZIP: P(Y=k) = (1-pi) * Poisson(k; mu) for k > 0\n",
    "\n",
    "            # Probability of zero from the Poisson component\n",
    "            poisson_prob_zero = np.exp(-mu_node)\n",
    "\n",
    "            # Total probability of zero in ZIP\n",
    "            prob_zero = pi_node + (1 - pi_node) * poisson_prob_zero\n",
    "\n",
    "            # Define the quantiles we want\n",
    "            q_lower = 0.025\n",
    "            q_upper = 0.975\n",
    "\n",
    "            # For the lower bound:\n",
    "            # If q_lower <= prob_zero, then lower_bound = 0\n",
    "            # Otherwise, we need to find the quantile of the Poisson, adjusted for zero-inflation\n",
    "            # The adjusted quantile for Poisson: (q - pi) / (1 - pi)\n",
    "            q_lower_adj = (q_lower - pi_node) / (1 - pi_node + eps)\n",
    "            q_lower_adj = np.clip(q_lower_adj, eps, 1 - eps)\n",
    "            poisson_quantile_lower = sp_stats.poisson.ppf(q_lower_adj, mu=mu_node)\n",
    "            lower_bound = np.where(q_lower <= prob_zero, 0.0, poisson_quantile_lower)\n",
    "\n",
    "            # For the upper bound:\n",
    "            q_upper_adj = (q_upper - pi_node) / (1 - pi_node + eps)\n",
    "            q_upper_adj = np.clip(q_upper_adj, eps, 1 - eps)\n",
    "            poisson_quantile_upper = sp_stats.poisson.ppf(q_upper_adj, mu=mu_node)\n",
    "            upper_bound = np.where(q_upper <= prob_zero, 0.0, poisson_quantile_upper)\n",
    "\n",
    "            plot_interval = True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not compute prediction interval for node {node}. Plotting mean only. Error: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            plot_interval = False\n",
    "\n",
    "        # --- Plotting ---\n",
    "        # Plot Ground Truth\n",
    "        plt.plot(true_node[:max_points], label='True', alpha=0.9, color='blue')\n",
    "\n",
    "        # Plot Predicted Mean\n",
    "        plt.plot(pred_node[:max_points], label='Predicted (E[y])', alpha=0.9, color='orange', linestyle='--')\n",
    "\n",
    "        # Plot Prediction Interval\n",
    "        if plot_interval:\n",
    "            plt.fill_between(range(len(true_node[:max_points])),\n",
    "                             lower_bound[:max_points],\n",
    "                             upper_bound[:max_points],\n",
    "                             color='orange',\n",
    "                             alpha=0.2,\n",
    "                             label='95% P.I.')\n",
    "\n",
    "        plt.title(f'Node {node} {names[node]} Traffic Prediction')\n",
    "        plt.xlabel('Sample Index')\n",
    "        plt.ylabel('Traffic Count')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "# Make sure to pass only mu and pi (no theta)\n",
    "print(\"--- Plotting Test Set ---\")\n",
    "plot_preds_and_ground_truth(test_targets, test_mu, test_pi, id_to_name_map, 300)\n",
    "# print(\"--- Plotting Validation Set ---\")\n",
    "# plot_preds_and_ground_truth(val_targets, val_mu, val_pi, id_to_name_map)\n",
    "# print(\"--- Plotting Training Set ---\")\n",
    "# plot_preds_and_ground_truth(train_targets, train_mu, train_pi, id_to_name_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".updated-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
