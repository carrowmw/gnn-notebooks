{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Iter 1: MLPMSE Baseline Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "For this iteration, we implement a simple MLP baseline model that predicts traffic values based solely on static node features and time features. The model does not utilise any graph structure or temporal dependencies, serving as a foundational benchmark for future, more complex models. It is fed each timestep as a seperate feature. It uses Mean Squared Error (MSE) as the loss function which assumes Gaussian distribution which is not suitable for negative binomial distributed data. Static node features are not used in this iteration as there is no graph structure to leverage them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = torch.load(\"data/processed/processed_ac88600464456762_edge_index.pt\", weights_only=False)\n",
    "sensor_mask = torch.load(\"data/processed/processed_ac88600464456762_sensor_mask.pt\", weights_only=False)\n",
    "static_features = torch.load(\"data/processed/processed_ac88600464456762_static_features.pt\", weights_only=False)\n",
    "train_loader = torch.load(\"data/processed/processed_ac88600464456762_train_loader.pt\", weights_only=False)\n",
    "val_loader = torch.load(\"data/processed/processed_ac88600464456762_val_loader.pt\", weights_only=False)\n",
    "test_loader = torch.load(\"data/processed/processed_ac88600464456762_test_loader.pt\", weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"n batches:\", len(train_loader))\n",
    "for X, y in train_loader:\n",
    "    print(X.shape, y.shape)\n",
    "    print(\"Shape of tensor X: [batch_size, seq_len, num_nodes, num_features]\")\n",
    "    sample_window_a = X[0, :, 0, :]\n",
    "    sample_window_b = X[1, :, 0, :]\n",
    "    sample_window_c = X[2, :, 0, :]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_names = [\"value__mean_L12\",\n",
    "      \"value__std_L12\",\n",
    "      \"value__min_L12\",\n",
    "      \"value__max_L12\",\n",
    "      \"value__q25_L12\",\n",
    "      \"value__q75_L12\",\n",
    "      \"value__slope_L12\",\n",
    "      \"value__energy_L12\",\n",
    "      \"value__valid_frac_L12\",\n",
    "      \"value\"]\n",
    "fig, ax = plt.subplots(5,2,figsize=(12,16))\n",
    "for i, feature in enumerate(features_names):\n",
    "      ax[i//2, i%2].plot(sample_window_a[:, i], label=features_names[i])\n",
    "      ax[i//2, i%2].plot(sample_window_b[:, i], linestyle='--', label=f\"{features_names[i]} (Window B)\")\n",
    "      ax[i//2, i%2].plot(sample_window_c[:, i], linestyle=':', label=f\"{features_names[i]} (Window C)\")\n",
    "      ax[i//2, i%2].set_title(features_names[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This sections flattens the complex input structure for MLP input\n",
    "def flatten_loader(loader, batch_size):\n",
    "    # Only keep the first timestep for aggregated features\n",
    "    agg_loader = [(X[:, 0:1, :, :-1], y[:, 0:1, :, :]) for X, y in loader]\n",
    "    print(agg_loader[0][0].shape, agg_loader[0][1].shape)\n",
    "    raw_loader = [(X[:, :, :, -1:].permute(0,3,2,1), y[:, :1, :, :]) for X, y in loader]\n",
    "    print(raw_loader[0][0].shape, raw_loader[0][1].shape)\n",
    "    # Concatenate all batches into a single batch\n",
    "    flatten_X_agg = torch.cat([X for X, _ in agg_loader], dim=0)\n",
    "    flatten_X_raw = torch.cat([X for X, _ in raw_loader], dim=0)\n",
    "    flatten_X = torch.cat([flatten_X_agg, flatten_X_raw], dim=3)\n",
    "    flatten_y = torch.cat([y for _, y in agg_loader], dim=0)\n",
    "    print(\"New shape of tensor X:\", flatten_X.shape, \"tensor y:\", flatten_y.shape)\n",
    "    # Recreate DataLoader with flattened data\n",
    "    flat_dataset = torch.utils.data.TensorDataset(flatten_X, flatten_y)\n",
    "    flat_loader = torch.utils.data.DataLoader(flat_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "    print(\"New shape of DataLoader batches:\", next(iter(flat_loader))[0].shape, next(iter(flat_loader))[1].shape)\n",
    "    print(\"Length of new DataLoader:\", len(flat_loader))\n",
    "    return flat_loader\n",
    "\n",
    "train_loader = flatten_loader(train_loader, batch_size=16)\n",
    "val_loader = flatten_loader(val_loader, batch_size=16)\n",
    "test_loader = flatten_loader(test_loader, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class SimpleNodeMLPMSE(nn.Module):\n",
    "    def __init__(self, input_feature_dim, n_embd, dropout_rate):\n",
    "        super().__init__()\n",
    "        # Layer 1: Input features to embedding dimensions (our \"node embedder\")\n",
    "        self.embedder = nn.Linear(input_feature_dim * 2, n_embd)\n",
    "        self.hidden_layer = nn.Linear(n_embd, n_embd)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.output_layer = nn.Linear(n_embd, 1)\n",
    "\n",
    "    def forward(self, X_batch, targets, node_mask):\n",
    "        # X_batch: (Num_Nodes, Input_Batch_Feature_Dim)\n",
    "        # targets: (Num_Nodes, Output_Dim) - for training\n",
    "        # node_mask: (Num_Nodes) - boolean, True for nodes to include (e.g., sensor nodes)\n",
    "\n",
    "        missing_mask = torch.isnan(X_batch)\n",
    "        imputed_X = torch.nan_to_num(X_batch, nan=0.0)\n",
    "        mask_features = missing_mask.float()\n",
    "        combined_input = torch.cat([imputed_X, mask_features], dim=-1)\n",
    "\n",
    "        x = self.embedder(combined_input)\n",
    "        x = self.relu(x)\n",
    "        x = self.hidden_layer(x)\n",
    "        x = self.relu(x)\n",
    "        preds = self.output_layer(x)\n",
    "\n",
    "        loss, valid_sum = self.mse_loss(preds, targets, node_mask)\n",
    "\n",
    "        return preds, loss,  valid_sum\n",
    "\n",
    "    def mse_loss(self, preds, targets, node_mask):\n",
    "        \"\"\"\n",
    "        Standard MSE Loss\n",
    "        \"\"\"\n",
    "        # Creat NaN mask - True for valid (non-NaN) values\n",
    "        nan_mask = ~torch.isnan(targets)\n",
    "\n",
    "        # Combine with existing node_mask if provided\n",
    "        if node_mask is not None:\n",
    "            valid_mask = nan_mask & node_mask\n",
    "        else:\n",
    "            valid_mask = nan_mask\n",
    "\n",
    "        # Check if we have any valid values\n",
    "        if valid_mask.sum() == 0:\n",
    "            return torch.tensor(0.0, device=preds.device, requires_grad=True)\n",
    "\n",
    "        preds_valid = preds[valid_mask]\n",
    "        targets_valid = targets[valid_mask]\n",
    "\n",
    "        mse_loss = ((targets_valid - preds_valid)**2).mean()\n",
    "\n",
    "        return mse_loss, valid_mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 64  # Embedding dimension for the MLP\n",
    "output_dim = 1 # Predicting a single traffic value\n",
    "dropout = 0.1\n",
    "lr = 1e-4\n",
    "epochs = 100 # More epochs as the model is simple\n",
    "\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else ('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_static_input = static_features.to(device)\n",
    "\n",
    "sensor_mask_input = sensor_mask.to(device)\n",
    "\n",
    "num_nodes_input = static_features.shape[0]\n",
    "input_static_feature_dim_input = static_features.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleNodeMLPMSE(\n",
    "    input_feature_dim=21,\n",
    "    n_embd=n_embd,\n",
    "    dropout_rate=dropout\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "print(f\"Model Iteration 1 Parameters: {sum(p.numel() for p in model.parameters())/1e3:.1f} K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot(steps, train_loss, val_loss):\n",
    "    # Append scalar values, not lists\n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "    ax.plot(steps, train_loss, label='Train Loss', color='blue', marker='o')\n",
    "    ax.plot(steps, val_loss, label='Validation Loss', color='orange', marker='o')\n",
    "    ax.legend()\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title('Training and Validation Loss Over Time')\n",
    "    ax.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "def format_params(params):\n",
    "    params_cat = {\"mu\":[], \"theta\":[], \"pi\": []}\n",
    "    for batch in params:\n",
    "        for key in params_cat.keys():\n",
    "            value = batch[key]\n",
    "            params_cat[key].append(value)\n",
    "\n",
    "    for key in params_cat.keys():\n",
    "        params_cat[key] = torch.cat(params_cat[key], dim=0)\n",
    "\n",
    "    # Return as tuple for easy unpacking\n",
    "    return params_cat[\"mu\"], params_cat[\"theta\"], params_cat[\"pi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training Loop Iteration 1 ---\n",
    "import time\n",
    "start_time = time.time()\n",
    "print(f\"Training started at {start_time}\")\n",
    "model.train()\n",
    "# For plotting\n",
    "avg_train_losses = []\n",
    "avg_val_losses = []\n",
    "avg_mu = []\n",
    "avg_theta = []\n",
    "avg_pi = []\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    epoch_valid_samples = 0\n",
    "    epoch_total_samples = 0\n",
    "    num_batches = 0\n",
    "    val_epoch_loss = 0\n",
    "    val_epoch_valid_samples = 0\n",
    "    val_epoch_total_samples = 0\n",
    "    val_num_batches = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        # Track batch statistics\n",
    "        batch_size = y_batch.shape[0]\n",
    "        batch_total_samples = y_batch.numel() # Total elements in batch\n",
    "\n",
    "        mu = 6.003180503845215\n",
    "        sigma = 6.975292682647705\n",
    "\n",
    "        # 4. Unnormalize the target variable\n",
    "        # y_raw = (y_batch_normalized * sigma) + mu\n",
    "        y_raw = (y_batch * sigma) + mu # Using your notebook's variable name\n",
    "\n",
    "        # 5. Round to nearest integer and cast to long\n",
    "        # This is ESSENTIAL for the ZINB loss function\n",
    "        y_raw_int = torch.round(y_raw).long()\n",
    "\n",
    "        predictions, loss, valid_sum = model(X_batch=X_batch, targets=y_raw_int, node_mask=None)\n",
    "\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate metrics\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_valid_samples += valid_sum\n",
    "        epoch_total_samples += batch_total_samples\n",
    "        num_batches += 1\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            mu = 6.003180503845215\n",
    "            sigma = 6.975292682647705\n",
    "\n",
    "            # 4. Unnormalize the target variable\n",
    "            # y_raw = (y_batch_normalized * sigma) + mu\n",
    "            y_raw = (y_batch * sigma) + mu # Using your notebook's variable name\n",
    "\n",
    "            # 5. Round to nearest integer and cast to long\n",
    "            # This is ESSENTIAL for the ZINB loss function\n",
    "            y_raw_int = torch.round(y_raw).long()\n",
    "\n",
    "            predictions, loss, valid_sum = model(X_batch=X_batch, targets=y_raw_int, node_mask=None)\n",
    "\n",
    "            val_epoch_loss += loss.item()\n",
    "            val_epoch_valid_samples += valid_sum\n",
    "            val_epoch_total_samples += y_batch.numel()\n",
    "            val_num_batches += 1\n",
    "\n",
    "    avg_loss = epoch_loss / num_batches\n",
    "    valid_percentage = (epoch_valid_samples / epoch_total_samples) * 100\n",
    "    avg_valid_per_batch = epoch_valid_samples / num_batches\n",
    "\n",
    "    val_avg_loss = val_epoch_loss / val_num_batches\n",
    "    val_valid_percentage = (val_epoch_valid_samples / val_epoch_total_samples) * 100\n",
    "    val_avg_valid_per_batch = val_epoch_valid_samples / val_num_batches\n",
    "\n",
    "    if (epoch+1) % 10 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1:3d} | \"\n",
    "        f\"Loss: {avg_loss:.4f} | \"\n",
    "        f\"Valid: {epoch_valid_samples}/{epoch_total_samples} ({valid_percentage:.1f}%) | \"\n",
    "        f\"Avg/Batch: {avg_valid_per_batch:.1f} | \"\n",
    "        f\"Batches: {num_batches}\")\n",
    "        print(f\"      VAL | \"\n",
    "        f\"Loss: {val_avg_loss:.4f} | \"\n",
    "        f\"Valid: {val_epoch_valid_samples}/{val_epoch_total_samples} ({val_valid_percentage:.1f}%) | \"\n",
    "        f\"Avg/Batch: {val_avg_valid_per_batch:.1f} | \"\n",
    "        f\"Batches: {val_num_batches}\")\n",
    "\n",
    "    avg_train_losses.append(avg_loss)\n",
    "    avg_val_losses.append(val_avg_loss)\n",
    "\n",
    "print(f\"Training completed at {time.time()}, duration: {time.time() - start_time:.2f}s\")\n",
    "steps = list(range(1, epochs+1))\n",
    "plot(steps, avg_train_losses, avg_val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced metrics with node-level tracking\n",
    "def detailed_evaluation(model, data_loader, device, split_name=\"Val\"):\n",
    "    model.eval()\n",
    "\n",
    "    # Track per-node validity (assuming shape [batch, 1, num_nodes, 1])\n",
    "    num_nodes = None\n",
    "    node_valid_counts = None\n",
    "    node_total_counts = None\n",
    "\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    all_preds = []\n",
    "    params = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            if num_nodes is None:\n",
    "                num_nodes = y_batch.shape[2]\n",
    "                node_valid_counts = torch.zeros(num_nodes)\n",
    "                node_total_counts = torch.zeros(num_nodes)\n",
    "\n",
    "            mu = 6.003180503845215\n",
    "            sigma = 6.975292682647705\n",
    "\n",
    "            # 4. Unnormalize the target variable\n",
    "            # y_raw = (y_batch_normalized * sigma) + mu\n",
    "            y_raw = (y_batch * sigma) + mu # Using your notebook's variable name\n",
    "\n",
    "            # 5. Round to nearest integer and cast to long\n",
    "            # This is ESSENTIAL for the ZINB loss function\n",
    "            y_raw_int = torch.round(y_raw).long()\n",
    "\n",
    "            preds, loss, param = model(X_batch=X_batch, targets=y_raw_int, node_mask=None)\n",
    "\n",
    "            all_preds.append(preds.cpu())\n",
    "            params.append(param)\n",
    "\n",
    "            if loss is not None:\n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "\n",
    "                # Count valid samples per node\n",
    "                nan_mask = ~torch.isnan(y_batch)  # [batch, 1, num_nodes, 1]\n",
    "                node_valid_counts += nan_mask.sum(dim=(0, 1, 3)).cpu()\n",
    "                node_total_counts += torch.ones_like(node_valid_counts) * y_batch.shape[0]\n",
    "\n",
    "    if num_batches > 0:\n",
    "        avg_loss = total_loss / num_batches\n",
    "\n",
    "        print(f\"\\n{split_name} Detailed Metrics:\")\n",
    "        print(f\"  Avg Loss: {avg_loss:.4f}\")\n",
    "        print(f\"  Total batches: {num_batches}\")\n",
    "\n",
    "        # Per-node statistics\n",
    "        node_valid_pct = (node_valid_counts / node_total_counts * 100)\n",
    "        print(f\"\\n  Node Validity Statistics:\")\n",
    "        print(f\"    Min: {node_valid_pct.min():.1f}%\")\n",
    "        print(f\"    Max: {node_valid_pct.max():.1f}%\")\n",
    "        print(f\"    Mean: {node_valid_pct.mean():.1f}%\")\n",
    "        print(f\"    Nodes with 100% valid: {(node_valid_pct == 100).sum().item()}/{num_nodes}\")\n",
    "        print(f\"    Nodes with <50% valid: {(node_valid_pct < 50).sum().item()}/{num_nodes}\")\n",
    "\n",
    "        return all_preds, params\n",
    "\n",
    "# Run after training\n",
    "train_results = detailed_evaluation(model, train_loader, device, \"Train\")\n",
    "val_results = detailed_evaluation(model, val_loader, device, \"Validation\")\n",
    "test_results = detailed_evaluation(model, test_loader, device, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds, train_params = train_results\n",
    "val_preds, val_params = val_results\n",
    "test_preds, test_params = test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data_for_plotting(preds, dataloader):\n",
    "    targets = []\n",
    "    for _, y in dataloader:\n",
    "        targets.append(y)\n",
    "    cat_targets = torch.cat(targets, dim=0)\n",
    "    cat_preds = torch.cat(preds, dim=0)\n",
    "    return cat_targets, cat_preds\n",
    "\n",
    "train_targets, train_preds = format_data_for_plotting(train_preds, train_loader)\n",
    "val_targets, val_preds = format_data_for_plotting(val_preds, val_loader)\n",
    "test_targets, test_preds = format_data_for_plotting(test_preds, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_preds_and_ground_truth(targets, preds):\n",
    "    num_nodes = targets.shape[2]\n",
    "    for node in range(num_nodes):\n",
    "        # fig, (ax1, ax2) =\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(targets[:, 0, node, :].cpu().numpy(), label='True', alpha=0.7)\n",
    "        plt.plot(preds[:, 0, node, :].cpu().numpy(), label='Predicted', alpha=0.7)\n",
    "\n",
    "        plt.title(f'Node {node} Traffic Prediction')\n",
    "        plt.xlabel('Sample Index')\n",
    "        plt.ylabel('Traffic Count')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "plot_preds_and_ground_truth(test_targets, test_preds)\n",
    "plot_preds_and_ground_truth(val_targets, val_preds)\n",
    "plot_preds_and_ground_truth(train_targets, train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".updated-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
