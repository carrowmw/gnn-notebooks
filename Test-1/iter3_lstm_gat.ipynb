{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Iter 3 - LSTMGATZINBNLL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "For this iteration, we will change the GAT model to a LSTM GAT model to give the model temporal memory. This should help the model better capture the long range temporal dependencies in the traffic data. In this model, the LSTM embeddings are passed to the GAT layer directly along with the spatial graph data and aggregated temporal features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = torch.load(\"data/processed/processed_ac88600464456762_edge_index.pt\", weights_only=False)\n",
    "sensor_mask = torch.load(\"data/processed/processed_ac88600464456762_sensor_mask.pt\", weights_only=False)\n",
    "static_features = torch.load(\"data/processed/processed_ac88600464456762_static_features.pt\", weights_only=False)\n",
    "static_graph_data = torch.load(\"data/processed/custom_ac88600464456762_gabriel_graph_data.pt\", weights_only=False)\n",
    "train_loader = torch.load(\"data/processed/processed_ac88600464456762_train_loader.pt\", weights_only=False)\n",
    "val_loader = torch.load(\"data/processed/processed_ac88600464456762_val_loader.pt\", weights_only=False)\n",
    "test_loader = torch.load(\"data/processed/processed_ac88600464456762_test_loader.pt\", weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"n batches:\", len(train_loader))\n",
    "for X, y in train_loader:\n",
    "    print(X.shape, y.shape)\n",
    "    print(\"Shape of tensor X: [batch_size, seq_len, num_nodes, num_features]\")\n",
    "    sample_window_a = X[0, :, 0, :]\n",
    "    sample_window_b = X[1, :, 0, :]\n",
    "    sample_window_c = X[2, :, 0, :]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_names = [\"value__mean_L12\",\n",
    "      \"value__std_L12\",\n",
    "      \"value__min_L12\",\n",
    "      \"value__max_L12\",\n",
    "      \"value__q25_L12\",\n",
    "      \"value__q75_L12\",\n",
    "      \"value__slope_L12\",\n",
    "      \"value__energy_L12\",\n",
    "      \"value__valid_frac_L12\",\n",
    "      \"value\"]\n",
    "fig, ax = plt.subplots(5,2,figsize=(12,16))\n",
    "for i, feature in enumerate(features_names):\n",
    "      ax[i//2, i%2].plot(sample_window_a[:, i], label=features_names[i])\n",
    "      ax[i//2, i%2].plot(sample_window_b[:, i], linestyle='--', label=f\"{features_names[i]} (Window B)\")\n",
    "      ax[i//2, i%2].plot(sample_window_c[:, i], linestyle=':', label=f\"{features_names[i]} (Window C)\")\n",
    "      ax[i//2, i%2].set_title(features_names[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_hybrid_loader(loader, batch_size):\n",
    "    # Original X shape: [Batch, 12, 29, 10]\n",
    "    # Single iteration - collect all data first\n",
    "    all_batches = [(X, y) for X, y in loader]\n",
    "\n",
    "    # Temporal component (for LSTM)\n",
    "    # 1. Take all timesteps (12), all nodes, last feature (value)\n",
    "    # Shape: [Total_Samples, 12, 29, 1]\n",
    "    X_temporal_list = [(X[:, :, :, -1:]) for X, _ in all_batches]\n",
    "    X_temporal = torch.cat(X_temporal_list, dim=0)\n",
    "\n",
    "    # Spatial/Static Component for (GAT)\n",
    "    # Take first timestep (0:1), all nodes, features 0:-1 (aggregated stats) and the flattened value feature\n",
    "    # Shape: [Total_Samples, 1, 29, 21]\n",
    "\n",
    "\n",
    "    X_agg_list = [(X[:, 0:1, :, :-1]) for X, _ in all_batches]\n",
    "    X_raw_list = [(X[:, :, :, -1:].permute(0,3,2,1)) for X, _ in all_batches]\n",
    "    X_agg = torch.cat(X_agg_list, dim=0)\n",
    "    X_raw = torch.cat(X_raw_list, dim=0)\n",
    "    X_spatial = torch.cat([X_agg, X_raw], dim=3)\n",
    "\n",
    "    # 3. Targets\n",
    "    # Shape: [Total_Samples, 1, 29, 1]\n",
    "    y_list = [y[:, 0:1, :, :] for _, y in all_batches]\n",
    "    y_target = torch.cat(y_list, dim=0)\n",
    "\n",
    "    print(\"Temporal X shape:\", X_temporal.shape)\n",
    "    print(\"Spatial X shape:\", X_spatial.shape)\n",
    "    print(\"Target y shape:\", y_target.shape)\n",
    "\n",
    "    # Create a dataset that yields a tuple of inputs\n",
    "    dataset = torch.utils.data.TensorDataset(X_spatial, X_temporal, y_target)\n",
    "\n",
    "    # Shuffle should be True for training, False for validation/test\n",
    "    hybrid_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "    return hybrid_loader\n",
    "\n",
    "train_loader = prepare_hybrid_loader(train_loader, batch_size=16)\n",
    "val_loader = prepare_hybrid_loader(val_loader, batch_size=16)\n",
    "test_loader = prepare_hybrid_loader(test_loader, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def _finite_stats(name, t: torch.Tensor):\n",
    "    if t is None:\n",
    "        print(f\"[DEBUG] {name}: None\")\n",
    "        return False\n",
    "    is_finite = torch.isfinite(t)\n",
    "    if not is_finite.all():\n",
    "        n_nan = torch.isnan(t).sum().item()\n",
    "        n_inf = torch.isinf(t).sum().item()\n",
    "        print(f\"[NON-FINITE] {name}: nan={n_nan}, inf={n_inf}, shape={tuple(t.shape)}\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def _check(name, t:torch.Tensor):\n",
    "    ok = _finite_stats(name, t)\n",
    "    if not ok:\n",
    "        # Early stop by rasing to surface the exact point\n",
    "        raise RuntimeError(f\"Non-finite tensor detected in {name}\")\n",
    "\n",
    "class LSTMGATZINB(nn.Module):\n",
    "    def __init__(self, dynamic_node_dim, static_node_dim, edge_dim, n_embd, n_heads, static_graph_data, dropout_rate):\n",
    "        super().__init__()\n",
    "\n",
    "        # 1. LSTM Layer (Process Temporal Context)\n",
    "        # Input: Value (1) * 2 for missing mask\n",
    "        lstm_input_channels = 1 * 2\n",
    "        self.lstm = nn.LSTM(input_size=lstm_input_channels, hidden_size=n_embd, batch_first=True)\n",
    "        self.norm_lstm = nn.LayerNorm(n_embd)\n",
    "\n",
    "        # 2. GAT Layers (Process Spatial + Temporal Context)\n",
    "        # Input to GAT:\n",
    "        #   LSTM Output (n_embd)\n",
    "        # + Spatial  Features (dynamic_node_dim * 2 for missing mask)\n",
    "        # + Static   Features (static_node_dim)\n",
    "        gat_input_dim = n_embd + (dynamic_node_dim * 2) + static_node_dim\n",
    "        # Use concat=False so output dims remain n_embd when using multi-head attention\n",
    "        self.gat1 = GATv2Conv(in_channels=gat_input_dim, out_channels=n_embd, edge_dim=edge_dim, heads=n_heads, concat=False, dropout=dropout_rate)\n",
    "        self.gat2 = GATv2Conv(in_channels=n_embd, out_channels=n_embd, edge_dim=edge_dim, heads=n_heads, concat=False, dropout=dropout_rate)\n",
    "        self.norm_gat = nn.LayerNorm(n_embd)\n",
    "\n",
    "        # 3. Prediction Heads (ZINB Parameters)\n",
    "        # Input is just the GAT output\n",
    "        self.mu_head = nn.Linear(n_embd, 1) # Mean (positive)\n",
    "        self.theta_head = nn.Linear(n_embd, 1) # Dispersion (positive)\n",
    "        self.pi_head = nn.Linear(n_embd, 1) # Zero-inflation probability (0-1)\n",
    "\n",
    "        # ELU and dropout do not have learnable parameters so we can reuse them\n",
    "        self.elu = nn.ELU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # Register Static Data as Buffers\n",
    "        # This makes them part of the model, moves them to CUDA with .to(device),\n",
    "        # and saves them in the state_dict, without being trainable.\n",
    "        self.register_buffer('edge_index', static_graph_data.edge_index)\n",
    "        self.register_buffer('edge_attr', static_graph_data.edge_attr)\n",
    "        self.register_buffer('static_node_features', static_graph_data.x)\n",
    "\n",
    "    def forward(self, X_spatial, X_temporal, targets, node_mask):\n",
    "        # X_spatial:  [Batch, 1, Num_Nodes, Spatial_Feats] (Aggregated/Flattened info)\n",
    "        # X_temporal: [Batch, Seq_Len, Num_Nodes, 1]\n",
    "\n",
    "        B, T, N, _ = X_temporal.shape\n",
    "\n",
    "        # ---- 1. LSTM: Extract Temporal Features ----\n",
    "        # Prepare LSTM input (Value + Missing Mask)\n",
    "        missing_mask_temporal = torch.isnan(X_temporal)\n",
    "        imputed_X_temporal = torch.nan_to_num(X_temporal, nan=0.0)\n",
    "        mask_features_temporal = missing_mask_temporal.float()\n",
    "\n",
    "        # [B, T, N, 2]\n",
    "        lstm_input_seq = torch.cat([imputed_X_temporal, mask_features_temporal], dim=-1)\n",
    "\n",
    "        # Reshape for LSTM: treat every node in the batch as a separate sequence\n",
    "        # [B, T, N, F] -> [B, N, T, F] -> [B*N, T, F]\n",
    "        lstm_input_flat = lstm_input_seq.permute(0, 2, 1, 3).reshape(B * N, T, -1)\n",
    "\n",
    "        # Run LSTM\n",
    "        # out: [B*N, T, n_embd], (h_n, c_n)\n",
    "        lstm_out, _ = self.lstm(lstm_input_flat)\n",
    "\n",
    "        # Take the output of the last timestep\n",
    "        # last_emb_flat: [B*N, n_embd]\n",
    "        last_emb_flat = lstm_out[:, -1, :]\n",
    "\n",
    "        # Reshape back to [B, N, n_embd]\n",
    "        lstm_emb = last_emb_flat.reshape(B, N, -1)\n",
    "\n",
    "        # Normalize LSTM output before feeding to GAT\n",
    "        lstm_emb = self.norm_lstm(lstm_emb)\n",
    "        lstm_emb = self.elu(lstm_emb)\n",
    "        lstm_emb = self.dropout(lstm_emb)\n",
    "\n",
    "        # --- 2. GAT: Spatial Missing ---\n",
    "        # Prepare Spatial Input (Stats + Flattened Value + Missing Mask)\n",
    "        # Squeeze the singleton time dim from spatial input\n",
    "        X_spatial_squeeze = X_spatial.squeeze(1) # [B, N, F]\n",
    "        missing_mask_spatial = torch.isnan(X_spatial_squeeze)\n",
    "        imputed_X = torch.nan_to_num(X_spatial_squeeze, nan=0.0)\n",
    "        mask_features = missing_mask_spatial.float()\n",
    "\n",
    "        # [B, N, F + F]\n",
    "        spatial_feat = torch.cat([imputed_X, mask_features], dim=-1)\n",
    "\n",
    "        spatial_embeddings_list = []\n",
    "        for b in range(B):\n",
    "            # Combine EVERYTHING:\n",
    "            # LSTM Context (n_embd) + Statistical Context (dynamic_node_dim * 2) + Static Context (static_node_dim)\n",
    "            node_features = torch.cat([lstm_emb[b], spatial_feat[b], self.static_node_features], dim=-1)\n",
    "\n",
    "            xb = self.dropout(node_features)\n",
    "            xb = self.gat1(xb, self.edge_index, self.edge_attr)\n",
    "            xb = self.norm_gat(xb)\n",
    "            xb = self.elu(xb)\n",
    "            xb = self.dropout(xb)\n",
    "            xb = self.gat2(xb, self.edge_index, self.edge_attr)\n",
    "            xb = self.norm_gat(xb)\n",
    "            spatial_embeddings_list.append(xb)\n",
    "\n",
    "        # Stack back to batch: [B, Num_Nodes, n_embd]\n",
    "        final_emb = torch.stack(spatial_embeddings_list, dim=0)\n",
    "\n",
    "        # ---- 4. Prediction Heads----\n",
    "        mu = torch.nn.functional.softplus(self.mu_head(final_emb)) + 1e-6 # Ensure positivity\n",
    "        theta = torch.nn.functional.softplus(self.theta_head(final_emb)) + 1e-6 # Ensure positivity\n",
    "        pi = torch.sigmoid(self.pi_head(final_emb)) # Probability between 0 and 1\n",
    "        pi = torch.clamp(pi, min=1e-6, max=1-1e-6) # Avoid exact 0 or 1\n",
    "\n",
    "        # Add a singleton dimension for compatibility with the loss function [B, 1, N, 1]\n",
    "        mu = mu.unsqueeze(1)\n",
    "        theta = theta.unsqueeze(1)\n",
    "        pi = pi.unsqueeze(1)\n",
    "\n",
    "        zinb_nll_loss, valid_sum = self.zinb_nll_loss(mu, theta, pi, targets, node_mask)\n",
    "        # valid_sum = torch.tensor(total_valid, device=mu.device, dtype=torch.float32)\n",
    "\n",
    "        # Return mu as point prediction for evaluation\n",
    "        preds = mu * (1 - pi) # [B, 1, N, 1]\n",
    "        mse_loss, _ = self.mse_loss(preds, targets, node_mask)\n",
    "        return preds, zinb_nll_loss, mse_loss, {'mu': mu, 'theta': theta, 'pi': pi, 'valid_sum': valid_sum}\n",
    "\n",
    "    def zinb_nll_loss(self, mu, theta, pi, targets, node_mask):\n",
    "        \"\"\"\n",
    "        Zero-Inflated Negative Binomial Negative Log-Likelihood Loss\n",
    "\n",
    "        Args:\n",
    "            mu: predicted mean (batch_size, 1)\n",
    "            theta: dispersion parameter (batch_size, 1)\n",
    "            pi: zero-inflation probability (batch_size, 1)\n",
    "            targets: actual counts (batch_size, 1)\n",
    "            node_mask: boolean mask for which nodes to include\n",
    "        \"\"\"\n",
    "        eps = 1e-8\n",
    "\n",
    "        # Creat NaN mask - True for valid (non-NaN) values\n",
    "        nan_mask = ~torch.isnan(targets)\n",
    "\n",
    "        # Combine with existing node_mask if provided\n",
    "        if node_mask is not None:\n",
    "            valid_mask = nan_mask & node_mask\n",
    "        else:\n",
    "            valid_mask = nan_mask\n",
    "\n",
    "        # Check if we have any valid values\n",
    "        if valid_mask.sum() == 0:\n",
    "            return torch.tensor(0.0, device=mu.device, requires_grad=True), torch.tensor(0.0, device=mu.device)\n",
    "\n",
    "        # Index only valid positions\n",
    "        mu_valid = mu[valid_mask]\n",
    "        theta_valid = theta[valid_mask]\n",
    "        pi_valid = pi[valid_mask]\n",
    "        targets_valid = targets[valid_mask]\n",
    "\n",
    "        _check(\"mu_valid\", mu_valid)\n",
    "        _check(\"theta_valid\", theta_valid)\n",
    "        _check(\"pi_valid\", pi_valid)\n",
    "        _check(\"targets_valid\", targets_valid)\n",
    "\n",
    "        # NB log probability\n",
    "        # log p(y(NB)) = log Gamma(theta+y) - log Gamma(theta) - log Gamma(y+1)\n",
    "        #               + theta*log(theta) - theta*log(theta + mu)\n",
    "        #               + y*log(mu) - y*log(theta + mu)\n",
    "\n",
    "        theta_mu = theta_valid + mu_valid\n",
    "\n",
    "        _check(\"theta_mu\", theta_mu)\n",
    "\n",
    "        # Using lgamma for numerical stability\n",
    "        nb_log_prob = (\n",
    "            torch.lgamma(theta_valid + targets_valid + eps)\n",
    "            - torch.lgamma(theta_valid + eps)\n",
    "            - torch.lgamma(targets_valid + 1)\n",
    "            + theta_valid * torch.log(theta_valid + eps)\n",
    "            - theta_valid * torch.log(theta_mu + eps)\n",
    "            + targets_valid * torch.log(mu_valid + eps)\n",
    "            - targets_valid * torch.log(theta_mu + eps)\n",
    "        )\n",
    "\n",
    "        # ZINB combine zero-inflation with NB\n",
    "        # p(y) = pi*I(y=0) + (1-pi)*NB(y)\n",
    "        # For y=0: log p(0) = log(pi + (1-pi)*NB(0))\n",
    "        # For y>0: log p(y) = log(1-pi) + log NB(y)\n",
    "\n",
    "        zero_mask = (targets_valid < eps).float()\n",
    "\n",
    "        # For zero counts\n",
    "        nb_zero_prob = theta_valid * torch.log(theta_valid / (theta_mu + eps))\n",
    "        zero_log_prob = torch.log(pi_valid + (1 - pi_valid) * torch.exp(nb_zero_prob) + eps)\n",
    "\n",
    "        # For non-zer counts\n",
    "        non_zero_log_prob = torch.log(1 - pi_valid + eps) + nb_log_prob\n",
    "\n",
    "        # Combine\n",
    "        log_prob = zero_mask * zero_log_prob + (1 - zero_mask) * non_zero_log_prob\n",
    "\n",
    "        # Mean over valid samples only\n",
    "        nll = -log_prob.mean()\n",
    "\n",
    "        return nll, valid_mask.sum()\n",
    "\n",
    "    def mse_loss(self, predictions, targets, node_mask):\n",
    "        # Create NaN mask - True for valid (non-NaN) values\n",
    "        nan_mask = ~torch.isnan(targets)\n",
    "\n",
    "        # Combine with existing node_mask if provided\n",
    "        if node_mask is not None:\n",
    "            valid_mask = nan_mask & node_mask\n",
    "        else:\n",
    "            valid_mask = nan_mask\n",
    "\n",
    "        # Check if we have any valid values\n",
    "        if valid_mask.sum() == 0:\n",
    "            return torch.tensor(0.0, device=predictions.device, requires_grad=True), 0\n",
    "\n",
    "        preds_valid = predictions[valid_mask]\n",
    "        targets_valid = targets[valid_mask]\n",
    "\n",
    "        mse_loss = ((targets_valid - preds_valid)**2).mean()\n",
    "\n",
    "        return mse_loss, valid_mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 16  # Embedding dimension for the MLP\n",
    "n_heads = 4\n",
    "output_dim = 1 # Predicting a single traffic value\n",
    "dropout = 0.1\n",
    "lr = 1e-3\n",
    "epochs = 100 # More epochs as the model is simple\n",
    "\n",
    "# device = torch.device('mps' if torch.backends.mps.is_available() else ('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "device = \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_static_input = static_features.to(device)\n",
    "\n",
    "sensor_mask_input = sensor_mask.to(device)\n",
    "\n",
    "num_nodes_input = static_features.shape[0]\n",
    "\n",
    "static_nodes_dim = static_graph_data.x.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMGATZINB(\n",
    "    dynamic_node_dim=21,\n",
    "    static_node_dim=11,\n",
    "    edge_dim=3,\n",
    "    n_embd=n_embd,\n",
    "    n_heads=n_heads,\n",
    "    dropout_rate=dropout,\n",
    "    static_graph_data=static_graph_data\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "print(f\"Model Iteration 1 Parameters: {sum(p.numel() for p in model.parameters())/1e3:.1f} K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot(steps, train_zinb_nll, val_zinb_nll, train_mse, val_mse, mu, theta, pi):\n",
    "    # Append scalar values, not lists\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(8,16))\n",
    "\n",
    "    ax1.clear()\n",
    "    ax2.clear()\n",
    "    ax3.clear()\n",
    "\n",
    "    ax1.plot(steps, train_zinb_nll, label='Train ZINB NLL', marker='o')\n",
    "    ax1.plot(steps, val_zinb_nll, label='Validation ZINB NLL', marker='o')\n",
    "    ax1.legend()\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('ZINB Training and Validation Loss Over Time')\n",
    "    ax1.grid(True)\n",
    "\n",
    "    ax2.plot(steps, train_mse, label='Train MSE', marker='x')\n",
    "    ax2.plot(steps, val_mse, label='Validation MSE', marker='x')\n",
    "    ax2.legend()\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.set_title('MSE Training and Validation Loss Over Time')\n",
    "    ax2.grid(True)\n",
    "\n",
    "    ax3.plot(steps, mu, label='Mu', color='green', marker='o')\n",
    "    ax3.plot(steps, theta, label='Theta', color='red', marker='o')\n",
    "    ax3.plot(steps, pi, label='Pi', color='purple', marker='o')\n",
    "    ax3.legend()\n",
    "    ax3.set_xlabel('Epoch')\n",
    "    ax3.set_ylabel('Parameter Value')\n",
    "    ax3.set_title('Model Parameters Over Time')\n",
    "    ax3.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "def format_params(params):\n",
    "    params_cat = {\"mu\":[], \"theta\":[], \"pi\": []}\n",
    "    for batch in params:\n",
    "        for key in params_cat.keys():\n",
    "            value = batch[key]\n",
    "            params_cat[key].append(value)\n",
    "\n",
    "    for key in params_cat.keys():\n",
    "        params_cat[key] = torch.cat(params_cat[key], dim=0)\n",
    "\n",
    "    # Return as tuple for easy unpacking\n",
    "    return params_cat[\"mu\"], params_cat[\"theta\"], params_cat[\"pi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training Loop Iteration 1 ---\n",
    "import time\n",
    "start_time = time.time()\n",
    "print(f\"Training started at {start_time}\")\n",
    "model.train()\n",
    "# For plotting\n",
    "avg_train_zinb_nll = []\n",
    "avg_train_mse = []\n",
    "avg_val_zinb_nll = []\n",
    "avg_val_mse = []\n",
    "avg_mu = []\n",
    "avg_theta = []\n",
    "avg_pi = []\n",
    "for epoch in range(epochs):\n",
    "    epoch_zinb_nll = 0\n",
    "    epoch_mse = 0\n",
    "    epoch_valid_samples = 0\n",
    "    epoch_total_samples = 0\n",
    "    num_batches = 0\n",
    "    val_epoch_zinb_nll = 0\n",
    "    val_epoch_mse = 0\n",
    "    val_epoch_valid_samples = 0\n",
    "    val_epoch_total_samples = 0\n",
    "    val_num_batches = 0\n",
    "    epoch_params = []\n",
    "    for X_spatial, X_temporal, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        X_spatial_batch = X_spatial.to(device)\n",
    "        X_temporal_batch = X_temporal.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        # Track batch statistics\n",
    "        batch_size = y_batch.shape[0]\n",
    "        batch_total_samples = y_batch.numel() # Total elements in batch\n",
    "\n",
    "        mu = 6.003180503845215\n",
    "        sigma = 6.975292682647705\n",
    "\n",
    "        # 4. Unnormalize the target variable\n",
    "        # y_raw = (y_batch_normalized * sigma) + mu\n",
    "        y_raw = (y_batch * sigma) + mu # Using your notebook's variable name\n",
    "\n",
    "        # 5. Round to nearest integer and cast to long\n",
    "        # This is ESSENTIAL for the ZINB loss function\n",
    "        y_raw_int = torch.round(y_raw).long()\n",
    "\n",
    "        predictions, zinb_nll_loss, mse_loss, params = model(X_spatial=X_spatial_batch, X_temporal=X_temporal_batch, targets=y_raw_int, node_mask=None)\n",
    "\n",
    "        zinb_nll_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate metrics\n",
    "        epoch_params.append(params)\n",
    "        epoch_zinb_nll += zinb_nll_loss.item()\n",
    "        epoch_mse += mse_loss.item()\n",
    "        epoch_valid_samples += params['valid_sum'].item()\n",
    "        epoch_total_samples += batch_total_samples\n",
    "        num_batches += 1\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_spatial, X_temporal, y_batch in val_loader:\n",
    "            X_spatial_batch = X_spatial.to(device)\n",
    "            X_temporal_batch = X_temporal.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            mu = 6.003180503845215\n",
    "            sigma = 6.975292682647705\n",
    "\n",
    "            # 4. Unnormalize the target variable\n",
    "            # y_raw = (y_batch_normalized * sigma) + mu\n",
    "            y_raw = (y_batch * sigma) + mu # Using your notebook's variable name\n",
    "\n",
    "            # 5. Round to nearest integer and cast to long\n",
    "            # This is ESSENTIAL for the ZINB loss function\n",
    "            y_raw_int = torch.round(y_raw).long()\n",
    "\n",
    "            predictions, zinb_nll_loss, mse_loss, params = model(X_spatial=X_spatial_batch, X_temporal=X_temporal_batch, targets=y_raw_int, node_mask=None)\n",
    "\n",
    "            val_epoch_zinb_nll += zinb_nll_loss.item()\n",
    "            val_epoch_mse += mse_loss.item()\n",
    "            val_epoch_valid_samples += params['valid_sum'].item()\n",
    "            val_epoch_total_samples += y_batch.numel()\n",
    "            val_num_batches += 1\n",
    "\n",
    "    avg_zinb_nll = epoch_zinb_nll / num_batches\n",
    "    avg_mse = epoch_mse / num_batches\n",
    "    valid_percentage = (epoch_valid_samples / epoch_total_samples) * 100\n",
    "    avg_valid_per_batch = epoch_valid_samples / num_batches\n",
    "\n",
    "    val_avg_zinb_nll = val_epoch_zinb_nll / val_num_batches\n",
    "    val_avg_mse = val_epoch_mse / val_num_batches\n",
    "    val_valid_percentage = (val_epoch_valid_samples / val_epoch_total_samples) * 100\n",
    "    val_avg_valid_per_batch = val_epoch_valid_samples / val_num_batches\n",
    "\n",
    "    if (epoch+1) % 10 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1:3d} | \"\n",
    "        f\"ZINB NLL: {avg_zinb_nll:.4f} | \"\n",
    "        f\"MSE: {avg_mse:.4f} | \"\n",
    "        f\"Valid: {epoch_valid_samples}/{epoch_total_samples} ({valid_percentage:.1f}%) | \"\n",
    "        f\"Avg/Batch: {avg_valid_per_batch:.1f} | \"\n",
    "        f\"Batches: {num_batches}\")\n",
    "        print(f\"      VAL | \"\n",
    "        f\"ZINB NLL: {val_avg_zinb_nll:.4f} | \"\n",
    "        f\"MSE: {val_avg_mse:.4f} | \"\n",
    "        f\"Valid: {val_epoch_valid_samples}/{val_epoch_total_samples} ({val_valid_percentage:.1f}%) | \"\n",
    "        f\"Avg/Batch: {val_avg_valid_per_batch:.1f} | \"\n",
    "        f\"Batches: {val_num_batches}\")\n",
    "\n",
    "    mu, theta, pi = format_params(epoch_params)\n",
    "\n",
    "    avg_train_zinb_nll.append(avg_zinb_nll)\n",
    "    avg_val_zinb_nll.append(val_avg_zinb_nll)\n",
    "\n",
    "    avg_train_mse.append(avg_mse)\n",
    "    avg_val_mse.append(val_avg_mse)\n",
    "\n",
    "    avg_mu.append(mu.mean().item())\n",
    "    avg_theta.append(theta.mean().item())\n",
    "    avg_pi.append(pi.mean().item())\n",
    "\n",
    "print(f\"Training completed at {time.time()}, duration: {time.time() - start_time:.2f}s\")\n",
    "steps = list(range(1, epochs+1))\n",
    "plot(steps, avg_train_zinb_nll, avg_val_zinb_nll, avg_train_mse, avg_val_mse, avg_mu, avg_theta, avg_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced metrics with node-level tracking\n",
    "def detailed_evaluation(model, dataloader, device, split_name=\"Val\"):\n",
    "    model.eval()\n",
    "\n",
    "    # Track per-node validity (assuming shape [batch, 1, num_nodes, 1])\n",
    "    num_nodes = None\n",
    "    node_valid_counts = None\n",
    "    node_total_counts = None\n",
    "\n",
    "    total_zinb_nll_loss = 0.0\n",
    "    total_mse_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    all_preds = []\n",
    "    all_params = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_spatial, X_temporal, y_batch in dataloader:\n",
    "            X_spatial_batch = X_spatial.to(device)\n",
    "            X_temporal_batch = X_temporal.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            if num_nodes is None:\n",
    "                num_nodes = y_batch.shape[2]\n",
    "                node_valid_counts = torch.zeros(num_nodes)\n",
    "                node_total_counts = torch.zeros(num_nodes)\n",
    "\n",
    "            mu = 6.003180503845215\n",
    "            sigma = 6.975292682647705\n",
    "\n",
    "            # 4. Unnormalize the target variable\n",
    "            # y_raw = (y_batch_normalized * sigma) + mu\n",
    "            y_raw = (y_batch * sigma) + mu # Using your notebook's variable name\n",
    "\n",
    "            # 5. Round to nearest integer and cast to long\n",
    "            # This is ESSENTIAL for the ZINB loss function\n",
    "            y_raw_int = torch.round(y_raw).long()\n",
    "\n",
    "            preds, zinb_nll_loss, mse_loss, params = model(X_spatial=X_spatial_batch, X_temporal=X_temporal_batch, targets=y_raw_int, node_mask=None)\n",
    "\n",
    "            all_preds.append(preds.cpu())\n",
    "            all_params.append(params)\n",
    "\n",
    "            if zinb_nll_loss is not None:\n",
    "                total_zinb_nll_loss += zinb_nll_loss.item()\n",
    "                total_mse_loss += mse_loss.item()\n",
    "                num_batches += 1\n",
    "\n",
    "                # Count valid samples per node\n",
    "                nan_mask = ~torch.isnan(y_batch)  # [batch, 1, num_nodes, 1]\n",
    "                node_valid_counts += nan_mask.sum(dim=(0, 1, 3)).cpu()\n",
    "                node_total_counts += torch.ones_like(node_valid_counts) * y_batch.shape[0]\n",
    "\n",
    "    if num_batches > 0:\n",
    "        avg_zinb_nll_loss = total_zinb_nll_loss / num_batches\n",
    "        avg_mse_loss = total_mse_loss / num_batches\n",
    "\n",
    "        print(f\"\\n{split_name} Detailed Metrics:\")\n",
    "        print(f\"  Avg ZINB NLL Loss: {avg_zinb_nll_loss:.4f}\")\n",
    "        print(f\"  Avg MSE Loss: {avg_mse_loss:.4f}\")\n",
    "        print(f\"  Total batches: {num_batches}\")\n",
    "\n",
    "        # Per-node statistics\n",
    "        node_valid_pct = (node_valid_counts / node_total_counts * 100)\n",
    "        print(f\"\\n  Node Validity Statistics:\")\n",
    "        print(f\"    Min: {node_valid_pct.min():.1f}%\")\n",
    "        print(f\"    Max: {node_valid_pct.max():.1f}%\")\n",
    "        print(f\"    Mean: {node_valid_pct.mean():.1f}%\")\n",
    "        print(f\"    Nodes with 100% valid: {(node_valid_pct == 100).sum().item()}/{num_nodes}\")\n",
    "        print(f\"    Nodes with <50% valid: {(node_valid_pct < 50).sum().item()}/{num_nodes}\")\n",
    "\n",
    "        return all_preds, all_params\n",
    "\n",
    "# Run after training\n",
    "train_results = detailed_evaluation(model, train_loader, device, \"Train\")\n",
    "val_results = detailed_evaluation(model, val_loader, device, \"Validation\")\n",
    "test_results = detailed_evaluation(model, test_loader, device, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds, train_params = train_results\n",
    "val_preds, val_params = val_results\n",
    "test_preds, test_params = test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mu, train_theta, train_pi = format_params(train_params)\n",
    "val_mu, val_theta, val_pi = format_params(val_params)\n",
    "test_mu, test_theta, test_pi = format_params(test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.distributions as dist\n",
    "import numpy as np\n",
    "from scipy import stats as sp_stats # Import scipy.stats\n",
    "\n",
    "# Define your mu and sigma for unnormalization\n",
    "SCALER_MU = 6.003180503845215\n",
    "SCALER_SIGMA = 6.975292682647705\n",
    "\n",
    "with open(\"data/processed/processed_ac88600464456762_sensor_name_to_id_map.json\", \"r\") as f:\n",
    "    name_to_id_map = json.load(f)\n",
    "    id_to_name_map = {v: k for k, v in name_to_id_map.items()}\n",
    "\n",
    "# This function is now correct, just a small rename for clarity\n",
    "def format_data_for_plotting(params_list, dataloader):\n",
    "    # 1. Unnormalize Ground Truth Targets\n",
    "    targets = []\n",
    "    for _, _, y in dataloader:\n",
    "        y_raw = (y * SCALER_SIGMA) + SCALER_MU\n",
    "        y_raw_int = torch.round(y_raw).long()\n",
    "        targets.append(y_raw_int)\n",
    "    cat_targets = torch.cat(targets, dim=0)\n",
    "\n",
    "    # 2. Concatenate Model Outputs\n",
    "    cat_mu, cat_theta, cat_pi = format_params(params_list)\n",
    "\n",
    "    return cat_targets, cat_mu, cat_theta, cat_pi\n",
    "\n",
    "# --- RE-RUN YOUR DATA FORMATTING ---\n",
    "# We need pi, so we must re-run this part\n",
    "train_targets, train_mu, train_theta, train_pi = format_data_for_plotting(train_params, train_loader)\n",
    "val_targets, val_mu, val_theta, val_pi = format_data_for_plotting(val_params, val_loader)\n",
    "test_targets, test_mu, test_theta, test_pi = format_data_for_plotting(test_params, test_loader)\n",
    "\n",
    "\n",
    "# --- YOUR NEW PLOTTING FUNCTION ---\n",
    "\n",
    "def plot_preds_and_ground_truth(targets, mu, theta, pi, names):\n",
    "    \"\"\"\n",
    "    Plots the ground truth against the predicted ZINB distribution.\n",
    "\n",
    "    - 'True': The actual ground truth counts.\n",
    "    - 'Predicted': The ZINB Expected Value, E[y] = (1-pi) * mu\n",
    "    - '95% P.I.': The 95% prediction interval (2.5th to 97.5th percentile)\n",
    "    \"\"\"\n",
    "\n",
    "    # An small value to prevent division by zero or invalid probs\n",
    "    eps = 1e-8\n",
    "\n",
    "    # --- 1. Calculate the Expected Value (The \"Prediction\") ---\n",
    "    # This can stay in torch\n",
    "    expected_value = (1 - pi) * mu\n",
    "\n",
    "    # --- 3. Plot ---\n",
    "    num_nodes = targets.shape[2]\n",
    "    for node in range(num_nodes):\n",
    "        plt.figure(figsize=(10, 4))\n",
    "\n",
    "        # Get data for the current node and move to CPU/NumPy\n",
    "        # We plot the last feature dimension\n",
    "        true_node = targets[:, 0, node, -1].cpu().numpy()\n",
    "        pred_node = expected_value[:, 0, node, -1].cpu().numpy()\n",
    "\n",
    "        # --- 2. Calculate Prediction Interval (with Scipy) ---\n",
    "        # We do this inside the loop, one node at a time\n",
    "        try:\n",
    "            # Move params to numpy for scipy\n",
    "            mu_node = mu[:, 0, node, -1].cpu().numpy()\n",
    "            theta_node = theta[:, 0, node, -1].cpu().numpy()\n",
    "            pi_node = pi[:, 0, node, -1].cpu().numpy()\n",
    "\n",
    "            # A. Create the Negative Binomial component\n",
    "            # Scipy uses 'n' (total_count) and 'p' (prob)\n",
    "            n_scipy = np.maximum(theta_node, eps) # n = theta\n",
    "            p_scipy = n_scipy / (mu_node + n_scipy + eps) # p = theta / (mu + theta)\n",
    "            p_scipy = np.clip(p_scipy, eps, 1-eps)\n",
    "\n",
    "            # B. Calculate the total probability of zero\n",
    "            prob_zero = pi_node + (1 - pi_node) * sp_stats.nbinom.pmf(0, n=n_scipy, p=p_scipy)\n",
    "\n",
    "            # C. Define the quantiles we want\n",
    "            q_lower = 0.025\n",
    "            q_upper = 0.975\n",
    "\n",
    "            # D. Calculate lower bound\n",
    "            q_lower_adj = (q_lower - pi_node) / (1 - pi_node + eps)\n",
    "            q_lower_adj = np.clip(q_lower_adj, eps, 1-eps)\n",
    "            nb_quantile_lower = sp_stats.nbinom.ppf(q_lower_adj, n=n_scipy, p=p_scipy)\n",
    "            lower_bound = np.where(q_lower <= prob_zero, 0.0, nb_quantile_lower)\n",
    "\n",
    "            # E. Calculate upper bound\n",
    "            q_upper_adj = (q_upper - pi_node) / (1 - pi_node + eps)\n",
    "            q_upper_adj = np.clip(q_upper_adj, eps, 1-eps)\n",
    "            nb_quantile_upper = sp_stats.nbinom.ppf(q_upper_adj, n=n_scipy, p=p_scipy)\n",
    "            upper_bound = np.where(q_upper <= prob_zero, 0.0, nb_quantile_upper)\n",
    "\n",
    "            plot_interval = True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not compute prediction interval for node {node}. Plotting mean only. Error: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            plot_interval = False\n",
    "\n",
    "        # --- Plotting ---\n",
    "        # Plot Ground Truth\n",
    "        plt.plot(true_node, label='True', alpha=0.9, color='blue')\n",
    "\n",
    "        # Plot Predicted Mean\n",
    "        plt.plot(pred_node, label='Predicted (E[y])', alpha=0.9, color='orange', linestyle='--')\n",
    "\n",
    "        # Plot Prediction Interval\n",
    "        if plot_interval:\n",
    "            plt.fill_between(range(len(true_node)),\n",
    "                             lower_bound,\n",
    "                             upper_bound,\n",
    "                             color='orange',\n",
    "                             alpha=0.2,\n",
    "                             label='95% P.I.')\n",
    "\n",
    "        plt.title(f'Node {node} {names[node]} Traffic Prediction')\n",
    "        plt.xlabel('Sample Index')\n",
    "        plt.ylabel('Traffic Count')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "# --- YOUR NEW FUNCTION CALLS ---\n",
    "# Make sure to pass pi as the fourth argument\n",
    "print(\"--- Plotting Test Set ---\")\n",
    "plot_preds_and_ground_truth(test_targets, test_mu, test_theta, test_pi, id_to_name_map)\n",
    "print(\"--- Plotting Validation Set ---\")\n",
    "plot_preds_and_ground_truth(val_targets, val_mu, val_theta, val_pi, id_to_name_map)\n",
    "print(\"--- Plotting Training Set ---\")\n",
    "plot_preds_and_ground_truth(train_targets, train_mu, train_theta, train_pi, id_to_name_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".updated-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
