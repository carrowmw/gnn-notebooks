{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Iter 4 - GAT-ZINB Evaluation Metrics and Visualizations\n",
    "\n",
    "This notebook extends the GAT-ZINB traffic prediction model with comprehensive evaluation metrics and visualizations to assess model performance, calibration, and interpretability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "import torch.nn.functional as F\n",
    "from scipy import stats as sp_stats\n",
    "from scipy.special import gammaln\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "SCALER_MU = 14.323774337768555\n",
    "SCALER_SIGMA = 34.9963493347168\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROCESSED_DIR = Path('../Test-5/data/training/processed/c56f869b05279744')\n",
    "CKPT_PATH = Path('../Test-5/data/models/iter4_gat.pth')\n",
    "\n",
    "\n",
    "# Load data\n",
    "edge_index = torch.load(PROCESSED_DIR / \"edge_index.pt\", weights_only=False)\n",
    "edge_attr_data = torch.load(PROCESSED_DIR / \"edge_attr.pt\", weights_only=False)\n",
    "static_features = torch.load(PROCESSED_DIR / \"static_features.pt\", weights_only=False)\n",
    "sensor_mask = torch.load(PROCESSED_DIR / \"sensor_mask.pt\", weights_only=False)\n",
    "train_loader = torch.load(PROCESSED_DIR / \"train_loader.pt\", weights_only=False)\n",
    "val_loader = torch.load(PROCESSED_DIR / \"val_loader.pt\", weights_only=False)\n",
    "test_loader = torch.load(PROCESSED_DIR / \"test_loader.pt\", weights_only=False)\n",
    "\n",
    "with open(PROCESSED_DIR / \"sensor_name_to_id_map.json\", \"r\") as f:\n",
    "    name_to_id_map = json.load(f)\n",
    "    id_to_name_map = {v: k for k, v in name_to_id_map.items()}\n",
    "\n",
    "print(f\"Loaded {len(id_to_name_map)} sensor nodes\")\n",
    "print(f\"Edge index shape: {edge_index.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_hybrid_loader(loader, batch_size):\n",
    "    all_batches = [(X, y) for X, y in loader]\n",
    "\n",
    "    X_temporal_list = [(X[:, :, :, -1:]) for X, _ in all_batches]\n",
    "    X_temporal = torch.cat(X_temporal_list, dim=0)\n",
    "\n",
    "    X_agg_list = [(X[:, 0:1, :, :-1]) for X, _ in all_batches]\n",
    "    X_raw_list = [(X[:, :, :, -1:].permute(0,3,2,1)) for X, _ in all_batches]\n",
    "    X_agg = torch.cat(X_agg_list, dim=0)\n",
    "    X_raw = torch.cat(X_raw_list, dim=0)\n",
    "    X_spatial = torch.cat([X_agg, X_raw], dim=3)\n",
    "\n",
    "    y_list = [y[:, 0:1, :, :] for _, y in all_batches]\n",
    "    y_target = torch.cat(y_list, dim=0)\n",
    "\n",
    "    dataset = torch.utils.data.TensorDataset(X_spatial, X_temporal, y_target)\n",
    "    hybrid_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "    return hybrid_loader\n",
    "\n",
    "train_loader = prepare_hybrid_loader(train_loader, batch_size=16)\n",
    "val_loader = prepare_hybrid_loader(val_loader, batch_size=16)\n",
    "test_loader = prepare_hybrid_loader(test_loader, batch_size=16)\n",
    "print(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}, Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition with attention weight extraction\n",
    "class DynamicNodeGATZINB(nn.Module):\n",
    "    def __init__(self, dynamic_node_dim, static_node_dim, edge_dim, n_embd, n_heads, dropout_rate):\n",
    "        super().__init__()\n",
    "\n",
    "        dynamic_input_dim = dynamic_node_dim * 2\n",
    "        static_input_dim = static_node_dim\n",
    "        gat1_input_channels = dynamic_input_dim + static_input_dim\n",
    "\n",
    "        self.gat1 = GATv2Conv(in_channels=gat1_input_channels, out_channels=n_embd, edge_dim=edge_dim,\n",
    "                              heads=n_heads, concat=False, dropout=dropout_rate)\n",
    "        self.gat2 = GATv2Conv(in_channels=n_embd, out_channels=n_embd, edge_dim=edge_dim,\n",
    "                              heads=n_heads, concat=False, dropout=dropout_rate)\n",
    "        self.norm = nn.LayerNorm(n_embd)\n",
    "\n",
    "        self.mu_head = nn.Linear(n_embd, 1)\n",
    "        self.theta_head = nn.Linear(n_embd, 1)\n",
    "        self.pi_head = nn.Linear(n_embd, 1)\n",
    "\n",
    "        self.elu = nn.ELU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # Store attention weights\n",
    "        self.attention_weights_1 = None\n",
    "        self.attention_weights_2 = None\n",
    "\n",
    "    def forward(self, X_batch, targets, node_mask, edge_index, edge_attr, static_node_features, return_attention=False):\n",
    "        mu_list, theta_list, pi_list = [], [], []\n",
    "        attn_weights_1_list, attn_weights_2_list = [], []\n",
    "\n",
    "        missing_mask = torch.isnan(X_batch)\n",
    "        imputed_X = torch.nan_to_num(X_batch, nan=0.0)\n",
    "        mask_features = missing_mask.float()\n",
    "        combined_input = torch.cat([imputed_X, mask_features], dim=-1)\n",
    "\n",
    "        B = combined_input.shape[0]\n",
    "        Xb = combined_input[:,0,:,:]\n",
    "\n",
    "        for b in range(B):\n",
    "            combined_features = torch.cat([Xb[b], static_node_features], dim=-1)\n",
    "            xb = self.dropout(combined_features)\n",
    "\n",
    "            # Get attention weights from first GAT layer\n",
    "            xb, attn1 = self.gat1(xb, edge_index, edge_attr, return_attention_weights=True)\n",
    "            xb = self.norm(xb)\n",
    "            xb = self.elu(xb)\n",
    "            xb = self.dropout(xb)\n",
    "\n",
    "            # Get attention weights from second GAT layer\n",
    "            xb, attn2 = self.gat2(xb, edge_index, edge_attr, return_attention_weights=True)\n",
    "            xb = self.norm(xb)\n",
    "            xb = self.elu(xb)\n",
    "            xb = self.dropout(xb)\n",
    "\n",
    "            if return_attention:\n",
    "                attn_weights_1_list.append(attn1)\n",
    "                attn_weights_2_list.append(attn2)\n",
    "\n",
    "            mu_b = torch.nn.functional.softplus(self.mu_head(xb)) + 1e-6\n",
    "            theta_b = torch.nn.functional.softplus(self.theta_head(xb)) + 1e-6\n",
    "            pi_b = torch.sigmoid(self.pi_head(xb))\n",
    "            pi_b = torch.clamp(pi_b, min=1e-6, max=1-1e-6)\n",
    "\n",
    "            mu_list.append(mu_b)\n",
    "            theta_list.append(theta_b)\n",
    "            pi_list.append(pi_b)\n",
    "\n",
    "        mu = torch.stack(mu_list, dim=0).unsqueeze(1)\n",
    "        theta = torch.stack(theta_list, dim=0).unsqueeze(1)\n",
    "        pi = torch.stack(pi_list, dim=0).unsqueeze(1)\n",
    "\n",
    "        zinb_nll_loss, valid_sum = self.zinb_nll_loss(mu, theta, pi, targets, node_mask)\n",
    "        preds = mu * (1 - pi)\n",
    "        mse_loss, _ = self.mse_loss(preds, targets, node_mask)\n",
    "\n",
    "        result = {\n",
    "            'mu': mu, 'theta': theta, 'pi': pi, 'valid_sum': valid_sum\n",
    "        }\n",
    "\n",
    "        if return_attention:\n",
    "            result['attention_1'] = attn_weights_1_list\n",
    "            result['attention_2'] = attn_weights_2_list\n",
    "\n",
    "        return preds, zinb_nll_loss, mse_loss, result\n",
    "\n",
    "    def zinb_nll_loss(self, mu, theta, pi, targets, node_mask):\n",
    "        eps = 1e-8\n",
    "        nan_mask = ~torch.isnan(targets)\n",
    "\n",
    "        if node_mask is not None:\n",
    "            valid_mask = nan_mask & node_mask\n",
    "        else:\n",
    "            valid_mask = nan_mask\n",
    "\n",
    "        if valid_mask.sum() == 0:\n",
    "            return torch.tensor(0.0, device=mu.device, requires_grad=True), torch.tensor(0.0, device=mu.device)\n",
    "\n",
    "        mu_valid = mu[valid_mask]\n",
    "        theta_valid = theta[valid_mask]\n",
    "        pi_valid = pi[valid_mask]\n",
    "        targets_valid = targets[valid_mask]\n",
    "        targets_valid = torch.round(targets_valid).clamp_min(0).to(mu_valid.dtype)\n",
    "\n",
    "        theta_mu = theta_valid + mu_valid\n",
    "\n",
    "        nb_log_prob = (\n",
    "            torch.lgamma(theta_valid + targets_valid + eps)\n",
    "            - torch.lgamma(theta_valid + eps)\n",
    "            - torch.lgamma(targets_valid + 1)\n",
    "            + theta_valid * torch.log(theta_valid + eps)\n",
    "            - theta_valid * torch.log(theta_mu + eps)\n",
    "            + targets_valid * torch.log(mu_valid + eps)\n",
    "            - targets_valid * torch.log(theta_mu + eps)\n",
    "        )\n",
    "\n",
    "        zero_mask = (targets_valid < eps).float()\n",
    "        nb_zero_prob = theta_valid * torch.log(theta_valid / (theta_mu + eps))\n",
    "        zero_log_prob = torch.log(pi_valid + (1 - pi_valid) * torch.exp(nb_zero_prob) + eps)\n",
    "        non_zero_log_prob = torch.log(1 - pi_valid + eps) + nb_log_prob\n",
    "        log_prob = zero_mask * zero_log_prob + (1 - zero_mask) * non_zero_log_prob\n",
    "\n",
    "        nll = -log_prob.mean()\n",
    "        return nll, valid_mask.sum()\n",
    "\n",
    "    def mse_loss(self, predictions, targets, node_mask):\n",
    "        nan_mask = ~torch.isnan(targets)\n",
    "        if node_mask is not None:\n",
    "            valid_mask = nan_mask & node_mask\n",
    "        else:\n",
    "            valid_mask = nan_mask\n",
    "\n",
    "        if valid_mask.sum() == 0:\n",
    "            return torch.tensor(0.0, device=predictions.device, requires_grad=True), 0\n",
    "\n",
    "        preds_valid = predictions[valid_mask]\n",
    "        targets_valid = targets[valid_mask]\n",
    "        mse_loss = ((targets_valid - preds_valid)**2).mean()\n",
    "\n",
    "        return mse_loss, valid_mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model\n",
    "n_embd = 32\n",
    "n_heads = 4\n",
    "dropout = 0.1\n",
    "static_nodes_dim = static_features.shape[1]\n",
    "edge_attr_dim = edge_attr_data.shape[1]\n",
    "\n",
    "model = DynamicNodeGATZINB(\n",
    "    dynamic_node_dim=21,\n",
    "    static_node_dim=static_nodes_dim,\n",
    "    edge_dim=edge_attr_dim,\n",
    "    n_embd=n_embd,\n",
    "    n_heads=n_heads,\n",
    "    dropout_rate=dropout,\n",
    ").to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(CKPT_PATH, map_location=device))\n",
    "model.eval()\n",
    "print(f\"Model loaded with {sum(p.numel() for p in model.parameters())/1e3:.1f}K parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare graph data\n",
    "X_static_input = static_features.to(device)\n",
    "edge_index_input = edge_index.to(device)\n",
    "edge_attr_input = edge_attr_data.to(device)\n",
    "num_nodes = static_features.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract predictions and parameters from test set\n",
    "def extract_predictions(model, data_loader, device):\n",
    "    model.eval()\n",
    "\n",
    "    all_targets = []\n",
    "    all_mu = []\n",
    "    all_theta = []\n",
    "    all_pi = []\n",
    "    all_preds = []\n",
    "    all_attention_1 = []\n",
    "    all_attention_2 = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_spatial, _, y_batch in data_loader:\n",
    "            X_batch = X_spatial.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            y_raw = (y_batch * SCALER_SIGMA) + SCALER_MU\n",
    "\n",
    "            preds, _, _, params = model(\n",
    "                X_batch=X_batch,\n",
    "                targets=y_raw,\n",
    "                node_mask=None,\n",
    "                edge_index=edge_index_input,\n",
    "                edge_attr=edge_attr_input,\n",
    "                static_node_features=X_static_input,\n",
    "                return_attention=True\n",
    "            )\n",
    "\n",
    "            all_targets.append(y_raw.cpu())\n",
    "            all_preds.append(preds.cpu())\n",
    "            all_mu.append(params['mu'].cpu())\n",
    "            all_theta.append(params['theta'].cpu())\n",
    "            all_pi.append(params['pi'].cpu())\n",
    "            all_attention_1.extend(params['attention_1'])\n",
    "            all_attention_2.extend(params['attention_2'])\n",
    "\n",
    "    return {\n",
    "        'targets': torch.cat(all_targets, dim=0),\n",
    "        'preds': torch.cat(all_preds, dim=0),\n",
    "        'mu': torch.cat(all_mu, dim=0),\n",
    "        'theta': torch.cat(all_theta, dim=0),\n",
    "        'pi': torch.cat(all_pi, dim=0),\n",
    "        'attention_1': all_attention_1,\n",
    "        'attention_2': all_attention_2\n",
    "    }\n",
    "\n",
    "test_results = extract_predictions(model, test_loader, device)\n",
    "val_results = extract_predictions(model, val_loader, device)\n",
    "train_results = extract_predictions(model, train_loader, device)\n",
    "\n",
    "print(f\"Test samples: {test_results['targets'].shape[0]}\")\n",
    "print(f\"Validation samples: {val_results['targets'].shape[0]}\")\n",
    "print(f\"Training samples: {train_results['targets'].shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## 1. Compute Calibration Metrics for ZINB Predictions\n",
    "\n",
    "Calculate calibration metrics to assess how well the predicted ZINB distributions match observed frequencies. Compute PIT (Probability Integral Transform) histograms and reliability diagrams for the zero-inflation component.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_zinb_cdf(y, mu, theta, pi):\n",
    "    \"\"\"\n",
    "    Compute CDF of ZINB distribution at point y.\n",
    "\n",
    "    Uses the incomplete beta function for the NB CDF for speed:\n",
    "      NB_CDF(k; n, p) = I_p(n, k+1)\n",
    "\n",
    "    ZINB CDF = 0                         for y < 0\n",
    "            = pi + (1 - pi) * NB_CDF(y) for y >= 0\n",
    "    \"\"\"\n",
    "    from scipy.special import betainc\n",
    "    eps = 1e-8\n",
    "\n",
    "    y = np.asarray(y)\n",
    "    mu = np.asarray(mu)\n",
    "    theta = np.asarray(theta)\n",
    "    pi = np.asarray(pi)\n",
    "\n",
    "    n = np.maximum(theta, eps)\n",
    "    p = np.clip(n / (mu + n + eps), eps, 1 - eps)\n",
    "\n",
    "    # NB CDF via regularized incomplete beta (vectorized + fast)\n",
    "    y_clip = np.maximum(y, 0)\n",
    "    nb_cdf = betainc(n, y_clip + 1.0, p)\n",
    "\n",
    "    zinb_cdf = pi + (1 - pi) * nb_cdf\n",
    "    zinb_cdf = np.where(y < 0, 0.0, zinb_cdf)\n",
    "    return zinb_cdf\n",
    "\n",
    "def compute_pit_values(targets, mu, theta, pi):\n",
    "    \"\"\"\n",
    "    Compute Probability Integral Transform (PIT) values.\n",
    "    For a well-calibrated model, PIT values should be uniformly distributed.\n",
    "    \"\"\"\n",
    "    targets_flat = targets.numpy().flatten()\n",
    "    mu_flat = mu.numpy().flatten()\n",
    "    theta_flat = theta.numpy().flatten()\n",
    "    pi_flat = pi.numpy().flatten()\n",
    "\n",
    "    valid_mask = ~np.isnan(targets_flat)\n",
    "    targets_valid = targets_flat[valid_mask]\n",
    "    mu_valid = mu_flat[valid_mask]\n",
    "    theta_valid = theta_flat[valid_mask]\n",
    "    pi_valid = pi_flat[valid_mask]\n",
    "\n",
    "    targets_valid = np.maximum(np.round(targets_valid), 0)\n",
    "    pit_values = compute_zinb_cdf(targets_valid, mu_valid, theta_valid, pi_valid)\n",
    "    return pit_values, targets_valid, mu_valid, theta_valid, pi_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute PIT values for test set\n",
    "pit_values, targets_valid, mu_valid, theta_valid, pi_valid = compute_pit_values(\n",
    "    test_results['targets'],\n",
    "    test_results['mu'],\n",
    "    test_results['theta'],\n",
    "    test_results['pi']\n",
    ")\n",
    "\n",
    "print(f\"Computed PIT values for {len(pit_values)} valid samples\")\n",
    "print(f\"PIT value range: [{pit_values.min():.4f}, {pit_values.max():.4f}]\")\n",
    "print(f\"PIT value mean: {pit_values.mean():.4f} (should be ~0.5 for calibrated model)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PIT histogram\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# PIT Histogram\n",
    "ax1 = axes[0]\n",
    "ax1.hist(pit_values, bins=20, density=True, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "ax1.axhline(y=1.0, color='red', linestyle='--', linewidth=2, label='Uniform (ideal)')\n",
    "ax1.set_xlabel('PIT Value')\n",
    "ax1.set_ylabel('Density')\n",
    "ax1.set_title('PIT Histogram (Test Set)')\n",
    "ax1.legend()\n",
    "ax1.set_xlim(0, 1)\n",
    "\n",
    "# Kolmogorov-Smirnov test for uniformity\n",
    "ks_stat, ks_pvalue = sp_stats.kstest(pit_values, 'uniform')\n",
    "ax1.text(0.05, 0.95, f'KS stat: {ks_stat:.4f}\\np-value: {ks_pvalue:.4f}',\n",
    "         transform=ax1.transAxes, verticalalignment='top', fontsize=10,\n",
    "         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# Reliability diagram for zero-inflation\n",
    "ax2 = axes[1]\n",
    "# Bin predictions by pi value\n",
    "n_bins = 10\n",
    "pi_bins = np.linspace(0, 1, n_bins + 1)\n",
    "observed_zero_fracs = []\n",
    "predicted_zero_fracs = []\n",
    "\n",
    "for i in range(n_bins):\n",
    "    mask = (pi_valid >= pi_bins[i]) & (pi_valid < pi_bins[i+1])\n",
    "    if mask.sum() > 0:\n",
    "        observed_zero_frac = (targets_valid[mask] == 0).mean()\n",
    "        predicted_zero_frac = pi_valid[mask].mean()\n",
    "        observed_zero_fracs.append(observed_zero_frac)\n",
    "        predicted_zero_fracs.append(predicted_zero_frac)\n",
    "\n",
    "ax2.scatter(predicted_zero_fracs, observed_zero_fracs, s=100, alpha=0.7, color='steelblue')\n",
    "ax2.plot([0, 1], [0, 1], 'r--', linewidth=2, label='Perfect calibration')\n",
    "ax2.set_xlabel('Predicted Zero Probability (π)')\n",
    "ax2.set_ylabel('Observed Zero Fraction')\n",
    "ax2.set_title('Zero-Inflation Reliability Diagram')\n",
    "ax2.legend()\n",
    "ax2.set_xlim(0, 1)\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.set_aspect('equal')\n",
    "\n",
    "# Distribution of pi values\n",
    "ax3 = axes[2]\n",
    "ax3.hist(pi_valid, bins=30, density=True, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "ax3.set_xlabel('Zero-Inflation Probability (π)')\n",
    "ax3.set_ylabel('Density')\n",
    "ax3.set_title('Distribution of Predicted π Values')\n",
    "ax3.axvline(x=pi_valid.mean(), color='red', linestyle='--', linewidth=2,\n",
    "            label=f'Mean: {pi_valid.mean():.3f}')\n",
    "ax3.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nCalibration Summary:\")\n",
    "print(f\"  KS Statistic (PIT uniformity): {ks_stat:.4f}\")\n",
    "print(f\"  KS p-value: {ks_pvalue:.4f}\")\n",
    "print(f\"  Mean predicted π: {pi_valid.mean():.4f}\")\n",
    "print(f\"  Observed zero fraction: {(targets_valid == 0).mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## 1b. Two-Extremes Evaluation (NaNs dropped vs NaNs=0)\n",
    "\n",
    "Because we can't distinguish “NaN = true zero” from “NaN = unknown”, we bracket performance between two extreme assumptions:\n",
    "\n",
    "- **Extreme A (Drop NaNs):** score only where targets are observed (conditional performance).\n",
    "- **Extreme B (NaNs as 0):** treat every missing target as a true zero (best-case for zero-as-missing).\n",
    "\n",
    "Reality should lie somewhere between these extremes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two-extremes bracket: (A) drop NaNs vs (B) treat NaNs as true zeros\n",
    "def _zinb_p0(mu, theta, pi):\n",
    "    eps = 1e-8\n",
    "    mu = np.asarray(mu)\n",
    "    theta = np.asarray(theta)\n",
    "    pi = np.asarray(pi)\n",
    "    n = np.maximum(theta, eps)\n",
    "    p = np.clip(n / (mu + n + eps), eps, 1 - eps)\n",
    "    nb_p0 = np.power(p, n)  # P_NB(Y=0) under scipy's parameterization\n",
    "    return pi + (1 - pi) * nb_p0\n",
    "\n",
    "def _randomized_pit_discrete(y_int, mu, theta, pi, rng):\n",
    "    y_int = np.asarray(y_int).astype(int)\n",
    "    F_y = compute_zinb_cdf(y_int, mu, theta, pi)\n",
    "    F_ymin1 = compute_zinb_cdf(y_int - 1, mu, theta, pi)\n",
    "    u = rng.random(len(y_int))\n",
    "    return F_ymin1 + u * (F_y - F_ymin1)\n",
    "\n",
    "def _randomized_pit_conditional_positive(y_int, mu, theta, pi, rng):\n",
    "    \"\"\"Randomized PIT for the distribution conditional on Y>0 (for y>=1 only).\"\"\"\n",
    "    y_int = np.asarray(y_int).astype(int)\n",
    "    assert np.all(y_int >= 1)\n",
    "    p0 = _zinb_p0(mu, theta, pi)\n",
    "    denom = np.clip(1 - p0, 1e-8, 1.0)\n",
    "    F_y = compute_zinb_cdf(y_int, mu, theta, pi)\n",
    "    F_ymin1 = compute_zinb_cdf(y_int - 1, mu, theta, pi)\n",
    "    Fcond_y = np.clip((F_y - p0) / denom, 0.0, 1.0)\n",
    "    Fcond_ymin1 = np.clip((F_ymin1 - p0) / denom, 0.0, 1.0)\n",
    "    u = rng.random(len(y_int))\n",
    "    return Fcond_ymin1 + u * (Fcond_y - Fcond_ymin1)\n",
    "\n",
    "def _flatten_np(x):\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return x.detach().cpu().numpy().flatten()\n",
    "    return np.asarray(x).flatten()\n",
    "\n",
    "def evaluate_extreme(name, treat_nan_as_zero, seed=0, max_points=None):\n",
    "    \"\"\"\n",
    "    Brackets evaluation between:\n",
    "      - treat_nan_as_zero=False: score only observed targets (drop NaNs)\n",
    "      - treat_nan_as_zero=True: score all targets, converting NaNs to 0\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    targets = _flatten_np(test_results['targets'])\n",
    "    preds_point = _flatten_np(test_results['preds'])\n",
    "    mu = _flatten_np(test_results['mu'])\n",
    "    theta = _flatten_np(test_results['theta'])\n",
    "    pi = _flatten_np(test_results['pi'])\n",
    "\n",
    "    if treat_nan_as_zero:\n",
    "        y = np.nan_to_num(targets, nan=0.0)\n",
    "        mask = np.ones_like(y, dtype=bool)\n",
    "    else:\n",
    "        mask = ~np.isnan(targets)\n",
    "        y = targets.copy()\n",
    "\n",
    "    y = y[mask]\n",
    "    preds_point_s = preds_point[mask]\n",
    "    mu_s = mu[mask]\n",
    "    theta_s = theta[mask]\n",
    "    pi_s = pi[mask]\n",
    "\n",
    "    y_int = np.maximum(np.round(y), 0).astype(int)\n",
    "\n",
    "    if max_points is not None and len(y_int) > max_points:\n",
    "        idx = rng.choice(len(y_int), size=max_points, replace=False)\n",
    "        y_int = y_int[idx]\n",
    "        preds_point_s = preds_point_s[idx]\n",
    "        mu_s = mu_s[idx]\n",
    "        theta_s = theta_s[idx]\n",
    "        pi_s = pi_s[idx]\n",
    "\n",
    "    pit = _randomized_pit_discrete(y_int, mu_s, theta_s, pi_s, rng=rng)\n",
    "    ks_stat, ks_p = sp_stats.kstest(pit, 'uniform')\n",
    "\n",
    "    p0 = _zinb_p0(mu_s, theta_s, pi_s)\n",
    "\n",
    "    mae = np.mean(np.abs(preds_point_s - y_int))\n",
    "    rmse = np.sqrt(np.mean((preds_point_s - y_int) ** 2))\n",
    "\n",
    "    pos_mask = y_int >= 1\n",
    "    if pos_mask.sum() >= 10:\n",
    "        pit_pos = _randomized_pit_conditional_positive(\n",
    "            y_int[pos_mask], mu_s[pos_mask], theta_s[pos_mask], pi_s[pos_mask], rng=rng\n",
    "        )\n",
    "        ks_pos_stat, ks_pos_p = sp_stats.kstest(pit_pos, 'uniform')\n",
    "        pit_pos_mean = float(np.mean(pit_pos))\n",
    "    else:\n",
    "        ks_pos_stat, ks_pos_p, pit_pos_mean = np.nan, np.nan, np.nan\n",
    "\n",
    "    return {\n",
    "        'setting': name,\n",
    "        'treat_nan_as_zero': treat_nan_as_zero,\n",
    "        'n_scored': int(len(y_int)),\n",
    "        'observed_zero_frac(scored)': float(np.mean(y_int == 0)),\n",
    "        'mean_pred_P(Y=0)': float(np.mean(p0)),\n",
    "        'pit_mean': float(np.mean(pit)),\n",
    "        'ks_stat': float(ks_stat),\n",
    "        'ks_p': float(ks_p),\n",
    "        'mae(point)': float(mae),\n",
    "        'rmse(point)': float(rmse),\n",
    "        'pit_pos_mean(y>=1)': pit_pos_mean,\n",
    "        'ks_pos_stat(y>=1)': float(ks_pos_stat) if ks_pos_stat == ks_pos_stat else np.nan,\n",
    "        'ks_pos_p(y>=1)': float(ks_pos_p) if ks_pos_p == ks_pos_p else np.nan,\n",
    "    }\n",
    "\n",
    "# Compute both extremes\n",
    "results = []\n",
    "results.append(evaluate_extreme('Extreme A: drop NaNs (observed-only)', treat_nan_as_zero=False, seed=0, max_points=None))\n",
    "results.append(evaluate_extreme('Extreme B: NaNs as 0 (all-missing-are-zero)', treat_nan_as_zero=True, seed=0, max_points=None))\n",
    "\n",
    "df_extremes = pd.DataFrame(results)\n",
    "display(df_extremes)\n",
    "\n",
    "# Quick visual: PIT mean + P(Y=0) under both assumptions\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "ax[0].bar(df_extremes['setting'], df_extremes['pit_mean'], color=['steelblue', 'darkorange'])\n",
    "ax[0].axhline(0.5, color='red', linestyle='--', linewidth=2)\n",
    "ax[0].set_title('Randomized PIT mean (ideal ≈ 0.5)')\n",
    "ax[0].tick_params(axis='x', rotation=20)\n",
    "\n",
    "ax[1].bar(df_extremes['setting'], df_extremes['mean_pred_P(Y=0)'], color=['steelblue', 'darkorange'])\n",
    "ax[1].set_title('Mean predicted P(Y=0)')\n",
    "ax[1].tick_params(axis='x', rotation=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## 1c. Positive-only diagnostics (PIT ECDF + residual vs level)\n",
    "\n",
    "To avoid the zero/missing ambiguity, these plots focus on the **positive-only subset** ($y \\ge 1$).\n",
    "\n",
    "- **PIT ECDF:** compare the empirical CDF of randomized PIT values vs the diagonal $y=x$ (ideal calibration).\n",
    "- **Residual vs level:** check systematic bias as counts increase (e.g., underprediction at high counts).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positive-only subset diagnostics (y >= 1)\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "# Build the scoring arrays for Extreme A (observed-only); positive-only subset is typically the same under both extremes\n",
    "targets = _flatten_np(test_results['targets'])\n",
    "preds_point = _flatten_np(test_results['preds'])\n",
    "mu = _flatten_np(test_results['mu'])\n",
    "theta = _flatten_np(test_results['theta'])\n",
    "pi = _flatten_np(test_results['pi'])\n",
    "\n",
    "mask_obs = ~np.isnan(targets)\n",
    "y_obs = targets[mask_obs]\n",
    "preds_obs = preds_point[mask_obs]\n",
    "mu_obs = mu[mask_obs]\n",
    "theta_obs = theta[mask_obs]\n",
    "pi_obs = pi[mask_obs]\n",
    "\n",
    "y_int = np.maximum(np.round(y_obs), 0).astype(int)\n",
    "pos = y_int >= 1\n",
    "\n",
    "y_pos_all = y_int[pos]\n",
    "preds_pos_all = preds_obs[pos]\n",
    "mu_pos_all = mu_obs[pos]\n",
    "theta_pos_all = theta_obs[pos]\n",
    "pi_pos_all = pi_obs[pos]\n",
    "\n",
    "print(f\"Positive-only samples (observed-only): {len(y_pos_all)}\")\n",
    "\n",
    "# Subsample early to keep scipy CDF calls fast\n",
    "max_pit = 100_000\n",
    "if len(y_pos_all) > max_pit:\n",
    "    sidx = rng.choice(len(y_pos_all), size=max_pit, replace=False)\n",
    "    y_pos = y_pos_all[sidx]\n",
    "    preds_pos = preds_pos_all[sidx]\n",
    "    mu_pos = mu_pos_all[sidx]\n",
    "    theta_pos = theta_pos_all[sidx]\n",
    "    pi_pos = pi_pos_all[sidx]\n",
    "else:\n",
    "    y_pos = y_pos_all\n",
    "    preds_pos = preds_pos_all\n",
    "    mu_pos = mu_pos_all\n",
    "    theta_pos = theta_pos_all\n",
    "    pi_pos = pi_pos_all\n",
    "\n",
    "# --- PIT ECDF on positive-only (conditional Y>0) ---\n",
    "pit_pos = _randomized_pit_conditional_positive(y_pos, mu_pos, theta_pos, pi_pos, rng=rng)\n",
    "pit_pos_sorted = np.sort(pit_pos)\n",
    "ecdf = np.arange(1, len(pit_pos_sorted) + 1) / len(pit_pos_sorted)\n",
    "\n",
    "ks_pos_stat, ks_pos_p = sp_stats.kstest(pit_pos, 'uniform')\n",
    "print(f\"Conditional-positive PIT mean: {pit_pos.mean():.4f} (ideal ~0.5)\")\n",
    "print(f\"Conditional-positive PIT KS stat/p: {ks_pos_stat:.4f}, {ks_pos_p:.4g}\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(8, 10))\n",
    "\n",
    "ax = axes[0]\n",
    "ax.plot(pit_pos_sorted, ecdf, color='steelblue', linewidth=2, label='Empirical CDF (PIT | Y>0)')\n",
    "ax.plot([0, 1], [0, 1], 'r--', linewidth=2, label='Ideal (uniform)')\n",
    "ax.set_xlabel('u')\n",
    "ax.set_ylabel('F(u)')\n",
    "ax.set_title('PIT Empirical CDF (positive-only, subsampled)')\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.legend(loc='lower right')\n",
    "ax.grid(True, alpha=0.2)\n",
    "\n",
    "# --- Residual vs level on positive-only ---\n",
    "residual = preds_pos - y_pos  # positive = overprediction, negative = underprediction\n",
    "\n",
    "# Downsample scatter further for responsiveness\n",
    "max_scatter = 25_000\n",
    "if len(y_pos) > max_scatter:\n",
    "    idx = rng.choice(len(y_pos), size=max_scatter, replace=False)\n",
    "    y_sc = y_pos[idx]\n",
    "    r_sc = residual[idx]\n",
    "else:\n",
    "    y_sc = y_pos\n",
    "    r_sc = residual\n",
    "\n",
    "ax = axes[1]\n",
    "ax.scatter(y_sc, r_sc, s=8, alpha=0.10, color='black', rasterized=True)\n",
    "ax.axhline(0, color='red', linestyle='--', linewidth=2)\n",
    "ax.set_xlabel('Observed count (y)')\n",
    "ax.set_ylabel('Residual (pred - y)')\n",
    "ax.set_title('Residual vs level (positive-only, subsampled)')\n",
    "ax.grid(True, alpha=0.2)\n",
    "\n",
    "# Add binned mean residual curve (helps see bias trend)\n",
    "n_bins = 25\n",
    "bins = np.quantile(y_sc, np.linspace(0, 1, n_bins + 1))\n",
    "bins = np.unique(bins)\n",
    "if len(bins) >= 3:\n",
    "    bin_idx = np.digitize(y_sc, bins[1:-1], right=True)\n",
    "    bin_centers = []\n",
    "    bin_means = []\n",
    "    for b in range(bin_idx.min(), bin_idx.max() + 1):\n",
    "        m = bin_idx == b\n",
    "        if m.sum() < 100:\n",
    "            continue\n",
    "        bin_centers.append(np.median(y_sc[m]))\n",
    "        bin_means.append(np.mean(r_sc[m]))\n",
    "    if len(bin_centers) > 2:\n",
    "        ax.plot(bin_centers, bin_means, color='steelblue', linewidth=3, label='Binned mean residual')\n",
    "        ax.legend(loc='best')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## 1d. Tail performance (missed peaks)\n",
    "\n",
    "These diagnostics focus on whether the model is systematically missing high counts:\n",
    "\n",
    "- **Binned observed vs predicted:** compares mean predicted count vs mean observed count across bins of observed $y$.\n",
    "- **Top-tail metrics:** MAE/RMSE on the highest 1% (and 5%) of observed positive counts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tail performance diagnostics on positive-only subset\n",
    "rng = np.random.default_rng(1)\n",
    "\n",
    "# Prefer using the full positive-only arrays (y_pos_all, preds_pos_all) if they exist; otherwise recompute quickly.\n",
    "if 'y_pos_all' in globals() and 'preds_pos_all' in globals():\n",
    "    y_all = np.asarray(y_pos_all)\n",
    "    p_all = np.asarray(preds_pos_all)\n",
    "else:\n",
    "    targets = _flatten_np(test_results['targets'])\n",
    "    preds_point = _flatten_np(test_results['preds'])\n",
    "    mask_obs = ~np.isnan(targets)\n",
    "    y_obs = np.maximum(np.round(targets[mask_obs]), 0).astype(int)\n",
    "    p_obs = preds_point[mask_obs]\n",
    "    pos = y_obs >= 1\n",
    "    y_all = y_obs[pos]\n",
    "    p_all = p_obs[pos]\n",
    "\n",
    "# Basic summary\n",
    "print(f\"Positive-only N (full observed-only): {len(y_all)}\")\n",
    "print(f\"y range: [{y_all.min()}, {y_all.max()}]\")\n",
    "\n",
    "# Optional subsample for plotting (keeps curves stable, makes it fast)\n",
    "max_plot = 200_000\n",
    "if len(y_all) > max_plot:\n",
    "    idx = rng.choice(len(y_all), size=max_plot, replace=False)\n",
    "    y = y_all[idx]\n",
    "    p = p_all[idx]\n",
    "else:\n",
    "    y = y_all\n",
    "    p = p_all\n",
    "\n",
    "# --- Binned observed vs predicted means ---\n",
    "n_bins = 30\n",
    "qs = np.linspace(0, 1, n_bins + 1)\n",
    "edges = np.quantile(y, qs)\n",
    "edges = np.unique(edges)\n",
    "if len(edges) < 3:\n",
    "    raise RuntimeError(\"Not enough unique values in y to bin.\")\n",
    "\n",
    "bin_idx = np.digitize(y, edges[1:-1], right=True)\n",
    "bin_obs = []\n",
    "bin_pred = []\n",
    "bin_count = []\n",
    "for b in range(bin_idx.min(), bin_idx.max() + 1):\n",
    "    m = bin_idx == b\n",
    "    if m.sum() < 200:\n",
    "        continue\n",
    "    bin_obs.append(np.mean(y[m]))\n",
    "    bin_pred.append(np.mean(p[m]))\n",
    "    bin_count.append(int(m.sum()))\n",
    "\n",
    "bin_obs = np.asarray(bin_obs)\n",
    "bin_pred = np.asarray(bin_pred)\n",
    "\n",
    "# Plot mean predicted vs mean observed by bin\n",
    "fig, ax = plt.subplots(2, 1, figsize=(8, 10))\n",
    "\n",
    "ax0 = ax[0]\n",
    "ax0.scatter(bin_obs, bin_pred, s=60, color='steelblue', alpha=0.9)\n",
    "mx = max(bin_obs.max(), bin_pred.max())\n",
    "ax0.plot([0, mx], [0, mx], 'r--', linewidth=2, label='Ideal (y = pred)')\n",
    "ax0.set_xlabel('Mean observed y (bin)')\n",
    "ax0.set_ylabel('Mean predicted (bin)')\n",
    "ax0.set_title('Binned observed vs predicted (positive-only)')\n",
    "ax0.grid(True, alpha=0.2)\n",
    "ax0.legend(loc='best')\n",
    "\n",
    "# Also show mean residual by bin vs mean observed (bias-by-level)\n",
    "ax1 = ax[1]\n",
    "ax1.plot(bin_obs, bin_pred - bin_obs, color='steelblue', linewidth=3)\n",
    "ax1.axhline(0, color='red', linestyle='--', linewidth=2)\n",
    "ax1.set_xlabel('Mean observed y (bin)')\n",
    "ax1.set_ylabel('Mean residual (pred - y)')\n",
    "ax1.set_title('Bias vs level (binned, positive-only)')\n",
    "ax1.grid(True, alpha=0.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Top-tail metrics ---\n",
    "def _mae_rmse(yv, pv):\n",
    "    mae = float(np.mean(np.abs(pv - yv)))\n",
    "    rmse = float(np.sqrt(np.mean((pv - yv) ** 2)))\n",
    "    return mae, rmse\n",
    "\n",
    "mae_all, rmse_all = _mae_rmse(y_all, p_all)\n",
    "print(f\"\\nAll positive-only: MAE={mae_all:.3f}, RMSE={rmse_all:.3f}\")\n",
    "\n",
    "for q in [0.95, 0.99]:\n",
    "    thr = np.quantile(y_all, q)\n",
    "    m = y_all >= thr\n",
    "    mae_q, rmse_q = _mae_rmse(y_all[m], p_all[m])\n",
    "    print(f\"Top {(1-q)*100:.0f}% (y >= {thr:.1f}): N={int(m.sum())}, MAE={mae_q:.3f}, RMSE={rmse_q:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## 1e. Peak cases (top 1%)\n",
    "\n",
    "This section visualizes and lists the worst underpredicted peak events among the positive-only observed targets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peak diagnostics: top-1% scatter + worst underpredicted cases (with node/time indices)\n",
    "rng = np.random.default_rng(2)\n",
    "\n",
    "# Pull full [T, N] arrays (observed-only)\n",
    "targets_t = test_results['targets'].detach().cpu().numpy()[:, 0, :, -1]  # [T, N]\n",
    "preds_t = test_results['preds'].detach().cpu().numpy()[:, 0, :, -1]      # [T, N]\n",
    "mu_t = test_results['mu'].detach().cpu().numpy()[:, 0, :, -1]            # [T, N]\n",
    "theta_t = test_results['theta'].detach().cpu().numpy()[:, 0, :, -1]      # [T, N]\n",
    "pi_t = test_results['pi'].detach().cpu().numpy()[:, 0, :, -1]            # [T, N]\n",
    "\n",
    "valid = ~np.isnan(targets_t)\n",
    "y_int = np.maximum(np.round(np.nan_to_num(targets_t, nan=-1.0)), 0).astype(int)\n",
    "pos = valid & (y_int >= 1)\n",
    "\n",
    "y_pos_full = y_int[pos]\n",
    "p_pos_full = preds_t[pos]\n",
    "\n",
    "# Define \"peaks\" as top 1% of observed positive counts\n",
    "q = 0.99\n",
    "thr = float(np.quantile(y_pos_full, q))\n",
    "peak = pos & (y_int >= thr)\n",
    "\n",
    "n_pos = int(pos.sum())\n",
    "n_peak = int(peak.sum())\n",
    "print(f\"Positive observed points: {n_pos}\")\n",
    "print(f\"Top 1% threshold: y >= {thr:.1f}\")\n",
    "print(f\"Peak points (top 1%): {n_peak}\")\n",
    "\n",
    "# Scatter plot (subsample for readability)\n",
    "max_scatter = 30_000\n",
    "peak_y = y_int[peak]\n",
    "peak_p = preds_t[peak]\n",
    "if len(peak_y) > max_scatter:\n",
    "    idx = rng.choice(len(peak_y), size=max_scatter, replace=False)\n",
    "    sy = peak_y[idx]\n",
    "    sp = peak_p[idx]\n",
    "else:\n",
    "    sy = peak_y\n",
    "    sp = peak_p\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.scatter(sy, sp, s=10, alpha=0.12, color='black', rasterized=True)\n",
    "mx = max(float(np.max(sy)), float(np.max(sp)))\n",
    "plt.plot([0, mx], [0, mx], 'r--', linewidth=2, label='Ideal (y = pred)')\n",
    "plt.xlabel('Observed y (peak subset)')\n",
    "plt.ylabel('Predicted (point)')\n",
    "plt.title('Top-1% peaks: predicted vs observed')\n",
    "plt.grid(True, alpha=0.2)\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Worst underpredicted peak cases table\n",
    "# residual = pred - y (most negative => worst underprediction)\n",
    "residual = preds_t - y_int\n",
    "r_peak = residual[peak]\n",
    "t_idx, n_idx = np.where(peak)\n",
    "\n",
    "# Take K worst underpredictions\n",
    "K = 25\n",
    "order = np.argsort(r_peak)  # ascending = most negative first\n",
    "order = order[:min(K, len(order))]\n",
    "\n",
    "worst = pd.DataFrame({\n",
    "    'time_idx': t_idx[order],\n",
    "    'node': n_idx[order],\n",
    "    'sensor_name': [id_to_name_map.get(int(n), f'Node_{int(n)}') for n in n_idx[order]],\n",
    "    'y': y_int[peak][order],\n",
    "    'pred': preds_t[peak][order],\n",
    "    'residual(pred-y)': r_peak[order],\n",
    "    'pi': pi_t[peak][order],\n",
    "    'mu': mu_t[peak][order],\n",
    "    'theta': theta_t[peak][order],\n",
    "})\n",
    "\n",
    "# Helpful aggregation: are peaks concentrated in a few sensors?\n",
    "unique_nodes = worst['node'].nunique()\n",
    "print(f\"\\nWorst-{len(worst)} peak underpredictions span {unique_nodes} unique nodes\")\n",
    "\n",
    "display(worst)\n",
    "\n",
    "# Optional: show top nodes by number of peak events (overall, not just worst-K)\n",
    "peak_nodes = n_idx\n",
    "counts = pd.Series(peak_nodes).value_counts().head(15)\n",
    "peak_node_df = pd.DataFrame({\n",
    "    'node': counts.index.astype(int),\n",
    "    'sensor_name': [id_to_name_map.get(int(n), f'Node_{int(n)}') for n in counts.index],\n",
    "    'n_peak_events': counts.values,\n",
    "})\n",
    "display(peak_node_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-node calibration analysis\n",
    "def compute_per_node_calibration(targets, mu, theta, pi):\n",
    "    \"\"\"Compute calibration metrics per node.\"\"\"\n",
    "    num_nodes = targets.shape[2]\n",
    "    node_calibration = []\n",
    "\n",
    "    for node in range(num_nodes):\n",
    "        node_targets = targets[:, 0, node, -1].numpy()\n",
    "        node_mu = mu[:, 0, node, -1].numpy()\n",
    "        node_theta = theta[:, 0, node, -1].numpy()\n",
    "        node_pi = pi[:, 0, node, -1].numpy()\n",
    "\n",
    "        valid_mask = ~np.isnan(node_targets)\n",
    "        if valid_mask.sum() < 10:\n",
    "            node_calibration.append({\n",
    "                'node': node, 'ks_stat': np.nan, 'ks_pvalue': np.nan,\n",
    "                'observed_zero_frac': np.nan, 'predicted_zero_frac': np.nan\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        node_targets_valid = np.maximum(np.round(node_targets[valid_mask]), 0)\n",
    "        node_mu_valid = node_mu[valid_mask]\n",
    "        node_theta_valid = node_theta[valid_mask]\n",
    "        node_pi_valid = node_pi[valid_mask]\n",
    "\n",
    "        pit = compute_zinb_cdf(node_targets_valid, node_mu_valid, node_theta_valid, node_pi_valid)\n",
    "        ks_stat, ks_pvalue = sp_stats.kstest(pit, 'uniform')\n",
    "\n",
    "        node_calibration.append({\n",
    "            'node': node,\n",
    "            'name': id_to_name_map.get(node, f'Node_{node}'),\n",
    "            'ks_stat': ks_stat,\n",
    "            'ks_pvalue': ks_pvalue,\n",
    "            'observed_zero_frac': (node_targets_valid == 0).mean(),\n",
    "            'predicted_zero_frac': node_pi_valid.mean(),\n",
    "            'n_samples': len(node_targets_valid)\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(node_calibration)\n",
    "\n",
    "node_calibration_df = compute_per_node_calibration(\n",
    "    test_results['targets'],\n",
    "    test_results['mu'],\n",
    "    test_results['theta'],\n",
    "    test_results['pi']\n",
    ")\n",
    "\n",
    "print(\"Per-Node Calibration Metrics (sorted by KS statistic):\")\n",
    "print(node_calibration_df.sort_values('ks_stat', ascending=False).head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats as sp_stats\n",
    "\n",
    "def zinb_p0(mu, theta, pi, eps=1e-8):\n",
    "    mu = np.asarray(mu)\n",
    "    theta = np.asarray(theta)\n",
    "    pi = np.asarray(pi)\n",
    "    n = np.maximum(theta, eps)\n",
    "    p = np.clip(n / (mu + n + eps), eps, 1 - eps)\n",
    "    nb_p0 = sp_stats.nbinom.pmf(0, n=n, p=p)\n",
    "    return pi + (1 - pi) * nb_p0\n",
    "\n",
    "def zinb_cdf(y, mu, theta, pi, eps=1e-8):\n",
    "    y = np.asarray(y)\n",
    "    mu = np.asarray(mu)\n",
    "    theta = np.asarray(theta)\n",
    "    pi = np.asarray(pi)\n",
    "    n = np.maximum(theta, eps)\n",
    "    p = np.clip(n / (mu + n + eps), eps, 1 - eps)\n",
    "    nb_cdf = sp_stats.nbinom.cdf(y, n=n, p=p)\n",
    "    # for y < 0, CDF should be 0\n",
    "    cdf = np.where(y < 0, 0.0, pi + (1 - pi) * nb_cdf)\n",
    "    return cdf\n",
    "\n",
    "def randomized_pit_discrete(y, mu, theta, pi, rng=None):\n",
    "    \"\"\"\n",
    "    Randomized PIT for discrete distributions:\n",
    "      u ~ Uniform(F(y-1), F(y))\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng() if rng is None else rng\n",
    "    y = np.asarray(y).astype(int)\n",
    "    F_y = zinb_cdf(y, mu, theta, pi)\n",
    "    F_ymin1 = zinb_cdf(y - 1, mu, theta, pi)\n",
    "    v = rng.random(len(y))\n",
    "    return F_ymin1 + v * (F_y - F_ymin1)\n",
    "\n",
    "def prepare_targets_for_scoring(targets_flat, treat_nan_as_zero=False):\n",
    "    t = np.asarray(targets_flat)\n",
    "    if treat_nan_as_zero:\n",
    "        t = np.nan_to_num(t, nan=0.0)\n",
    "        valid = np.ones_like(t, dtype=bool)\n",
    "    else:\n",
    "        valid = ~np.isnan(t)\n",
    "        t = t[valid]\n",
    "    t = np.maximum(np.round(t), 0).astype(int)\n",
    "    return t, valid\n",
    "\n",
    "# Example usage on your flattened arrays:\n",
    "targets_flat = test_results[\"targets\"].numpy().flatten()\n",
    "mu_flat = test_results[\"mu\"].numpy().flatten()\n",
    "theta_flat = test_results[\"theta\"].numpy().flatten()\n",
    "pi_flat = test_results[\"pi\"].numpy().flatten()\n",
    "\n",
    "y_scored, valid_mask = prepare_targets_for_scoring(targets_flat, treat_nan_as_zero=False)  # <- switch to True if appropriate\n",
    "mu_scored = mu_flat[valid_mask] if valid_mask.shape == targets_flat.shape else mu_flat[~np.isnan(targets_flat)]\n",
    "theta_scored = theta_flat[valid_mask] if valid_mask.shape == targets_flat.shape else theta_flat[~np.isnan(targets_flat)]\n",
    "pi_scored = pi_flat[valid_mask] if valid_mask.shape == targets_flat.shape else pi_flat[~np.isnan(targets_flat)]\n",
    "\n",
    "pit = randomized_pit_discrete(y_scored, mu_scored, theta_scored, pi_scored)\n",
    "ks_stat, ks_p = sp_stats.kstest(pit, \"uniform\")\n",
    "\n",
    "print(\"Observed zero frac (scored):\", np.mean(y_scored == 0))\n",
    "print(\"Mean predicted P(Y=0):\", zinb_p0(mu_scored, theta_scored, pi_scored).mean())\n",
    "print(\"Randomized PIT mean:\", pit.mean())\n",
    "print(\"KS stat/p:\", ks_stat, ks_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## 2. Calculate CRPS (Continuous Ranked Probability Score)\n",
    "\n",
    "Implement CRPS scoring for the ZINB distribution to evaluate probabilistic forecast quality. Compare against baseline models using proper scoring rules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_zinb_crps_mc(y, mu, theta, pi, n_samples=1000):\n",
    "    \"\"\"\n",
    "    Compute CRPS using Monte Carlo integration for ZINB distribution.\n",
    "    CRPS = E|Y - y| - 0.5 * E|Y - Y'|\n",
    "    where Y, Y' are independent samples from the forecast distribution.\n",
    "    \"\"\"\n",
    "    eps = 1e-8\n",
    "\n",
    "    # Ensure arrays\n",
    "    y = np.asarray(y).flatten()\n",
    "    mu = np.asarray(mu).flatten()\n",
    "    theta = np.asarray(theta).flatten()\n",
    "    pi = np.asarray(pi).flatten()\n",
    "\n",
    "    n_obs = len(y)\n",
    "\n",
    "    # NB parameters for scipy\n",
    "    n_scipy = np.maximum(theta, eps)\n",
    "    p_scipy = n_scipy / (mu + n_scipy + eps)\n",
    "    p_scipy = np.clip(p_scipy, eps, 1 - eps)\n",
    "\n",
    "    crps_values = np.zeros(n_obs)\n",
    "\n",
    "    for i in range(n_obs):\n",
    "        # Generate samples from ZINB\n",
    "        # First, determine if zero-inflated\n",
    "        is_zero_inflated = np.random.random(n_samples) < pi[i]\n",
    "\n",
    "        # Sample from NB for non-zero-inflated\n",
    "        nb_samples = sp_stats.nbinom.rvs(n=n_scipy[i], p=p_scipy[i], size=n_samples)\n",
    "\n",
    "        # Apply zero-inflation\n",
    "        samples = np.where(is_zero_inflated, 0, nb_samples)\n",
    "\n",
    "        # CRPS = E|Y - y|\n",
    "        term1 = np.abs(samples - y[i]).mean()\n",
    "\n",
    "        # - 0.5 * E|Y - Y'|\n",
    "        samples2 = samples.copy()\n",
    "        np.random.shuffle(samples2)\n",
    "        term2 = 0.5 * np.abs(samples - samples2).mean()\n",
    "\n",
    "        crps_values[i] = term1 - term2\n",
    "\n",
    "    return crps_values\n",
    "\n",
    "def _crps_normal_analytic(y, mu, sigma, eps=1e-8):\n",
    "    \"\"\"Analytic CRPS for Normal(mu, sigma) evaluated at y.\"\"\"\n",
    "    y = np.asarray(y)\n",
    "    mu = np.asarray(mu)\n",
    "    sigma = np.asarray(sigma)\n",
    "    s = np.maximum(sigma, eps)\n",
    "    z = (y - mu) / s\n",
    "    return s * (z * (2 * sp_stats.norm.cdf(z) - 1) + 2 * sp_stats.norm.pdf(z) - 1 / np.sqrt(np.pi))\n",
    "\n",
    "def _fit_group_stats(values, keys, eps=1e-8):\n",
    "    \"\"\"Fit mean/std/count per integer key. Returns keys sorted ascending.\"\"\"\n",
    "    values = np.asarray(values).astype(float)\n",
    "    keys = np.asarray(keys).astype(np.int64)\n",
    "    order = np.argsort(keys)\n",
    "    keys_sorted = keys[order]\n",
    "    values_sorted = values[order]\n",
    "    uniq, idx_start = np.unique(keys_sorted, return_index=True)\n",
    "    counts = np.diff(np.append(idx_start, len(keys_sorted)))\n",
    "    sums = np.add.reduceat(values_sorted, idx_start)\n",
    "    sums_sq = np.add.reduceat(values_sorted * values_sorted, idx_start)\n",
    "    denom = np.maximum(counts, 1)\n",
    "    means = sums / denom\n",
    "    vars_ = sums_sq / denom - means * means\n",
    "    stds = np.sqrt(np.maximum(vars_, eps))\n",
    "    return uniq, means, stds, counts\n",
    "\n",
    "def _lookup_group_stats(keys_score, uniq_keys, means, stds, counts, default_mean, default_std):\n",
    "    \"\"\"Lookup per-score mean/std/count for keys_score, using defaults when missing.\"\"\"\n",
    "    keys_score = np.asarray(keys_score).astype(np.int64)\n",
    "    pos = np.searchsorted(uniq_keys, keys_score)\n",
    "    hit = (pos < len(uniq_keys)) & (uniq_keys[pos] == keys_score)\n",
    "    mu = np.where(hit, means[pos], default_mean)\n",
    "    sigma = np.where(hit, stds[pos], default_std)\n",
    "    cnt = np.where(hit, counts[pos], 0)\n",
    "    return mu, sigma, cnt\n",
    "\n",
    "def compute_baseline_crps(\n",
    "    y,\n",
    "    baseline_type='climatology',\n",
    "    *,\n",
    "    t_idx=None,\n",
    "    node_idx=None,\n",
    "    period=24,\n",
    "    min_count=30,\n",
    "    eps=1e-8,\n",
    "    fallback='node_then_global',\n",
    "    fit_y=None,\n",
    "    fit_t_idx=None,\n",
    "    fit_node_idx=None,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Compute CRPS for baseline models.\n",
    "\n",
    "    Baselines:\n",
    "      - 'climatology': single global Normal fit\n",
    "      - 'seasonal_climatology': Normal fit per season bin (t_idx % period)\n",
    "      - 'seasonal_node_climatology': Normal fit per (node, season) with fallbacks\n",
    "      - 'persistence': |y[t] - y[t-1]| (sequential)\n",
    "\n",
    "    If fit_y is provided, group statistics are fit on (fit_y, fit_*), and CRPS is\n",
    "    evaluated on y. This is important to avoid \"fitting\" climatology on a tiny subset.\n",
    "    \"\"\"\n",
    "    y = np.asarray(y).flatten()\n",
    "    valid_mask = ~np.isnan(y)\n",
    "    y_score = y[valid_mask]\n",
    "\n",
    "    if y_score.size == 0:\n",
    "        return np.array([])\n",
    "\n",
    "    # Fit data defaults to scoring data\n",
    "    if fit_y is None:\n",
    "        fit_y = y\n",
    "        fit_t_idx = t_idx\n",
    "        fit_node_idx = node_idx\n",
    "    fit_y = np.asarray(fit_y).flatten()\n",
    "    fit_valid = ~np.isnan(fit_y)\n",
    "    y_fit = fit_y[fit_valid]\n",
    "\n",
    "    if y_fit.size == 0:\n",
    "        # fall back to scoring set\n",
    "        y_fit = y_score\n",
    "        fit_valid = valid_mask\n",
    "        fit_t_idx = t_idx\n",
    "        fit_node_idx = node_idx\n",
    "\n",
    "    global_mean = y_fit.mean()\n",
    "    global_std = y_fit.std()\n",
    "\n",
    "    if baseline_type in ('climatology', 'normal_climatology'):\n",
    "        return np.abs(_crps_normal_analytic(y_score, global_mean, global_std, eps=eps))\n",
    "\n",
    "    if baseline_type == 'seasonal_climatology':\n",
    "        if t_idx is None:\n",
    "            raise ValueError(\"t_idx is required for seasonal_climatology\")\n",
    "        if fit_t_idx is None:\n",
    "            raise ValueError(\"fit_t_idx is required when fit_y is provided\")\n",
    "\n",
    "        t_fit = np.asarray(fit_t_idx).flatten()[fit_valid]\n",
    "        season_fit = (t_fit % int(period)).astype(np.int64)\n",
    "        uniq, mu_s, sig_s, cnt_s = _fit_group_stats(y_fit, season_fit, eps=eps)\n",
    "\n",
    "        t_sc = np.asarray(t_idx).flatten()[valid_mask]\n",
    "        season_sc = (t_sc % int(period)).astype(np.int64)\n",
    "        mu, sigma, cnt = _lookup_group_stats(season_sc, uniq, mu_s, sig_s, cnt_s, global_mean, global_std)\n",
    "        mu = np.where(cnt >= min_count, mu, global_mean)\n",
    "        sigma = np.where(cnt >= min_count, sigma, global_std)\n",
    "        return np.abs(_crps_normal_analytic(y_score, mu, sigma, eps=eps))\n",
    "\n",
    "    if baseline_type == 'seasonal_node_climatology':\n",
    "        if t_idx is None or node_idx is None:\n",
    "            raise ValueError(\"t_idx and node_idx are required for seasonal_node_climatology\")\n",
    "        if fit_t_idx is None or fit_node_idx is None:\n",
    "            raise ValueError(\"fit_t_idx and fit_node_idx are required when fit_y is provided\")\n",
    "\n",
    "        t_fit = np.asarray(fit_t_idx).flatten()[fit_valid]\n",
    "        n_fit = np.asarray(fit_node_idx).flatten()[fit_valid].astype(np.int64)\n",
    "        season_fit = (t_fit % int(period)).astype(np.int64)\n",
    "        key_ns_fit = n_fit * int(period) + season_fit\n",
    "        uniq_ns, mu_ns, sig_ns, cnt_ns = _fit_group_stats(y_fit, key_ns_fit, eps=eps)\n",
    "\n",
    "        if fallback not in ('node_then_global', 'global_only'):\n",
    "            raise ValueError(\"fallback must be 'node_then_global' or 'global_only'\")\n",
    "\n",
    "        t_sc = np.asarray(t_idx).flatten()[valid_mask]\n",
    "        n_sc = np.asarray(node_idx).flatten()[valid_mask].astype(np.int64)\n",
    "        season_sc = (t_sc % int(period)).astype(np.int64)\n",
    "        key_ns_sc = n_sc * int(period) + season_sc\n",
    "        mu, sigma, cnt = _lookup_group_stats(key_ns_sc, uniq_ns, mu_ns, sig_ns, cnt_ns, global_mean, global_std)\n",
    "\n",
    "        if fallback == 'global_only':\n",
    "            mu = np.where(cnt >= min_count, mu, global_mean)\n",
    "            sigma = np.where(cnt >= min_count, sigma, global_std)\n",
    "            return np.abs(_crps_normal_analytic(y_score, mu, sigma, eps=eps))\n",
    "\n",
    "        # Node-only fallback\n",
    "        uniq_n, mu_n, sig_n, cnt_n = _fit_group_stats(y_fit, n_fit, eps=eps)\n",
    "        mu_f, sig_f, cnt_f = _lookup_group_stats(n_sc, uniq_n, mu_n, sig_n, cnt_n, global_mean, global_std)\n",
    "\n",
    "        mu = np.where(cnt >= min_count, mu, mu_f)\n",
    "        sigma = np.where(cnt >= min_count, sigma, sig_f)\n",
    "        mu = np.where(cnt_f >= min_count, mu, global_mean)\n",
    "        sigma = np.where(cnt_f >= min_count, sigma, global_std)\n",
    "        return np.abs(_crps_normal_analytic(y_score, mu, sigma, eps=eps))\n",
    "\n",
    "    if baseline_type == 'persistence':\n",
    "        crps = np.abs(y_score[1:] - y_score[:-1])\n",
    "        return np.concatenate([[np.nan], crps])\n",
    "\n",
    "    raise ValueError(f\"Unknown baseline_type: {baseline_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute CRPS for a subset of test data (MC is expensive)\n",
    "n_subset = min(1000, len(targets_valid))\n",
    "subset_idx = np.random.choice(len(targets_valid), n_subset, replace=False)\n",
    "\n",
    "print(f\"Computing CRPS for {n_subset} samples (Monte Carlo with 500 samples each)...\")\n",
    "\n",
    "zinb_crps = compute_zinb_crps_mc(\n",
    "    targets_valid[subset_idx],\n",
    "    mu_valid[subset_idx],\n",
    "    theta_valid[subset_idx],\n",
    "    pi_valid[subset_idx],\n",
    "    n_samples=500\n",
    ")\n",
    "\n",
    "# ---- Advanced climatology configuration ----\n",
    "# NOTE: period is \"time steps per seasonal cycle\". If your data is 15-min, set 96 for a daily cycle.\n",
    "CLIMO_PERIOD = 96\n",
    "CLIMO_MIN_COUNT = 30\n",
    "\n",
    "# Where to fit climatology stats from: 'train' avoids leakage; falls back to 'test' if train_results is missing.\n",
    "CLIMO_FIT_SOURCE = 'train'\n",
    "\n",
    "# Build time/node indices aligned with targets_valid (assumes targets_valid comes from flatten+~nan masking of test targets)\n",
    "test_targets_tensor = test_results['targets']\n",
    "test_targets_np = test_targets_tensor.numpy() if hasattr(test_targets_tensor, 'numpy') else np.asarray(test_targets_tensor)\n",
    "test_targets_flat_raw = test_targets_np.reshape(-1)\n",
    "test_valid_mask_idx = ~np.isnan(test_targets_flat_raw)\n",
    "test_idx = np.indices(test_targets_np.shape)\n",
    "t_valid_full = test_idx[0].reshape(-1)[test_valid_mask_idx]\n",
    "if test_targets_np.ndim >= 3:\n",
    "    node_valid_full = test_idx[2].reshape(-1)[test_valid_mask_idx]\n",
    "else:\n",
    "    node_valid_full = np.zeros_like(t_valid_full)\n",
    "\n",
    "if len(t_valid_full) != len(targets_valid):\n",
    "    print(\"Warning: (t,node) index alignment mismatch with targets_valid; falling back to sequential indices.\")\n",
    "    t_valid_full = np.arange(len(targets_valid), dtype=np.int64)\n",
    "    node_valid_full = np.zeros(len(targets_valid), dtype=np.int64)\n",
    "\n",
    "# Fit set for climatology parameters (train preferred)\n",
    "if CLIMO_FIT_SOURCE == 'train' and 'train_results' in globals() and isinstance(train_results, dict) and 'targets' in train_results:\n",
    "    fit_targets_tensor = train_results['targets']\n",
    "    fit_targets_np = fit_targets_tensor.numpy() if hasattr(fit_targets_tensor, 'numpy') else np.asarray(fit_targets_tensor)\n",
    "    fit_targets_flat_raw = fit_targets_np.reshape(-1)\n",
    "    fit_valid_mask = ~np.isnan(fit_targets_flat_raw)\n",
    "    fit_idx = np.indices(fit_targets_np.shape)\n",
    "    fit_t = fit_idx[0].reshape(-1)[fit_valid_mask]\n",
    "    fit_node = fit_idx[2].reshape(-1)[fit_valid_mask] if fit_targets_np.ndim >= 3 else np.zeros_like(fit_t)\n",
    "    fit_y = np.maximum(np.round(fit_targets_flat_raw[fit_valid_mask]), 0)\n",
    "else:\n",
    "    fit_y = targets_valid\n",
    "    fit_t = t_valid_full\n",
    "    fit_node = node_valid_full\n",
    "\n",
    "# Baseline CRPS: simple global climatology\n",
    "climatology_crps = compute_baseline_crps(\n",
    "    targets_valid[subset_idx],\n",
    "    baseline_type='climatology',\n",
    "    fit_y=fit_y,\n",
    ")\n",
    "\n",
    "# Baseline CRPS: seasonal + per-node climatology (advanced)\n",
    "seasonal_node_climo_crps = compute_baseline_crps(\n",
    "    targets_valid[subset_idx],\n",
    "    baseline_type='seasonal_node_climatology',\n",
    "    t_idx=t_valid_full[subset_idx],\n",
    "    node_idx=node_valid_full[subset_idx],\n",
    "    fit_y=fit_y,\n",
    "    fit_t_idx=fit_t,\n",
    "    fit_node_idx=fit_node,\n",
    "    period=CLIMO_PERIOD,\n",
    "    min_count=CLIMO_MIN_COUNT,\n",
    ")\n",
    "\n",
    "print(f\"\\nCRPS Summary:\")\n",
    "print(f\"  ZINB Model CRPS (mean): {zinb_crps.mean():.4f}\")\n",
    "print(f\"  ZINB Model CRPS (std): {zinb_crps.std():.4f}\")\n",
    "print(f\"  Climatology CRPS (mean): {climatology_crps.mean():.4f}\")\n",
    "print(f\"  Climatology CRPS (std): {climatology_crps.std():.4f}\")\n",
    "print(f\"  Seasonal+Node Climatology CRPS (mean): {seasonal_node_climo_crps.mean():.4f}\")\n",
    "print(f\"  Seasonal+Node Climatology CRPS (std): {seasonal_node_climo_crps.std():.4f}\")\n",
    "print(f\"  Skill vs Climatology: {1 - zinb_crps.mean() / climatology_crps.mean():.4f}\")\n",
    "print(f\"  Skill vs Seasonal+Node Climatology: {1 - zinb_crps.mean() / seasonal_node_climo_crps.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize CRPS comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# CRPS distribution comparison\n",
    "ax1 = axes[0]\n",
    "ax1.hist(zinb_crps, bins=30, alpha=0.7, label='ZINB Model', density=True, color='steelblue')\n",
    "ax1.hist(climatology_crps, bins=30, alpha=0.7, label='Climatology', density=True, color='orange')\n",
    "ax1.set_xlabel('CRPS')\n",
    "ax1.set_ylabel('Density')\n",
    "ax1.set_title('CRPS Distribution Comparison')\n",
    "ax1.legend()\n",
    "\n",
    "# CRPS vs observed value\n",
    "ax2 = axes[1]\n",
    "scatter = ax2.scatter(targets_valid[subset_idx], zinb_crps, alpha=0.3, s=10, c='steelblue')\n",
    "ax2.set_xlabel('Observed Value')\n",
    "ax2.set_ylabel('CRPS')\n",
    "ax2.set_title('CRPS vs Observed Value')\n",
    "\n",
    "# Compute binned average\n",
    "bins = np.percentile(targets_valid[subset_idx], np.linspace(0, 100, 11))\n",
    "bin_centers = []\n",
    "bin_crps_means = []\n",
    "for i in range(len(bins)-1):\n",
    "    mask = (targets_valid[subset_idx] >= bins[i]) & (targets_valid[subset_idx] < bins[i+1])\n",
    "    if mask.sum() > 0:\n",
    "        bin_centers.append((bins[i] + bins[i+1]) / 2)\n",
    "        bin_crps_means.append(zinb_crps[mask].mean())\n",
    "\n",
    "ax2.plot(bin_centers, bin_crps_means, 'ro-', linewidth=2, markersize=8, label='Binned mean')\n",
    "ax2.legend()\n",
    "\n",
    "# CRPS per node (for first few nodes)\n",
    "ax3 = axes[2]\n",
    "node_crps = []\n",
    "for node in range(min(10, num_nodes)):\n",
    "    node_targets = test_results['targets'][:, 0, node, -1].numpy()\n",
    "    node_mu = test_results['mu'][:, 0, node, -1].numpy()\n",
    "    node_theta = test_results['theta'][:, 0, node, -1].numpy()\n",
    "    node_pi = test_results['pi'][:, 0, node, -1].numpy()\n",
    "\n",
    "    valid_mask = ~np.isnan(node_targets)\n",
    "    if valid_mask.sum() > 50:\n",
    "        node_targets_valid = np.maximum(np.round(node_targets[valid_mask][:100]), 0)\n",
    "        crps = compute_zinb_crps_mc(\n",
    "            node_targets_valid,\n",
    "            node_mu[valid_mask][:100],\n",
    "            node_theta[valid_mask][:100],\n",
    "            node_pi[valid_mask][:100],\n",
    "            n_samples=200\n",
    "        )\n",
    "        node_crps.append({'node': node, 'name': id_to_name_map.get(node, f'N{node}')[:15], 'crps': crps.mean()})\n",
    "\n",
    "if node_crps:\n",
    "    crps_df = pd.DataFrame(node_crps)\n",
    "    ax3.barh(range(len(crps_df)), crps_df['crps'], color='steelblue')\n",
    "    ax3.set_yticks(range(len(crps_df)))\n",
    "    ax3.set_yticklabels(crps_df['name'])\n",
    "    ax3.set_xlabel('Mean CRPS')\n",
    "    ax3.set_title('CRPS by Node (First 10)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## 3. Analyze Prediction Intervals Coverage\n",
    "\n",
    "Compute empirical coverage rates for 50%, 80%, and 95% prediction intervals across all nodes. Identify nodes with under/over-confident predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_prediction_intervals(mu, theta, pi, coverage_levels=[0.5, 0.8, 0.95]):\n",
    "    \"\"\"\n",
    "    Compute prediction intervals for ZINB distribution.\n",
    "    Returns lower and upper bounds for each coverage level.\n",
    "    \"\"\"\n",
    "    eps = 1e-8\n",
    "\n",
    "    mu = np.asarray(mu)\n",
    "    theta = np.asarray(theta)\n",
    "    pi = np.asarray(pi)\n",
    "\n",
    "    n_scipy = np.maximum(theta, eps)\n",
    "    p_scipy = n_scipy / (mu + n_scipy + eps)\n",
    "    p_scipy = np.clip(p_scipy, eps, 1-eps)\n",
    "\n",
    "    intervals = {}\n",
    "\n",
    "    for coverage in coverage_levels:\n",
    "        alpha = 1 - coverage\n",
    "        q_lower = alpha / 2\n",
    "        q_upper = 1 - alpha / 2\n",
    "\n",
    "        # Probability of zero in ZINB\n",
    "        prob_zero = pi + (1 - pi) * sp_stats.nbinom.pmf(0, n=n_scipy, p=p_scipy)\n",
    "\n",
    "        # Lower bound\n",
    "        q_lower_adj = np.clip((q_lower - pi) / (1 - pi + eps), eps, 1-eps)\n",
    "        nb_lower = sp_stats.nbinom.ppf(q_lower_adj, n=n_scipy, p=p_scipy)\n",
    "        lower = np.where(q_lower <= prob_zero, 0.0, nb_lower)\n",
    "\n",
    "        # Upper bound\n",
    "        q_upper_adj = np.clip((q_upper - pi) / (1 - pi + eps), eps, 1-eps)\n",
    "        nb_upper = sp_stats.nbinom.ppf(q_upper_adj, n=n_scipy, p=p_scipy)\n",
    "        upper = np.where(q_upper <= prob_zero, 0.0, nb_upper)\n",
    "\n",
    "        intervals[coverage] = {'lower': lower, 'upper': upper}\n",
    "\n",
    "    return intervals\n",
    "\n",
    "def compute_coverage_rates(targets, intervals):\n",
    "    \"\"\"Compute empirical coverage rates.\"\"\"\n",
    "    coverage_rates = {}\n",
    "\n",
    "    for coverage, bounds in intervals.items():\n",
    "        lower = bounds['lower']\n",
    "        upper = bounds['upper']\n",
    "\n",
    "        in_interval = (targets >= lower) & (targets <= upper)\n",
    "        coverage_rates[coverage] = {\n",
    "            'empirical': in_interval.mean(),\n",
    "            'nominal': coverage,\n",
    "            'calibration_error': in_interval.mean() - coverage\n",
    "        }\n",
    "\n",
    "    return coverage_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute prediction intervals for test set\n",
    "coverage_levels = [0.5, 0.8, 0.95]\n",
    "\n",
    "intervals = compute_prediction_intervals(\n",
    "    mu_valid, theta_valid, pi_valid, coverage_levels\n",
    ")\n",
    "\n",
    "coverage_rates = compute_coverage_rates(targets_valid, intervals)\n",
    "\n",
    "print(\"Prediction Interval Coverage Analysis:\")\n",
    "print(\"-\" * 50)\n",
    "for coverage, rates in coverage_rates.items():\n",
    "    print(f\"{int(coverage*100)}% Interval:\")\n",
    "    print(f\"  Nominal coverage: {rates['nominal']:.1%}\")\n",
    "    print(f\"  Empirical coverage: {rates['empirical']:.1%}\")\n",
    "    print(f\"  Calibration error: {rates['calibration_error']:+.1%}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize coverage analysis\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Coverage comparison bar chart\n",
    "ax1 = axes[0]\n",
    "x = np.arange(len(coverage_levels))\n",
    "width = 0.35\n",
    "\n",
    "nominal = [coverage_rates[c]['nominal'] for c in coverage_levels]\n",
    "empirical = [coverage_rates[c]['empirical'] for c in coverage_levels]\n",
    "\n",
    "bars1 = ax1.bar(x - width/2, nominal, width, label='Nominal', color='steelblue', alpha=0.7)\n",
    "bars2 = ax1.bar(x + width/2, empirical, width, label='Empirical', color='orange', alpha=0.7)\n",
    "\n",
    "ax1.set_ylabel('Coverage Rate')\n",
    "ax1.set_title('Prediction Interval Coverage')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels([f'{int(c*100)}%' for c in coverage_levels])\n",
    "ax1.legend()\n",
    "ax1.set_ylim(0, 1.1)\n",
    "\n",
    "# Add reference line at perfect calibration\n",
    "for i, c in enumerate(coverage_levels):\n",
    "    ax1.axhline(y=c, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Per-node coverage for 95% interval\n",
    "ax2 = axes[1]\n",
    "node_coverage_95 = []\n",
    "\n",
    "for node in range(num_nodes):\n",
    "    node_targets = test_results['targets'][:, 0, node, -1].numpy()\n",
    "    node_mu = test_results['mu'][:, 0, node, -1].numpy()\n",
    "    node_theta = test_results['theta'][:, 0, node, -1].numpy()\n",
    "    node_pi = test_results['pi'][:, 0, node, -1].numpy()\n",
    "\n",
    "    valid_mask = ~np.isnan(node_targets)\n",
    "    if valid_mask.sum() < 10:\n",
    "        node_coverage_95.append(np.nan)\n",
    "        continue\n",
    "\n",
    "    node_targets_valid = np.maximum(np.round(node_targets[valid_mask]), 0)\n",
    "\n",
    "    node_intervals = compute_prediction_intervals(\n",
    "        node_mu[valid_mask], node_theta[valid_mask], node_pi[valid_mask], [0.95]\n",
    "    )\n",
    "\n",
    "    in_interval = (node_targets_valid >= node_intervals[0.95]['lower']) & \\\n",
    "                  (node_targets_valid <= node_intervals[0.95]['upper'])\n",
    "    node_coverage_95.append(in_interval.mean())\n",
    "\n",
    "ax2.bar(range(num_nodes), node_coverage_95, color='steelblue', alpha=0.7)\n",
    "ax2.axhline(y=0.95, color='red', linestyle='--', linewidth=2, label='Nominal 95%')\n",
    "ax2.set_xlabel('Node Index')\n",
    "ax2.set_ylabel('Empirical Coverage')\n",
    "ax2.set_title('95% PI Coverage by Node')\n",
    "ax2.legend()\n",
    "\n",
    "# Identify over/under-confident nodes\n",
    "ax3 = axes[2]\n",
    "node_coverage_95 = np.array(node_coverage_95)\n",
    "calibration_errors = node_coverage_95 - 0.95\n",
    "\n",
    "colors = ['red' if e < -0.1 else 'green' if e > 0.1 else 'steelblue' for e in calibration_errors]\n",
    "ax3.bar(range(num_nodes), calibration_errors, color=colors, alpha=0.7)\n",
    "ax3.axhline(y=0, color='black', linewidth=1)\n",
    "ax3.axhline(y=0.1, color='green', linestyle='--', alpha=0.5, label='Over-confident threshold')\n",
    "ax3.axhline(y=-0.1, color='red', linestyle='--', alpha=0.5, label='Under-confident threshold')\n",
    "ax3.set_xlabel('Node Index')\n",
    "ax3.set_ylabel('Coverage Error (Empirical - Nominal)')\n",
    "ax3.set_title('95% PI Calibration Error by Node')\n",
    "ax3.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print nodes with significant calibration issues\n",
    "print(\"\\nNodes with significant calibration issues (|error| > 10%):\")\n",
    "for node in range(num_nodes):\n",
    "    if not np.isnan(calibration_errors[node]) and abs(calibration_errors[node]) > 0.1:\n",
    "        status = \"UNDER-confident\" if calibration_errors[node] < 0 else \"OVER-confident\"\n",
    "        print(f\"  Node {node} ({id_to_name_map.get(node, 'Unknown')}): {calibration_errors[node]:+.1%} ({status})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "## 4. Visualize Attention Weights from GAT Layers\n",
    "\n",
    "Extract and visualize attention coefficients from GATv2Conv layers to understand which node connections the model prioritizes. Create heatmaps and graph visualizations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_attention_matrix(attention_tuple, num_nodes):\n",
    "    \"\"\"\n",
    "    Convert attention weights from sparse format to dense matrix.\n",
    "    attention_tuple = (edge_index, attention_weights)\n",
    "    \"\"\"\n",
    "    edge_index, attention_weights = attention_tuple\n",
    "\n",
    "    # Move to CPU if needed\n",
    "    if isinstance(edge_index, torch.Tensor):\n",
    "        edge_index = edge_index.cpu().numpy()\n",
    "    if isinstance(attention_weights, torch.Tensor):\n",
    "        attention_weights = attention_weights.cpu().numpy()\n",
    "\n",
    "    # attention_weights shape: [num_edges, num_heads]\n",
    "    # Average across heads\n",
    "    if len(attention_weights.shape) > 1:\n",
    "        attention_weights = attention_weights.mean(axis=1)\n",
    "\n",
    "    # Create dense attention matrix\n",
    "    attn_matrix = np.zeros((num_nodes, num_nodes))\n",
    "\n",
    "    for i in range(edge_index.shape[1]):\n",
    "        src, dst = edge_index[0, i], edge_index[1, i]\n",
    "        if src < num_nodes and dst < num_nodes:\n",
    "            attn_matrix[dst, src] = attention_weights[i]\n",
    "\n",
    "    return attn_matrix\n",
    "\n",
    "def aggregate_attention_weights(attention_list, num_nodes, n_samples=50):\n",
    "    \"\"\"Aggregate attention weights across multiple samples.\"\"\"\n",
    "    aggregated = np.zeros((num_nodes, num_nodes))\n",
    "    count = 0\n",
    "\n",
    "    for attn in attention_list[:n_samples]:\n",
    "        try:\n",
    "            attn_matrix = extract_attention_matrix(attn, num_nodes)\n",
    "            aggregated += attn_matrix\n",
    "            count += 1\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "    if count > 0:\n",
    "        aggregated /= count\n",
    "\n",
    "    return aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate attention weights\n",
    "print(\"Aggregating attention weights from first 50 samples...\")\n",
    "attn_matrix_1 = aggregate_attention_weights(test_results['attention_1'], num_nodes, n_samples=50)\n",
    "attn_matrix_2 = aggregate_attention_weights(test_results['attention_2'], num_nodes, n_samples=50)\n",
    "\n",
    "print(f\"Attention matrix 1 shape: {attn_matrix_1.shape}\")\n",
    "print(f\"Attention matrix 2 shape: {attn_matrix_2.shape}\")\n",
    "print(f\"Non-zero entries in layer 1: {(attn_matrix_1 > 0).sum()}\")\n",
    "print(f\"Non-zero entries in layer 2: {(attn_matrix_2 > 0).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention weights\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Heatmap for layer 1\n",
    "ax1 = axes[0]\n",
    "im1 = ax1.imshow(attn_matrix_1, cmap='Blues', aspect='auto')\n",
    "ax1.set_xlabel('Source Node')\n",
    "ax1.set_ylabel('Target Node')\n",
    "ax1.set_title('GAT Layer 1 Attention Weights')\n",
    "plt.colorbar(im1, ax=ax1, label='Attention Weight')\n",
    "\n",
    "# Heatmap for layer 2\n",
    "ax2 = axes[1]\n",
    "im2 = ax2.imshow(attn_matrix_2, cmap='Oranges', aspect='auto')\n",
    "ax2.set_xlabel('Source Node')\n",
    "ax2.set_ylabel('Target Node')\n",
    "ax2.set_title('GAT Layer 2 Attention Weights')\n",
    "plt.colorbar(im2, ax=ax2, label='Attention Weight')\n",
    "\n",
    "# Average attention received per node\n",
    "ax3 = axes[2]\n",
    "avg_attn_received_1 = attn_matrix_1.sum(axis=1)\n",
    "avg_attn_received_2 = attn_matrix_2.sum(axis=1)\n",
    "\n",
    "x = np.arange(num_nodes)\n",
    "width = 0.35\n",
    "ax3.bar(x - width/2, avg_attn_received_1, width, label='Layer 1', color='steelblue', alpha=0.7)\n",
    "ax3.bar(x + width/2, avg_attn_received_2, width, label='Layer 2', color='orange', alpha=0.7)\n",
    "ax3.set_xlabel('Node Index')\n",
    "ax3.set_ylabel('Total Attention Received')\n",
    "ax3.set_title('Attention Received per Node')\n",
    "ax3.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create graph visualization with attention weights\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# Create graph from edge_index\n",
    "edge_index_np = edge_index.numpy()\n",
    "G = nx.DiGraph()\n",
    "G.add_nodes_from(range(num_nodes))\n",
    "\n",
    "# Add edges with attention weights\n",
    "for i in range(edge_index_np.shape[1]):\n",
    "    src, dst = edge_index_np[0, i], edge_index_np[1, i]\n",
    "    weight = attn_matrix_1[dst, src]\n",
    "    if weight > 0:\n",
    "        G.add_edge(src, dst, weight=weight)\n",
    "\n",
    "# Get positions using spring layout\n",
    "pos = nx.spring_layout(G, seed=42, k=2)\n",
    "\n",
    "# Layer 1 attention visualization\n",
    "ax1 = axes[0]\n",
    "edges = G.edges()\n",
    "weights = [G[u][v]['weight'] * 10 for u, v in edges]  # Scale for visibility\n",
    "\n",
    "node_colors = avg_attn_received_1\n",
    "nodes = nx.draw_networkx_nodes(G, pos, node_color=node_colors,\n",
    "                                cmap=plt.cm.Blues, node_size=300, ax=ax1)\n",
    "nx.draw_networkx_edges(G, pos, edge_color='gray', alpha=0.5,\n",
    "                       width=weights, arrows=True, arrowsize=10, ax=ax1)\n",
    "\n",
    "# Add node labels\n",
    "labels = {i: id_to_name_map.get(i, str(i))[:8] for i in range(num_nodes)}\n",
    "nx.draw_networkx_labels(G, pos, labels, font_size=6, ax=ax1)\n",
    "\n",
    "ax1.set_title('Graph with Layer 1 Attention (Node color = attention received)')\n",
    "plt.colorbar(nodes, ax=ax1, label='Attention Received')\n",
    "\n",
    "# Update graph with layer 2 attention\n",
    "for i in range(edge_index_np.shape[1]):\n",
    "    src, dst = edge_index_np[0, i], edge_index_np[1, i]\n",
    "    if G.has_edge(src, dst):\n",
    "        G[src][dst]['weight'] = attn_matrix_2[dst, src]\n",
    "\n",
    "ax2 = axes[1]\n",
    "weights = [G[u][v]['weight'] * 10 for u, v in edges]\n",
    "node_colors = avg_attn_received_2\n",
    "\n",
    "nodes = nx.draw_networkx_nodes(G, pos, node_color=node_colors,\n",
    "                                cmap=plt.cm.Oranges, node_size=300, ax=ax2)\n",
    "nx.draw_networkx_edges(G, pos, edge_color='gray', alpha=0.5,\n",
    "                       width=weights, arrows=True, arrowsize=10, ax=ax2)\n",
    "nx.draw_networkx_labels(G, pos, labels, font_size=6, ax=ax2)\n",
    "\n",
    "ax2.set_title('Graph with Layer 2 Attention (Node color = attention received)')\n",
    "plt.colorbar(nodes, ax=ax2, label='Attention Received')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify most important edges by attention\n",
    "print(\"\\nTop 10 Most Important Edges (by Layer 1 attention):\")\n",
    "edge_importance = []\n",
    "for i in range(edge_index_np.shape[1]):\n",
    "    src, dst = edge_index_np[0, i], edge_index_np[1, i]\n",
    "    weight = attn_matrix_1[dst, src]\n",
    "    if weight > 0:\n",
    "        edge_importance.append({\n",
    "            'source': src,\n",
    "            'source_name': id_to_name_map.get(src, f'Node_{src}')[:20],\n",
    "            'target': dst,\n",
    "            'target_name': id_to_name_map.get(dst, f'Node_{dst}')[:20],\n",
    "            'attention': weight\n",
    "        })\n",
    "\n",
    "edge_df = pd.DataFrame(edge_importance).sort_values('attention', ascending=False)\n",
    "print(edge_df.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "## 5. Compare Node-Level Performance Metrics\n",
    "\n",
    "Calculate per-node MAE, RMSE, and NLL metrics. Create bar charts comparing performance across sensor nodes and identify systematic prediction errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_node_metrics(targets, preds, mu, theta, pi):\n",
    "    \"\"\"Compute per-node performance metrics.\"\"\"\n",
    "    num_nodes = targets.shape[2]\n",
    "    metrics = []\n",
    "\n",
    "    for node in range(num_nodes):\n",
    "        node_targets = targets[:, 0, node, -1].numpy()\n",
    "        node_preds = preds[:, 0, node, -1].numpy()\n",
    "        node_mu = mu[:, 0, node, -1].numpy()\n",
    "        node_theta = theta[:, 0, node, -1].numpy()\n",
    "        node_pi = pi[:, 0, node, -1].numpy()\n",
    "\n",
    "        valid_mask = ~np.isnan(node_targets)\n",
    "        n_valid = valid_mask.sum()\n",
    "\n",
    "        if n_valid < 10:\n",
    "            metrics.append({\n",
    "                'node': node,\n",
    "                'name': id_to_name_map.get(node, f'Node_{node}'),\n",
    "                'mae': np.nan, 'rmse': np.nan, 'nll': np.nan,\n",
    "                'bias': np.nan, 'n_samples': n_valid\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        targets_valid = node_targets[valid_mask]\n",
    "        preds_valid = node_preds[valid_mask]\n",
    "        mu_valid = node_mu[valid_mask]\n",
    "        theta_valid = node_theta[valid_mask]\n",
    "        pi_valid = node_pi[valid_mask]\n",
    "\n",
    "        # MAE\n",
    "        mae = np.abs(targets_valid - preds_valid).mean()\n",
    "\n",
    "        # RMSE\n",
    "        rmse = np.sqrt(((targets_valid - preds_valid) ** 2).mean())\n",
    "\n",
    "        # Bias (mean error)\n",
    "        bias = (preds_valid - targets_valid).mean()\n",
    "\n",
    "        # NLL (ZINB negative log-likelihood)\n",
    "        eps = 1e-8\n",
    "        targets_count = np.maximum(np.round(targets_valid), 0)\n",
    "        n_scipy = np.maximum(theta_valid, eps)\n",
    "        p_scipy = n_scipy / (mu_valid + n_scipy + eps)\n",
    "        p_scipy = np.clip(p_scipy, eps, 1-eps)\n",
    "\n",
    "        # ZINB log probability\n",
    "        nb_log_prob = sp_stats.nbinom.logpmf(targets_count.astype(int), n=n_scipy, p=p_scipy)\n",
    "\n",
    "        zero_mask = targets_count == 0\n",
    "        log_prob = np.where(\n",
    "            zero_mask,\n",
    "            np.log(pi_valid + (1 - pi_valid) * np.exp(nb_log_prob) + eps),\n",
    "            np.log(1 - pi_valid + eps) + nb_log_prob\n",
    "        )\n",
    "        nll = -log_prob.mean()\n",
    "\n",
    "        metrics.append({\n",
    "            'node': node,\n",
    "            'name': id_to_name_map.get(node, f'Node_{node}'),\n",
    "            'mae': mae,\n",
    "            'rmse': rmse,\n",
    "            'nll': nll,\n",
    "            'bias': bias,\n",
    "            'n_samples': n_valid,\n",
    "            'mean_target': targets_valid.mean(),\n",
    "            'std_target': targets_valid.std()\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(metrics)\n",
    "\n",
    "# Compute metrics for test set\n",
    "node_metrics_df = compute_node_metrics(\n",
    "    test_results['targets'],\n",
    "    test_results['preds'],\n",
    "    test_results['mu'],\n",
    "    test_results['theta'],\n",
    "    test_results['pi']\n",
    ")\n",
    "\n",
    "print(\"Node-Level Performance Metrics Summary:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Mean MAE: {node_metrics_df['mae'].mean():.4f}\")\n",
    "print(f\"Mean RMSE: {node_metrics_df['rmse'].mean():.4f}\")\n",
    "print(f\"Mean NLL: {node_metrics_df['nll'].mean():.4f}\")\n",
    "print(f\"Mean Bias: {node_metrics_df['bias'].mean():.4f}\")\n",
    "print()\n",
    "print(\"Top 5 Best Performing Nodes (by MAE):\")\n",
    "print(node_metrics_df.nsmallest(5, 'mae')[['node', 'name', 'mae', 'rmse', 'nll', 'bias']].to_string(index=False))\n",
    "print()\n",
    "print(\"Top 5 Worst Performing Nodes (by MAE):\")\n",
    "print(node_metrics_df.nlargest(5, 'mae')[['node', 'name', 'mae', 'rmse', 'nll', 'bias']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize node-level metrics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# MAE by node\n",
    "ax1 = axes[0, 0]\n",
    "colors = plt.cm.RdYlGn_r(np.linspace(0.2, 0.8, num_nodes))\n",
    "sorted_df = node_metrics_df.sort_values('mae')\n",
    "ax1.barh(range(num_nodes), sorted_df['mae'], color=colors)\n",
    "ax1.set_yticks(range(num_nodes))\n",
    "ax1.set_yticklabels([name[:15] for name in sorted_df['name']], fontsize=8)\n",
    "ax1.set_xlabel('MAE')\n",
    "ax1.set_title('MAE by Node (sorted)')\n",
    "ax1.axvline(x=node_metrics_df['mae'].mean(), color='red', linestyle='--', label=f'Mean: {node_metrics_df[\"mae\"].mean():.2f}')\n",
    "ax1.legend()\n",
    "\n",
    "# RMSE by node\n",
    "ax2 = axes[0, 1]\n",
    "ax2.barh(range(num_nodes), sorted_df['rmse'], color=colors)\n",
    "ax2.set_yticks(range(num_nodes))\n",
    "ax2.set_yticklabels([name[:15] for name in sorted_df['name']], fontsize=8)\n",
    "ax2.set_xlabel('RMSE')\n",
    "ax2.set_title('RMSE by Node (sorted by MAE)')\n",
    "ax2.axvline(x=node_metrics_df['rmse'].mean(), color='red', linestyle='--', label=f'Mean: {node_metrics_df[\"rmse\"].mean():.2f}')\n",
    "ax2.legend()\n",
    "\n",
    "# NLL by node\n",
    "ax3 = axes[1, 0]\n",
    "nll_sorted_df = node_metrics_df.sort_values('nll')\n",
    "ax3.barh(range(num_nodes), nll_sorted_df['nll'], color='steelblue', alpha=0.7)\n",
    "ax3.set_yticks(range(num_nodes))\n",
    "ax3.set_yticklabels([name[:15] for name in nll_sorted_df['name']], fontsize=8)\n",
    "ax3.set_xlabel('Negative Log-Likelihood')\n",
    "ax3.set_title('NLL by Node (sorted)')\n",
    "ax3.axvline(x=node_metrics_df['nll'].mean(), color='red', linestyle='--', label=f'Mean: {node_metrics_df[\"nll\"].mean():.2f}')\n",
    "ax3.legend()\n",
    "\n",
    "# Bias by node\n",
    "ax4 = axes[1, 1]\n",
    "bias_sorted_df = node_metrics_df.sort_values('bias')\n",
    "colors = ['red' if b < 0 else 'green' for b in bias_sorted_df['bias']]\n",
    "ax4.barh(range(num_nodes), bias_sorted_df['bias'], color=colors, alpha=0.7)\n",
    "ax4.set_yticks(range(num_nodes))\n",
    "ax4.set_yticklabels([name[:15] for name in bias_sorted_df['name']], fontsize=8)\n",
    "ax4.set_xlabel('Bias (Prediction - Target)')\n",
    "ax4.set_title('Prediction Bias by Node')\n",
    "ax4.axvline(x=0, color='black', linewidth=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze relationship between node characteristics and performance\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# MAE vs mean traffic volume\n",
    "ax1 = axes[0]\n",
    "ax1.scatter(node_metrics_df['mean_target'], node_metrics_df['mae'],\n",
    "            s=100, alpha=0.7, c='steelblue')\n",
    "ax1.set_xlabel('Mean Traffic Volume')\n",
    "ax1.set_ylabel('MAE')\n",
    "ax1.set_title('MAE vs Mean Traffic Volume')\n",
    "\n",
    "# Add regression line\n",
    "valid_mask = ~(node_metrics_df['mae'].isna() | node_metrics_df['mean_target'].isna())\n",
    "if valid_mask.sum() > 2:\n",
    "    z = np.polyfit(node_metrics_df.loc[valid_mask, 'mean_target'],\n",
    "                   node_metrics_df.loc[valid_mask, 'mae'], 1)\n",
    "    p = np.poly1d(z)\n",
    "    x_range = np.linspace(node_metrics_df['mean_target'].min(),\n",
    "                          node_metrics_df['mean_target'].max(), 100)\n",
    "    ax1.plot(x_range, p(x_range), 'r--', linewidth=2, label='Linear fit')\n",
    "    ax1.legend()\n",
    "\n",
    "# MAE vs traffic variability\n",
    "ax2 = axes[1]\n",
    "ax2.scatter(node_metrics_df['std_target'], node_metrics_df['mae'],\n",
    "            s=100, alpha=0.7, c='orange')\n",
    "ax2.set_xlabel('Traffic Std Dev')\n",
    "ax2.set_ylabel('MAE')\n",
    "ax2.set_title('MAE vs Traffic Variability')\n",
    "\n",
    "if valid_mask.sum() > 2:\n",
    "    valid_mask2 = ~(node_metrics_df['mae'].isna() | node_metrics_df['std_target'].isna())\n",
    "    z = np.polyfit(node_metrics_df.loc[valid_mask2, 'std_target'],\n",
    "                   node_metrics_df.loc[valid_mask2, 'mae'], 1)\n",
    "    p = np.poly1d(z)\n",
    "    x_range = np.linspace(node_metrics_df['std_target'].min(),\n",
    "                          node_metrics_df['std_target'].max(), 100)\n",
    "    ax2.plot(x_range, p(x_range), 'r--', linewidth=2, label='Linear fit')\n",
    "    ax2.legend()\n",
    "\n",
    "# Normalized MAE (MAE / mean)\n",
    "ax3 = axes[2]\n",
    "node_metrics_df['normalized_mae'] = node_metrics_df['mae'] / (node_metrics_df['mean_target'] + 1e-6)\n",
    "normalized_sorted = node_metrics_df.sort_values('normalized_mae')\n",
    "ax3.barh(range(num_nodes), normalized_sorted['normalized_mae'], color='steelblue', alpha=0.7)\n",
    "ax3.set_yticks(range(num_nodes))\n",
    "ax3.set_yticklabels([name[:15] for name in normalized_sorted['name']], fontsize=8)\n",
    "ax3.set_xlabel('Normalized MAE (MAE / Mean)')\n",
    "ax3.set_title('Normalized MAE by Node')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "## 6. Generate Residual Diagnostics Plots\n",
    "\n",
    "Plot residual distributions, Q-Q plots, and autocorrelation of residuals to diagnose model fit issues. Check for heteroscedasticity and temporal patterns in errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute residuals\n",
    "def compute_residuals(targets, preds):\n",
    "    \"\"\"Compute residuals and related statistics.\"\"\"\n",
    "    targets_flat = targets.numpy().flatten()\n",
    "    preds_flat = preds.numpy().flatten()\n",
    "\n",
    "    valid_mask = ~np.isnan(targets_flat)\n",
    "\n",
    "    residuals = preds_flat[valid_mask] - targets_flat[valid_mask]\n",
    "    targets_valid = targets_flat[valid_mask]\n",
    "    preds_valid = preds_flat[valid_mask]\n",
    "\n",
    "    # Standardized residuals\n",
    "    std_residuals = (residuals - residuals.mean()) / (residuals.std() + 1e-8)\n",
    "\n",
    "    return {\n",
    "        'residuals': residuals,\n",
    "        'std_residuals': std_residuals,\n",
    "        'targets': targets_valid,\n",
    "        'predictions': preds_valid\n",
    "    }\n",
    "\n",
    "residual_data = compute_residuals(test_results['targets'], test_results['preds'])\n",
    "\n",
    "print(f\"Residual Statistics:\")\n",
    "print(f\"  Mean: {residual_data['residuals'].mean():.4f}\")\n",
    "print(f\"  Std: {residual_data['residuals'].std():.4f}\")\n",
    "print(f\"  Min: {residual_data['residuals'].min():.4f}\")\n",
    "print(f\"  Max: {residual_data['residuals'].max():.4f}\")\n",
    "print(f\"  Skewness: {sp_stats.skew(residual_data['residuals']):.4f}\")\n",
    "print(f\"  Kurtosis: {sp_stats.kurtosis(residual_data['residuals']):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual distribution and Q-Q plot\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Residual histogram\n",
    "ax1 = axes[0, 0]\n",
    "ax1.hist(residual_data['residuals'], bins=50, density=True, alpha=0.7,\n",
    "         color='steelblue', edgecolor='black')\n",
    "# Overlay normal distribution\n",
    "x = np.linspace(residual_data['residuals'].min(), residual_data['residuals'].max(), 100)\n",
    "ax1.plot(x, sp_stats.norm.pdf(x, residual_data['residuals'].mean(),\n",
    "                               residual_data['residuals'].std()),\n",
    "         'r-', linewidth=2, label='Normal fit')\n",
    "ax1.set_xlabel('Residual')\n",
    "ax1.set_ylabel('Density')\n",
    "ax1.set_title('Residual Distribution')\n",
    "ax1.legend()\n",
    "\n",
    "# Q-Q plot\n",
    "ax2 = axes[0, 1]\n",
    "sp_stats.probplot(residual_data['std_residuals'], dist=\"norm\", plot=ax2)\n",
    "ax2.set_title('Q-Q Plot (Standardized Residuals)')\n",
    "\n",
    "# Residuals vs predicted values (heteroscedasticity check)\n",
    "ax3 = axes[0, 2]\n",
    "ax3.scatter(residual_data['predictions'], residual_data['residuals'],\n",
    "            alpha=0.1, s=5, c='steelblue')\n",
    "ax3.axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "ax3.set_xlabel('Predicted Value')\n",
    "ax3.set_ylabel('Residual')\n",
    "ax3.set_title('Residuals vs Predictions')\n",
    "\n",
    "# Add LOWESS smoothing line\n",
    "try:\n",
    "    from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "    # Subsample for performance\n",
    "    n_subsample = min(1000, len(residual_data['predictions']))\n",
    "    idx = np.random.choice(len(residual_data['predictions']), n_subsample, replace=False)\n",
    "    lowess_result = lowess(residual_data['residuals'][idx],\n",
    "                           residual_data['predictions'][idx], frac=0.3)\n",
    "    ax3.plot(lowess_result[:, 0], lowess_result[:, 1], 'orange', linewidth=2, label='LOWESS')\n",
    "    ax3.legend()\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "# Residuals vs actual values\n",
    "ax4 = axes[1, 0]\n",
    "ax4.scatter(residual_data['targets'], residual_data['residuals'],\n",
    "            alpha=0.1, s=5, c='steelblue')\n",
    "ax4.axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "ax4.set_xlabel('Actual Value')\n",
    "ax4.set_ylabel('Residual')\n",
    "ax4.set_title('Residuals vs Actual Values')\n",
    "\n",
    "# Scale-location plot (sqrt of abs standardized residuals)\n",
    "ax5 = axes[1, 1]\n",
    "sqrt_std_residuals = np.sqrt(np.abs(residual_data['std_residuals']))\n",
    "ax5.scatter(residual_data['predictions'], sqrt_std_residuals,\n",
    "            alpha=0.1, s=5, c='steelblue')\n",
    "ax5.set_xlabel('Predicted Value')\n",
    "ax5.set_ylabel('√|Standardized Residual|')\n",
    "ax5.set_title('Scale-Location Plot')\n",
    "\n",
    "try:\n",
    "    n_subsample = min(1000, len(residual_data['predictions']))\n",
    "    idx = np.random.choice(len(residual_data['predictions']), n_subsample, replace=False)\n",
    "    lowess_result = lowess(sqrt_std_residuals[idx],\n",
    "                           residual_data['predictions'][idx], frac=0.3)\n",
    "    ax5.plot(lowess_result[:, 0], lowess_result[:, 1], 'red', linewidth=2)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Autocorrelation of residuals\n",
    "ax6 = axes[1, 2]\n",
    "max_lags = min(50, len(residual_data['residuals']) // 10)\n",
    "autocorr = [1.0]  # lag 0\n",
    "for lag in range(1, max_lags + 1):\n",
    "    autocorr.append(np.corrcoef(residual_data['residuals'][:-lag],\n",
    "                                 residual_data['residuals'][lag:])[0, 1])\n",
    "\n",
    "ax6.bar(range(max_lags + 1), autocorr, color='steelblue', alpha=0.7)\n",
    "# Add significance bounds\n",
    "n = len(residual_data['residuals'])\n",
    "significance_bound = 1.96 / np.sqrt(n)\n",
    "ax6.axhline(y=significance_bound, color='red', linestyle='--', alpha=0.7)\n",
    "ax6.axhline(y=-significance_bound, color='red', linestyle='--', alpha=0.7)\n",
    "ax6.set_xlabel('Lag')\n",
    "ax6.set_ylabel('Autocorrelation')\n",
    "ax6.set_title('Residual Autocorrelation')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-node residual analysis\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Select a few representative nodes for detailed analysis\n",
    "# Best, worst, and median performing nodes\n",
    "mae_values = node_metrics_df['mae'].dropna()\n",
    "if len(mae_values) > 0:\n",
    "    best_node = node_metrics_df.loc[node_metrics_df['mae'].idxmin(), 'node']\n",
    "    worst_node = node_metrics_df.loc[node_metrics_df['mae'].idxmax(), 'node']\n",
    "    median_idx = mae_values.argsort().iloc[len(mae_values)//2]\n",
    "    median_node = node_metrics_df.loc[median_idx, 'node'] if pd.notna(median_idx) else 0\n",
    "else:\n",
    "    best_node, worst_node, median_node = 0, 1, 2\n",
    "\n",
    "selected_nodes = [best_node, median_node, worst_node]\n",
    "node_labels = ['Best', 'Median', 'Worst']\n",
    "\n",
    "for i, (node, label) in enumerate(zip(selected_nodes, node_labels)):\n",
    "    node_targets = test_results['targets'][:, 0, node, -1].numpy()\n",
    "    node_preds = test_results['preds'][:, 0, node, -1].numpy()\n",
    "\n",
    "    valid_mask = ~np.isnan(node_targets)\n",
    "    residuals = node_preds[valid_mask] - node_targets[valid_mask]\n",
    "\n",
    "    # Residual histogram\n",
    "    ax1 = axes[0, i]\n",
    "    ax1.hist(residuals, bins=30, density=True, alpha=0.7,\n",
    "             color='steelblue', edgecolor='black')\n",
    "    ax1.axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "    ax1.set_xlabel('Residual')\n",
    "    ax1.set_ylabel('Density')\n",
    "    node_name = id_to_name_map.get(node, f'Node_{node}')[:20]\n",
    "    ax1.set_title(f'{label} Node: {node_name}\\nMean={residuals.mean():.2f}, Std={residuals.std():.2f}')\n",
    "\n",
    "    # Time series of residuals\n",
    "    ax2 = axes[1, i]\n",
    "    ax2.plot(residuals[:200], alpha=0.7, color='steelblue')\n",
    "    ax2.axhline(y=0, color='red', linestyle='--', linewidth=1)\n",
    "    ax2.fill_between(range(len(residuals[:200])),\n",
    "                     -residuals.std()*2, residuals.std()*2,\n",
    "                     alpha=0.2, color='red', label='±2σ')\n",
    "    ax2.set_xlabel('Sample Index')\n",
    "    ax2.set_ylabel('Residual')\n",
    "    ax2.set_title(f'Residual Time Series (first 200 samples)')\n",
    "    ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
