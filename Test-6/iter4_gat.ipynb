{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Iter 4 - GATZINBNLL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "For this iteration, we will adapt the Graph Attention Network (GAT) to handle dynamic graph structures by passing the graph data (edge_index and edge_attr) directly to the forward method. This allows us to work with varying graph structures without storing them as part of the model state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SCALER_MU = 14.323774337768555\n",
    "SCALER_SIGMA = 34.9963493347168"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = torch.load(\"data/training/processed/50000065e63b1f0d/edge_index.pt\", weights_only=False)\n",
    "edge_attr_data = torch.load(\"data/training/processed/50000065e63b1f0d/edge_attr.pt\", weights_only=False)\n",
    "static_features = torch.load(\"data/training/processed/50000065e63b1f0d/static_features.pt\", weights_only=False)\n",
    "sensor_mask = torch.load(\"data/training/processed/50000065e63b1f0d/sensor_mask.pt\", weights_only=False)\n",
    "train_loader = torch.load(\"data/training/processed/50000065e63b1f0d/train_loader.pt\", weights_only=False)\n",
    "val_loader = torch.load(\"data/training/processed/50000065e63b1f0d/val_loader.pt\", weights_only=False)\n",
    "test_loader = torch.load(\"data/training/processed/50000065e63b1f0d/test_loader.pt\", weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"n batches:\", len(train_loader))\n",
    "for X, y in train_loader:\n",
    "    print(X.shape, y.shape)\n",
    "    print(\"Shape of tensor X: [batch_size, seq_len, num_nodes, num_features]\")\n",
    "    sample_window_a = X[0, :, 0, :]\n",
    "    sample_window_b = X[1, :, 0, :]\n",
    "    sample_window_c = X[2, :, 0, :]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_names = [\"value__mean_L12\",\n",
    "      \"value__std_L12\",\n",
    "      \"value__min_L12\",\n",
    "      \"value__max_L12\",\n",
    "      \"value__q25_L12\",\n",
    "      \"value__q75_L12\",\n",
    "      \"value__slope_L12\",\n",
    "      \"value__energy_L12\",\n",
    "      \"value__valid_frac_L12\",\n",
    "      \"value\"]\n",
    "fig, ax = plt.subplots(5,2,figsize=(12,16))\n",
    "for i, feature in enumerate(features_names):\n",
    "      ax[i//2, i%2].plot(sample_window_a[:, i], label=features_names[i])\n",
    "      ax[i//2, i%2].plot(sample_window_b[:, i], linestyle='--', label=f\"{features_names[i]} (Window B)\")\n",
    "      ax[i//2, i%2].plot(sample_window_c[:, i], linestyle=':', label=f\"{features_names[i]} (Window C)\")\n",
    "      ax[i//2, i%2].set_title(features_names[i])\n",
    "      ax[i//2, i%2].legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_hybrid_loader(loader, batch_size):\n",
    "    # Original X shape: [Batch, 12, 29, 10]\n",
    "    # Single iteration - collect all data first\n",
    "    all_batches = [(X, y) for X, y in loader]\n",
    "\n",
    "    # Temporal component (for LSTM)\n",
    "    # 1. Take all timesteps (12), all nodes, last feature (value)\n",
    "    # Shape: [Total_Samples, 12, 29, 1]\n",
    "    X_temporal_list = [(X[:, :, :, -1:]) for X, _ in all_batches]\n",
    "    X_temporal = torch.cat(X_temporal_list, dim=0)\n",
    "\n",
    "    # Spatial/Static Component for (GAT)\n",
    "    # Take first timestep (0:1), all nodes, features 0:-1 (aggregated stats) and the flattened value feature\n",
    "    # Shape: [Total_Samples, 1, 29, 21]\n",
    "\n",
    "\n",
    "    X_agg_list = [(X[:, 0:1, :, :-1]) for X, _ in all_batches]\n",
    "    X_raw_list = [(X[:, :, :, -1:].permute(0,3,2,1)) for X, _ in all_batches]\n",
    "    X_agg = torch.cat(X_agg_list, dim=0)\n",
    "    X_raw = torch.cat(X_raw_list, dim=0)\n",
    "    X_spatial = torch.cat([X_agg, X_raw], dim=3)\n",
    "\n",
    "    # 3. Targets\n",
    "    # Shape: [Total_Samples, 1, 29, 1]\n",
    "    y_list = [y[:, 0:1, :, :] for _, y in all_batches]\n",
    "    y_target = torch.cat(y_list, dim=0)\n",
    "\n",
    "    print(\"Temporal X shape:\", X_temporal.shape)\n",
    "    print(\"Spatial X shape:\", X_spatial.shape)\n",
    "    print(\"Target y shape:\", y_target.shape)\n",
    "\n",
    "    # Create a dataset that yields a tuple of inputs\n",
    "    dataset = torch.utils.data.TensorDataset(X_spatial, X_temporal, y_target)\n",
    "\n",
    "    # Shuffle should be True for training, False for validation/test\n",
    "    hybrid_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "    print(\"Number of batches in hybrid loader:\", len(hybrid_loader))\n",
    "\n",
    "    return hybrid_loader\n",
    "\n",
    "train_loader = prepare_hybrid_loader(train_loader, batch_size=16)\n",
    "val_loader = prepare_hybrid_loader(val_loader, batch_size=16)\n",
    "test_loader = prepare_hybrid_loader(test_loader, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def _finite_stats(name, t: torch.Tensor):\n",
    "    if t is None:\n",
    "        print(f\"[DEBUG] {name}: None\")\n",
    "        return False\n",
    "    is_finite = torch.isfinite(t)\n",
    "    if not is_finite.all():\n",
    "        n_nan = torch.isnan(t).sum().item()\n",
    "        n_inf = torch.isinf(t).sum().item()\n",
    "        print(f\"[NON-FINITE] {name}: nan={n_nan}, inf={n_inf}, shape={tuple(t.shape)}\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def _check(name, t:torch.Tensor):\n",
    "    ok = _finite_stats(name, t)\n",
    "    # if not ok:\n",
    "    #     # Early stop by rasing to surface the exact point\n",
    "    #     raise RuntimeError(f\"Non-finite tensor detected in {name}\")\n",
    "\n",
    "class DynamicNodeGATZINB(nn.Module):\n",
    "    def __init__(self, dynamic_node_dim, static_node_dim, edge_dim, n_embd, n_heads, dropout_rate):\n",
    "        super().__init__()\n",
    "\n",
    "        dynamic_input_dim = dynamic_node_dim * 2  # Original feature + missing mask\n",
    "        static_input_dim = static_node_dim\n",
    "        gat1_input_channels = dynamic_input_dim + static_input_dim\n",
    "\n",
    "        # Layer 1: Input features to embedding dimensions (our \"node embedder\")\n",
    "        # Use concat=False so output dims remain n_embd when using multi-head attention\n",
    "        self.gat1 = GATv2Conv(in_channels=gat1_input_channels, out_channels=n_embd, edge_dim=edge_dim, heads=n_heads, concat=False, dropout=dropout_rate)\n",
    "        self.gat2 = GATv2Conv(in_channels=n_embd, out_channels=n_embd, edge_dim=edge_dim, heads=n_heads, concat=False, dropout=dropout_rate)\n",
    "        self.norm = nn.LayerNorm(n_embd)\n",
    "\n",
    "        # ZINB has 3 parameter per output:\n",
    "        # 1. mu (mean of NB)\n",
    "        # 2. theta (dispersion of NB)\n",
    "        # 3. pi (probability of zero-inflation)\n",
    "        self.mu_head = nn.Linear(n_embd, 1) # Mean (positive)\n",
    "        self.theta_head = nn.Linear(n_embd, 1) # Dispersion (positive)\n",
    "        self.pi_head = nn.Linear(n_embd, 1) # Zero-inflation probability (0-1)\n",
    "\n",
    "        # Output layer takes the input of the 'embedder layer'\n",
    "        # self.output_layer = nn.Linear(n_embd, output_dim)\n",
    "        self.elu = nn.ELU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, X_batch, targets, node_mask, edge_index, edge_attr, static_node_features):\n",
    "\n",
    "        # X_batch: (Batch, 1, Num_Nodes, Input_Batch_Feature_Dim)\n",
    "        # targets: (Batch, 1, Num_Nodes, 1) - for training\n",
    "        # node_mask: (Num_Nodes) - boolean, True for nodes to include (e.g., sensor nodes)\n",
    "\n",
    "        mu_list, theta_list, pi_list = [], [], []\n",
    "\n",
    "        missing_mask = torch.isnan(X_batch)\n",
    "        imputed_X = torch.nan_to_num(X_batch, nan=0.0)\n",
    "        _check(\"imputed_X\", imputed_X)\n",
    "        mask_features = missing_mask.float()\n",
    "        combined_input = torch.cat([imputed_X, mask_features], dim=-1)\n",
    "        _check(\"combined_input\", combined_input)\n",
    "\n",
    "        B = combined_input.shape[0]\n",
    "        Xb = combined_input[:,0,:,:]\n",
    "\n",
    "        # Normalize edge_attr ONCE before the loop (use a local variable!)\n",
    "        edge_attr_normed = self.edge_norm(edge_attr)\n",
    "\n",
    "        for b in range(B):\n",
    "            combined_features = torch.cat([Xb[b], static_node_features], dim=-1)\n",
    "            xb = self.dropout(xb)  # Fixed: was using combined_features instead of xb\n",
    "            _check(\"dropout_out\", xb)\n",
    "            print(\"DEBUG - xb stats before GAT1:\", torch.min(xb).item(), torch.max(xb).item())\n",
    "            print(\"DEBUG - edge_attr stats before GAT1:\", torch.min(edge_attr_normed).item(), torch.max(edge_attr_normed).item())\n",
    "            xb = self.gat1(xb, edge_index, edge_attr_normed)\n",
    "            _check(\"gat1_out\", xb)\n",
    "            print(\"DEBUG - xb stats after GAT1:\", torch.min(xb).item(), torch.max(xb).item())\n",
    "            print(\"DEBUG - edge_attr stats after GAT1:\", torch.min(edge_attr_normed).item(), torch.max(edge_attr_normed).item())\n",
    "            xb = self.norm(xb)\n",
    "            xb = self.elu(xb)\n",
    "            xb = self.dropout(xb)\n",
    "            xb = self.gat2(xb, edge_index, edge_attr_normed)\n",
    "            _check(\"gat2_out\", xb)\n",
    "            xb = self.norm(xb)\n",
    "            xb = self.elu(xb)\n",
    "            xb = self.dropout(xb)\n",
    "\n",
    "            # Predict ZINB parameters\n",
    "            mu_b = torch.nn.functional.softplus(self.mu_head(xb)) + 1e-6 # Ensure positivity\n",
    "            theta_b = torch.nn.functional.softplus(self.theta_head(xb)) + 1e-6 # Ensure positivity\n",
    "            pi_b = torch.sigmoid(self.pi_head(xb)) # Probability between 0 and 1\n",
    "            pi_b = torch.clamp(pi_b, min=1e-6, max=1-1e-6) # Avoid exact 0 or 1\n",
    "\n",
    "            _check(\"mu_b\", mu_b)\n",
    "            _check(\"theta_b\", theta_b)\n",
    "            _check(\"pi_b\", pi_b)\n",
    "\n",
    "            mu_list.append(mu_b)\n",
    "            theta_list.append(theta_b)\n",
    "            pi_list.append(pi_b)\n",
    "\n",
    "            # total_zinb_nll_loss += zinb_nll_loss_b\n",
    "            # total_valid += int(valid_b.item())\n",
    "\n",
    "        # Unsqueeze at the 1 position to match expected output shape [B, 1, N, 1]\n",
    "        mu = torch.stack(mu_list, dim=0).unsqueeze(1)\n",
    "        theta = torch.stack(theta_list, dim=0).unsqueeze(1)\n",
    "        pi = torch.stack(pi_list, dim=0).unsqueeze(1)\n",
    "\n",
    "        zinb_nll_loss, valid_sum = self.zinb_nll_loss(mu, theta, pi, targets, node_mask)\n",
    "        # valid_sum = torch.tensor(total_valid, device=mu.device, dtype=torch.float32)\n",
    "\n",
    "        # Return mu as point prediction for evaluation\n",
    "        preds = mu * (1 - pi) # [B, 1, N, 1]\n",
    "        mse_loss, _ = self.mse_loss(preds, targets, node_mask)\n",
    "        return preds, zinb_nll_loss, mse_loss, {'mu': mu, 'theta': theta, 'pi': pi, 'valid_sum': valid_sum}\n",
    "\n",
    "    def zinb_nll_loss(self, mu, theta, pi, targets, node_mask):\n",
    "        \"\"\"\n",
    "        Zero-Inflated Negative Binomial Negative Log-Likelihood Loss\n",
    "\n",
    "        Args:\n",
    "            mu: predicted mean (batch_size, 1)\n",
    "            theta: dispersion parameter (batch_size, 1)\n",
    "            pi: zero-inflation probability (batch_size, 1)\n",
    "            targets: actual counts (batch_size, 1)\n",
    "            node_mask: boolean mask for which nodes to include\n",
    "        \"\"\"\n",
    "        eps = 1e-8\n",
    "\n",
    "        # Creat NaN mask - True for valid (non-NaN) values\n",
    "        nan_mask = ~torch.isnan(targets)\n",
    "\n",
    "        # Combine with existing node_mask if provided\n",
    "        if node_mask is not None:\n",
    "            valid_mask = nan_mask & node_mask\n",
    "        else:\n",
    "            valid_mask = nan_mask\n",
    "\n",
    "        # Check if we have any valid values\n",
    "        if valid_mask.sum() == 0:\n",
    "            return torch.tensor(0.0, device=mu.device, requires_grad=True), torch.tensor(0.0, device=mu.device)\n",
    "\n",
    "        # Index only valid positions\n",
    "        mu_valid = mu[valid_mask]\n",
    "        theta_valid = theta[valid_mask]\n",
    "        pi_valid = pi[valid_mask]\n",
    "        targets_valid = targets[valid_mask]\n",
    "        # Convert only VALID entries to non-negative counts:\n",
    "        targets_valid = torch.round(targets_valid).clamp_min(0).to(mu_valid.dtype)\n",
    "\n",
    "\n",
    "        _check(\"mu_valid\", mu_valid)\n",
    "        _check(\"theta_valid\", theta_valid)\n",
    "        _check(\"pi_valid\", pi_valid)\n",
    "        _check(\"targets_valid\", targets_valid)\n",
    "\n",
    "        # NB log probability\n",
    "        # log p(y(NB)) = log Gamma(theta+y) - log Gamma(theta) - log Gamma(y+1)\n",
    "        #               + theta*log(theta) - theta*log(theta + mu)\n",
    "        #               + y*log(mu) - y*log(theta + mu)\n",
    "\n",
    "        theta_mu = theta_valid + mu_valid\n",
    "\n",
    "        _check(\"theta_mu\", theta_mu)\n",
    "\n",
    "        # Using lgamma for numerical stability\n",
    "        nb_log_prob = (\n",
    "            torch.lgamma(theta_valid + targets_valid + eps)\n",
    "            - torch.lgamma(theta_valid + eps)\n",
    "            - torch.lgamma(targets_valid + 1)\n",
    "            + theta_valid * torch.log(theta_valid + eps)\n",
    "            - theta_valid * torch.log(theta_mu + eps)\n",
    "            + targets_valid * torch.log(mu_valid + eps)\n",
    "            - targets_valid * torch.log(theta_mu + eps)\n",
    "        )\n",
    "\n",
    "        # ZINB combine zero-inflation with NB\n",
    "        # p(y) = pi*I(y=0) + (1-pi)*NB(y)\n",
    "        # For y=0: log p(0) = log(pi + (1-pi)*NB(0))\n",
    "        # For y>0: log p(y) = log(1-pi) + log NB(y)\n",
    "\n",
    "        zero_mask = (targets_valid < eps).float()\n",
    "\n",
    "        # For zero counts\n",
    "        nb_zero_prob = theta_valid * torch.log(theta_valid / (theta_mu + eps))\n",
    "        zero_log_prob = torch.log(pi_valid + (1 - pi_valid) * torch.exp(nb_zero_prob) + eps)\n",
    "\n",
    "        # For non-zer counts\n",
    "        non_zero_log_prob = torch.log(1 - pi_valid + eps) + nb_log_prob\n",
    "\n",
    "        # Combine\n",
    "        log_prob = zero_mask * zero_log_prob + (1 - zero_mask) * non_zero_log_prob\n",
    "\n",
    "        # Mean over valid samples only\n",
    "        nll = -log_prob.mean()\n",
    "\n",
    "        return nll, valid_mask.sum()\n",
    "\n",
    "    def mse_loss(self, predictions, targets, node_mask):\n",
    "        # Create NaN mask - True for valid (non-NaN) values\n",
    "        nan_mask = ~torch.isnan(targets)\n",
    "\n",
    "        # Combine with existing node_mask if provided\n",
    "        if node_mask is not None:\n",
    "            valid_mask = nan_mask & node_mask\n",
    "        else:\n",
    "            valid_mask = nan_mask\n",
    "\n",
    "        # Check if we have any valid values\n",
    "        if valid_mask.sum() == 0:\n",
    "            return torch.tensor(0.0, device=predictions.device, requires_grad=True), 0\n",
    "\n",
    "        preds_valid = predictions[valid_mask]\n",
    "        targets_valid = targets[valid_mask]\n",
    "\n",
    "        mse_loss = ((targets_valid - preds_valid)**2).mean()\n",
    "\n",
    "        return mse_loss, valid_mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 32  # Embedding dimension for the MLP\n",
    "n_heads = 4\n",
    "output_dim = 1 # Predicting a single traffic value\n",
    "dropout = 0.1\n",
    "lr = 1e-4\n",
    "epochs = 40\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_static_input = static_features.to(device)\n",
    "edge_index_input = edge_index.to(device)\n",
    "edge_attr_input = edge_attr_data.to(device)\n",
    "\n",
    "sensor_mask_input = sensor_mask.to(device)\n",
    "\n",
    "num_nodes_input = static_features.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaNs/Infs in your graph data\n",
    "print(\"=== Data Validation ===\")\n",
    "print(f\"Edge Index - NaN: {torch.isnan(edge_index_input).any()}, Inf: {torch.isinf(edge_index_input).any()}\")\n",
    "print(f\"Edge Attr - NaN: {torch.isnan(edge_attr_input).any()}, Inf: {torch.isinf(edge_attr_input).any()}\")\n",
    "print(f\"Static Features - NaN: {torch.isnan(X_static_input).any()}, Inf: {torch.isinf(X_static_input).any()}\")\n",
    "\n",
    "# Check for extreme values\n",
    "print(f\"\\nEdge Attr - Min: {edge_attr_input.min():.4f}, Max: {edge_attr_input.max():.4f}, Mean: {edge_attr_input.mean():.4f}\")\n",
    "print(f\"Static Features - Min: {X_static_input.min():.4f}, Max: {X_static_input.max():.4f}, Mean: {X_static_input.mean():.4f}\")\n",
    "\n",
    "# Check first batch\n",
    "X_spatial, _, y_batch = next(iter(train_loader))\n",
    "print(f\"\\nFirst Batch - NaN: {torch.isnan(X_spatial).sum()}, Inf: {torch.isinf(X_spatial).any()}\")\n",
    "print(f\"First Batch - Min: {X_spatial[~torch.isnan(X_spatial)].min():.4f}, Max: {X_spatial[~torch.isnan(X_spatial)].max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check combined feature scales that will be input to GAT\n",
    "X_spatial, _, y_batch = next(iter(train_loader))\n",
    "\n",
    "# Simulate what happens in the forward pass\n",
    "missing_mask = torch.isnan(X_spatial)\n",
    "imputed_X = torch.nan_to_num(X_spatial, nan=0.0).to(device)\n",
    "mask_features = missing_mask.float().to(device)\n",
    "combined_dynamic = torch.cat([imputed_X, mask_features], dim=-1)\n",
    "\n",
    "# Take first sample, first timestep\n",
    "sample_dynamic = combined_dynamic[0, 0, :, :]  # [num_nodes, 42] (21*2)\n",
    "sample_combined = torch.cat([sample_dynamic, X_static_input], dim=-1)  # [num_nodes, 55]\n",
    "\n",
    "print(\"\\n=== Feature Scale Verification (Input to GAT) ===\")\n",
    "print(f\"Dynamic features (after imputation + mask): Min={sample_dynamic.min():.4f}, Max={sample_dynamic.max():.4f}, Std={sample_dynamic.std():.4f}\")\n",
    "print(f\"Static features (normalized): Min={X_static_input.min():.4f}, Max={X_static_input.max():.4f}, Std={X_static_input.std():.4f}\")\n",
    "print(f\"Combined features: Min={sample_combined.min():.4f}, Max={sample_combined.max():.4f}, Std={sample_combined.std():.4f}\")\n",
    "print(\"\\nAll feature ranges should be similar (roughly -3 to +3 for normalized data)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_nodes_dim = static_features.shape[1]\n",
    "edge_attr_dim = edge_attr_data.shape[1]\n",
    "print(\"Static Node Features Dim:\", static_nodes_dim)\n",
    "print(\"Edge Attribute Dim:\", edge_attr_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training expectations:\")\n",
    "print(\"  Dynamic features: 21\")\n",
    "print(\"  Static features: Should be 13\")\n",
    "print(f\"\\nValidation data:\")\n",
    "X_spatial, X_temporal, y_target = next(iter(val_loader))\n",
    "print(f\"  Dynamics features shape: {X_spatial.shape[3]}\")\n",
    "print(f\"  Static features shape: {static_features.shape[1]}\")\n",
    "print(f\"  Expected input to GAT1: {21*2 + static_features.shape[1]}\")\n",
    "print(f\"  Model GAT1 expects: 55\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DynamicNodeGATZINB(\n",
    "    dynamic_node_dim=21,\n",
    "    static_node_dim=static_nodes_dim,\n",
    "    edge_dim=edge_attr_dim,\n",
    "    n_embd=n_embd,\n",
    "    n_heads=n_heads,\n",
    "    dropout_rate=dropout,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "print(f\"Model Iteration 1 Parameters: {sum(p.numel() for p in model.parameters())/1e3:.1f} K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights more conservatively for numerical stability\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight, gain=0.5)  # Smaller gain for stability\n",
    "        if m.bias is not None:\n",
    "            torch.nn.init.zeros_(m.bias)\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "print(f\"Model weights initialized with conservative Xavier initialization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot(steps, train_zinb_nll, val_zinb_nll, train_mse, val_mse, mu, theta, pi):\n",
    "    # Append scalar values, not lists\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(8,16))\n",
    "\n",
    "    ax1.clear()\n",
    "    ax2.clear()\n",
    "    ax3.clear()\n",
    "\n",
    "    ax1.plot(steps, train_zinb_nll, label='Train ZINB NLL', marker='o')\n",
    "    ax1.plot(steps, val_zinb_nll, label='Validation ZINB NLL', marker='o')\n",
    "    ax1.legend()\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('ZINB Training and Validation Loss Over Time')\n",
    "    ax1.grid(True)\n",
    "\n",
    "    ax2.plot(steps, train_mse, label='Train MSE', marker='x')\n",
    "    ax2.plot(steps, val_mse, label='Validation MSE', marker='x')\n",
    "    ax2.legend()\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.set_title('MSE Training and Validation Loss Over Time')\n",
    "    ax2.grid(True)\n",
    "\n",
    "    ax3.plot(steps, mu, label='Mu', color='green', marker='o')\n",
    "    ax3.plot(steps, theta, label='Theta', color='red', marker='o')\n",
    "    ax3.plot(steps, pi, label='Pi', color='purple', marker='o')\n",
    "    ax3.legend()\n",
    "    ax3.set_xlabel('Epoch')\n",
    "    ax3.set_ylabel('Parameter Value')\n",
    "    ax3.set_title('Model Parameters Over Time')\n",
    "    ax3.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "def format_params(params):\n",
    "    params_cat = {\"mu\":[], \"theta\":[], \"pi\": []}\n",
    "    for batch in params:\n",
    "        for key in params_cat.keys():\n",
    "            value = batch[key]\n",
    "            params_cat[key].append(value)\n",
    "\n",
    "    for key in params_cat.keys():\n",
    "        params_cat[key] = torch.cat(params_cat[key], dim=0)\n",
    "\n",
    "    # Return as tuple for easy unpacking\n",
    "    return params_cat[\"mu\"], params_cat[\"theta\"], params_cat[\"pi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training Loop Iteration 1 ---\n",
    "import time\n",
    "start_time = time.time()\n",
    "print(f\"Training started at {start_time}\")\n",
    "model.train()\n",
    "# For plotting\n",
    "avg_train_zinb_nll = []\n",
    "avg_train_mse = []\n",
    "avg_val_zinb_nll = []\n",
    "avg_val_mse = []\n",
    "avg_mu = []\n",
    "avg_theta = []\n",
    "avg_pi = []\n",
    "for epoch in range(epochs):\n",
    "    epoch_zinb_nll = 0\n",
    "    epoch_mse = 0\n",
    "    epoch_valid_samples = 0\n",
    "    epoch_total_samples = 0\n",
    "    num_batches = 0\n",
    "    val_epoch_zinb_nll = 0\n",
    "    val_epoch_mse = 0\n",
    "    val_epoch_valid_samples = 0\n",
    "    val_epoch_total_samples = 0\n",
    "    val_num_batches = 0\n",
    "    epoch_params = []\n",
    "    for X_spatial, _, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        X_batch = X_spatial.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        # Track batch statistics\n",
    "        batch_size = y_batch.shape[0]\n",
    "        batch_total_samples = y_batch.numel() # Total elements in batch\n",
    "\n",
    "        # 4. Unnormalize the target variable\n",
    "        # y_raw = (y_batch_normalized * sigma) + mu\n",
    "        y_raw = (y_batch * SCALER_SIGMA) + SCALER_MU # Using your notebook's variable name\n",
    "\n",
    "        predictions, zinb_nll_loss, mse_loss, params = model(X_batch=X_batch, targets=y_raw, node_mask=None, edge_index=edge_index_input, edge_attr=edge_attr_input, static_node_features=X_static_input)\n",
    "\n",
    "        zinb_nll_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Add this line\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate metrics\n",
    "        epoch_params.append(params)\n",
    "        epoch_zinb_nll += zinb_nll_loss.item()\n",
    "        epoch_mse += mse_loss.item()\n",
    "        epoch_valid_samples += params['valid_sum'].item()\n",
    "        epoch_total_samples += batch_total_samples\n",
    "        num_batches += 1\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_spatial, _, y_batch in val_loader:\n",
    "            X_batch = X_spatial.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            # 4. Unnormalize the target variable\n",
    "            # y_raw = (y_batch_normalized * sigma) + mu\n",
    "            y_raw = (y_batch * SCALER_SIGMA) + SCALER_MU # Using your notebook's variable name\n",
    "\n",
    "\n",
    "            predictions, zinb_nll_loss, mse_loss, params = model(X_batch=X_batch, targets=y_raw, node_mask=None, edge_index=edge_index_input, edge_attr=edge_attr_input, static_node_features=X_static_input)\n",
    "\n",
    "            val_epoch_zinb_nll += zinb_nll_loss.item()\n",
    "            val_epoch_mse += mse_loss.item()\n",
    "            val_epoch_valid_samples += params['valid_sum'].item()\n",
    "            val_epoch_total_samples += y_batch.numel()\n",
    "            val_num_batches += 1\n",
    "\n",
    "    avg_zinb_nll = epoch_zinb_nll / num_batches\n",
    "    avg_mse = epoch_mse / num_batches\n",
    "    valid_percentage = (epoch_valid_samples / epoch_total_samples) * 100\n",
    "    avg_valid_per_batch = epoch_valid_samples / num_batches\n",
    "\n",
    "    val_avg_zinb_nll = val_epoch_zinb_nll / val_num_batches\n",
    "    val_avg_mse = val_epoch_mse / val_num_batches\n",
    "    val_valid_percentage = (val_epoch_valid_samples / val_epoch_total_samples) * 100\n",
    "    val_avg_valid_per_batch = val_epoch_valid_samples / val_num_batches\n",
    "\n",
    "    # if (epoch+1) % 10 == 0 or epoch == 0:\n",
    "    if epoch >=0:\n",
    "        print(f\"Epoch {epoch+1:3d} | \"\n",
    "        f\"ZINB NLL: {avg_zinb_nll:.4f} | \"\n",
    "        f\"MSE: {avg_mse:.4f} | \"\n",
    "        f\"Valid: {epoch_valid_samples}/{epoch_total_samples} ({valid_percentage:.1f}%) | \"\n",
    "        f\"Avg/Batch: {avg_valid_per_batch:.1f} | \"\n",
    "        f\"Batches: {num_batches}\")\n",
    "        print(f\"      VAL | \"\n",
    "        f\"ZINB NLL: {val_avg_zinb_nll:.4f} | \"\n",
    "        f\"MSE: {val_avg_mse:.4f} | \"\n",
    "        f\"Valid: {val_epoch_valid_samples}/{val_epoch_total_samples} ({val_valid_percentage:.1f}%) | \"\n",
    "        f\"Avg/Batch: {val_avg_valid_per_batch:.1f} | \"\n",
    "        f\"Batches: {val_num_batches}\")\n",
    "\n",
    "    mu, theta, pi = format_params(epoch_params)\n",
    "\n",
    "    avg_train_zinb_nll.append(avg_zinb_nll)\n",
    "    avg_val_zinb_nll.append(val_avg_zinb_nll)\n",
    "\n",
    "    avg_train_mse.append(avg_mse)\n",
    "    avg_val_mse.append(val_avg_mse)\n",
    "\n",
    "    avg_mu.append(mu.mean().item())\n",
    "    avg_theta.append(theta.mean().item())\n",
    "    avg_pi.append(pi.mean().item())\n",
    "\n",
    "print(f\"Training completed at {time.time()}, duration: {time.time() - start_time:.2f}s\")\n",
    "steps = list(range(1, epochs+1))\n",
    "plot(steps, avg_train_zinb_nll, avg_val_zinb_nll, avg_train_mse, avg_val_mse, avg_mu, avg_theta, avg_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(model.state_dict(), \"data/models/iter4_gat.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training, check gradient magnitudes\n",
    "for name, param in model.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        print(f\"{name}: grad norm = {param.grad.norm().item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced metrics with node-level tracking\n",
    "def detailed_evaluation(model, data_loader, device, split_name=\"Val\"):\n",
    "    model.eval()\n",
    "\n",
    "    # Track per-node validity (assuming shape [batch, 1, num_nodes, 1])\n",
    "    num_nodes = None\n",
    "    node_valid_counts = None\n",
    "    node_total_counts = None\n",
    "\n",
    "    total_zinb_nll_loss = 0.0\n",
    "    total_mse_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    all_preds = []\n",
    "    all_params = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_spatial, _, y_batch in data_loader:\n",
    "            X_batch = X_spatial.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            if num_nodes is None:\n",
    "                num_nodes = y_batch.shape[2]\n",
    "                node_valid_counts = torch.zeros(num_nodes)\n",
    "                node_total_counts = torch.zeros(num_nodes)\n",
    "\n",
    "            # 4. Unnormalize the target variable\n",
    "            # y_raw = (y_batch_normalized * sigma) + mu\n",
    "            y_raw = (y_batch * SCALER_SIGMA) + SCALER_MU # Using your notebook's variable name\n",
    "\n",
    "\n",
    "            preds, zinb_nll_loss, mse_loss, params = model(X_batch=X_batch, targets=y_raw, node_mask=None, edge_index=edge_index_input, edge_attr=edge_attr_input, static_node_features=X_static_input)\n",
    "\n",
    "            all_preds.append(preds.cpu())\n",
    "            all_params.append(params)\n",
    "\n",
    "            if zinb_nll_loss is not None:\n",
    "                total_zinb_nll_loss += zinb_nll_loss.item()\n",
    "                total_mse_loss += mse_loss.item()\n",
    "                num_batches += 1\n",
    "\n",
    "                # Count valid samples per node\n",
    "                nan_mask = ~torch.isnan(y_batch)  # [batch, 1, num_nodes, 1]\n",
    "                node_valid_counts += nan_mask.sum(dim=(0, 1, 3)).cpu()\n",
    "                node_total_counts += torch.ones_like(node_valid_counts) * y_batch.shape[0]\n",
    "\n",
    "    if num_batches > 0:\n",
    "        avg_zinb_nll_loss = total_zinb_nll_loss / num_batches\n",
    "        avg_mse_loss = total_mse_loss / num_batches\n",
    "\n",
    "        print(f\"\\n{split_name} Detailed Metrics:\")\n",
    "        print(f\"  Avg ZINB NLL Loss: {avg_zinb_nll_loss:.4f}\")\n",
    "        print(f\"  Avg MSE Loss: {avg_mse_loss:.4f}\")\n",
    "        print(f\"  Total batches: {num_batches}\")\n",
    "\n",
    "        # Per-node statistics\n",
    "        node_valid_pct = (node_valid_counts / node_total_counts * 100)\n",
    "        print(f\"\\n  Node Validity Statistics:\")\n",
    "        print(f\"    Min: {node_valid_pct.min():.1f}%\")\n",
    "        print(f\"    Max: {node_valid_pct.max():.1f}%\")\n",
    "        print(f\"    Mean: {node_valid_pct.mean():.1f}%\")\n",
    "        print(f\"    Nodes with 100% valid: {(node_valid_pct == 100).sum().item()}/{num_nodes}\")\n",
    "        print(f\"    Nodes with <50% valid: {(node_valid_pct < 50).sum().item()}/{num_nodes}\")\n",
    "\n",
    "        return all_preds, all_params\n",
    "\n",
    "# Run after training\n",
    "train_results = detailed_evaluation(model, train_loader, device, \"Train\")\n",
    "val_results = detailed_evaluation(model, val_loader, device, \"Validation\")\n",
    "test_results = detailed_evaluation(model, test_loader, device, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds, train_params = train_results\n",
    "val_preds, val_params = val_results\n",
    "test_preds, test_params = test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mu, train_theta, train_pi = format_params(train_params)\n",
    "val_mu, val_theta, val_pi = format_params(val_params)\n",
    "test_mu, test_theta, test_pi = format_params(test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.distributions as dist\n",
    "import numpy as np\n",
    "from scipy import stats as sp_stats # Import scipy.stats\n",
    "\n",
    "with open(\"data/training/processed/c56f869b05279744/sensor_name_to_id_map.json\", \"r\") as f:\n",
    "    name_to_id_map = json.load(f)\n",
    "    id_to_name_map = {v: k for k, v in name_to_id_map.items()}\n",
    "\n",
    "# This function is now correct, just a small rename for clarity\n",
    "def format_data_for_plotting(params_list, dataloader):\n",
    "    # 1. Unnormalize Ground Truth Targets\n",
    "    targets = []\n",
    "    for _, _, y in dataloader:\n",
    "        y_raw = (y * SCALER_SIGMA) + SCALER_MU\n",
    "        y_raw_int = torch.round(y_raw).long()\n",
    "        targets.append(y_raw_int)\n",
    "    cat_targets = torch.cat(targets, dim=0)\n",
    "\n",
    "    # 2. Concatenate Model Outputs\n",
    "    cat_mu, cat_theta, cat_pi = format_params(params_list)\n",
    "\n",
    "    return cat_targets, cat_mu, cat_theta, cat_pi\n",
    "\n",
    "# --- RE-RUN YOUR DATA FORMATTING ---\n",
    "# We need pi, so we must re-run this part\n",
    "train_targets, train_mu, train_theta, train_pi = format_data_for_plotting(train_params, train_loader)\n",
    "val_targets, val_mu, val_theta, val_pi = format_data_for_plotting(val_params, val_loader)\n",
    "test_targets, test_mu, test_theta, test_pi = format_data_for_plotting(test_params, test_loader)\n",
    "\n",
    "\n",
    "# --- YOUR NEW PLOTTING FUNCTION ---\n",
    "\n",
    "def plot_preds_and_ground_truth(targets, mu, theta, pi, names, n_samples=500):\n",
    "    \"\"\"\n",
    "    Plots the ground truth against the predicted ZINB distribution.\n",
    "\n",
    "    - 'True': The actual ground truth counts.\n",
    "    - 'Predicted': The ZINB Expected Value, E[y] = (1-pi) * mu\n",
    "    - '95% P.I.': The 95% prediction interval (2.5th to 97.5th percentile)\n",
    "    \"\"\"\n",
    "\n",
    "    # An small value to prevent division by zero or invalid probs\n",
    "    eps = 1e-8\n",
    "\n",
    "    # --- 1. Calculate the Expected Value (The \"Prediction\") ---\n",
    "    # This can stay in torch\n",
    "    expected_value = (1 - pi) * mu\n",
    "\n",
    "    # --- 3. Plot ---\n",
    "    num_nodes = targets.shape[2]\n",
    "    for node in range(num_nodes):\n",
    "        plt.figure(figsize=(10, 4))\n",
    "\n",
    "        # Get data for the current node and move to CPU/NumPy\n",
    "        # We plot the last feature dimension\n",
    "        true_node = targets[:, 0, node, -1].cpu().numpy()\n",
    "        pred_node = expected_value[:, 0, node, -1].cpu().numpy()\n",
    "\n",
    "        # --- 2. Calculate Prediction Interval (with Scipy) ---\n",
    "        # We do this inside the loop, one node at a time\n",
    "        try:\n",
    "            # Move params to numpy for scipy\n",
    "            mu_node = mu[:, 0, node, -1].cpu().numpy()\n",
    "            theta_node = theta[:, 0, node, -1].cpu().numpy()\n",
    "            pi_node = pi[:, 0, node, -1].cpu().numpy()\n",
    "\n",
    "            # A. Create the Negative Binomial component\n",
    "            # Scipy uses 'n' (total_count) and 'p' (prob)\n",
    "            n_scipy = np.maximum(theta_node, eps) # n = theta\n",
    "            p_scipy = n_scipy / (mu_node + n_scipy + eps) # p = theta / (mu + theta)\n",
    "            p_scipy = np.clip(p_scipy, eps, 1-eps)\n",
    "\n",
    "            # B. Calculate the total probability of zero\n",
    "            prob_zero = pi_node + (1 - pi_node) * sp_stats.nbinom.pmf(0, n=n_scipy, p=p_scipy)\n",
    "\n",
    "            # C. Define the quantiles we want\n",
    "            q_lower = 0.025\n",
    "            q_upper = 0.975\n",
    "\n",
    "            # D. Calculate lower bound\n",
    "            q_lower_adj = (q_lower - pi_node) / (1 - pi_node + eps)\n",
    "            q_lower_adj = np.clip(q_lower_adj, eps, 1-eps)\n",
    "            nb_quantile_lower = sp_stats.nbinom.ppf(q_lower_adj, n=n_scipy, p=p_scipy)\n",
    "            lower_bound = np.where(q_lower <= prob_zero, 0.0, nb_quantile_lower)\n",
    "\n",
    "            # E. Calculate upper bound\n",
    "            q_upper_adj = (q_upper - pi_node) / (1 - pi_node + eps)\n",
    "            q_upper_adj = np.clip(q_upper_adj, eps, 1-eps)\n",
    "            nb_quantile_upper = sp_stats.nbinom.ppf(q_upper_adj, n=n_scipy, p=p_scipy)\n",
    "            upper_bound = np.where(q_upper <= prob_zero, 0.0, nb_quantile_upper)\n",
    "\n",
    "            plot_interval = True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not compute prediction interval for node {node}. Plotting mean only. Error: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            plot_interval = False\n",
    "\n",
    "        # --- Plotting ---\n",
    "        # Plot Ground Truth\n",
    "        plt.plot(true_node[:n_samples], label='True', alpha=0.9, color='blue')\n",
    "\n",
    "        # Plot Predicted Mean\n",
    "        plt.plot(pred_node[:n_samples], label='Predicted (E[y])', alpha=0.9, color='orange', linestyle='--')\n",
    "\n",
    "        # Plot Prediction Interval\n",
    "        if plot_interval:\n",
    "            plt.fill_between(range(len(true_node[:n_samples])),\n",
    "                             lower_bound[:n_samples],\n",
    "                             upper_bound[:n_samples],\n",
    "                             color='orange',\n",
    "                             alpha=0.2,\n",
    "                             label='95% P.I.')\n",
    "\n",
    "        plt.title(f'Node {node} {names[node]} Traffic Prediction')\n",
    "        plt.xlabel('Sample Index')\n",
    "        plt.ylabel('Traffic Count')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "# Make sure to pass pi as the fourth argument\n",
    "print(\"--- Plotting Test Set ---\")\n",
    "plot_preds_and_ground_truth(test_targets, test_mu, test_theta, test_pi, id_to_name_map)\n",
    "# print(\"--- Plotting Validation Set ---\")\n",
    "# plot_preds_and_ground_truth(val_targets, val_mu, val_theta, val_pi, id_to_name_map)\n",
    "# print(\"--- Plotting Training Set ---\")\n",
    "# plot_preds_and_ground_truth(train_targets, train_mu, train_theta, train_pi, id_to_name_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
